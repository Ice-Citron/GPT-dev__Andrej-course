{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ebBCSJbIHoU",
        "outputId": "40833e59-65ed-42ac-c06a-8634545149db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "[ord(x) for x in \"hello world\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(\"hhh\".encode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3b6eX3mIPvK",
        "outputId": "9341fa3f-b58a-4997-a7c9-6c2963cf20fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[104, 104, 104]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/karpathy/minbpe/blob/master/tests/taylorswift.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXw4HUejMAnX",
        "outputId": "590499b1-931c-4c86-bc89-9b081ba207e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-28 16:34:40--  https://github.com/karpathy/minbpe/blob/master/tests/taylorswift.txt\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘taylorswift.txt’\n",
            "\n",
            "\rtaylorswift.txt         [<=>                 ]       0  --.-KB/s               \rtaylorswift.txt         [ <=>                ] 351.28K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-07-28 16:34:40 (5.51 MB/s) - ‘taylorswift.txt’ saved [359707]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class BasicTokenizer():\n",
        "\n",
        "  def __init__(self, max_vocab_count, max_n_gram, do_lower_case=True):\n",
        "    self.do_lower_case = do_lower_case\n",
        "    self.corpus = \"\"\n",
        "    self.vocab = {}         # for mapping token to id\n",
        "    self.id_to_token = {}   # for mapping id to token\n",
        "    self.max_vocab_count = max_vocab_count\n",
        "    self.vocab_count = 0\n",
        "    self.max_n_gram = max_n_gram\n",
        "\n",
        "  def insert_corpus(self, corpus):\n",
        "    if self.do_lower_case == True:\n",
        "      self.corpus = self.corpus.lower()\n",
        "    else:\n",
        "      self.corpus = corpus\n",
        "    self.vocab    = {char:idx for idx, char in enumerate(sorted(set(text)))}\n",
        "    self.id_token = {idx:char for char, idx in enumerate(sorted(set(text)))}\n",
        "    self.vocab_count = len(self.vocab)\n",
        "\n",
        "  def merge():\n",
        "    # need to just add token_to_id. then just reconvert corpus back and reprocess n-grams everytime.\n",
        "\n",
        "\n",
        "  def train():\n",
        "    while self.max_vocab_count >= self.vocab_count:\n",
        "      list_of_token = self.corpus.split()\n",
        "      counter = Counter(list_of_token)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ngJJ_PIv2E",
        "outputId": "db2f99a8-279d-4f59-9645-1ebae8e7de3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'hello': 1, 'world': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ctypes import c_char\n",
        "from contextlib import ContextDecorator\n",
        "text = \"\"\n",
        "with open('taylorswift.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "\n",
        "dicta = {char:idx for idx, char in enumerate(sorted(set(text)))}\n",
        "\n",
        "\n",
        "\n",
        "original = \"aabbccddeeffaabbccdd\"\n",
        "token_to_id = {char:idx for idx, char in enumerate(sorted(set(original)))}\n",
        "id_to_token = {idx:char for idx, char in enumerate(sorted(set(original)))}\n",
        "\n",
        "def encode_text(text, vocabulary, train=False):\n",
        "    # Split text into characters\n",
        "    characters = list(text)\n",
        "    encoded_output = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(characters):\n",
        "        matched = False\n",
        "        # Try to find the longest n-gram that fits\n",
        "        for n in range(5, 0, -1):  # Assuming we have up to trigrams of characters\n",
        "            if i + n <= len(characters):\n",
        "                n_gram = ''.join(characters[i:i+n])\n",
        "                if n_gram in vocabulary:\n",
        "                    encoded_output.append(vocabulary[n_gram])\n",
        "                    i += n  # Move past this n-gram\n",
        "                    matched = True\n",
        "                    break\n",
        "    if train==True:\n",
        "      encoded_output = [id_to_token[id] for id in encoded_output]\n",
        "\n",
        "    return encoded_output\n",
        "\n",
        "def decode_text(lista):\n",
        "  return\n",
        "def generate_ngrams(text):\n",
        "  concat = []\n",
        "  for n in range(2, 5, 1):\n",
        "    concat += [''.join(text[i:i+n]) for i in range(len(text) - n + 1)]\n",
        "  return concat\n",
        "\n",
        "text = encode_text(original, token_to_id, train=True)\n",
        "print(text)\n",
        "\n",
        "total = generate_ngrams(encode(original)) # Output: ['ab', 'bc', 'cd', 'de', 'ef'] also concatenated with 3-grams, 4-grams, etc.\n",
        "\n",
        "from collections import Counter\n",
        "counter = Counter(total)\n",
        "\n",
        "print(counter)\n",
        "\n",
        "count_of_twos = sum(1 for count in counter.values() if count == max(counter.values()))\n",
        "\n",
        "print(f\"Number of entries with a count of 2: {count_of_twos}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "-WaKr3aqNX0O",
        "outputId": "0287b635-ea19-4e9d-c986-5e4e39da4bc5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'a', 'b', 'b', 'c', 'c', 'd', 'd', 'e', 'e', 'f', 'f', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encode' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-35cb09bf5c97>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output: ['ab', 'bc', 'cd', 'de', 'ef'] also concatenated with 3-grams, 4-grams, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encode' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tDVwefAwOiuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}