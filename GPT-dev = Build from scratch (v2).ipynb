{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eDYZqwvGKhd6",
        "LAzJ_z_Bd0Sc"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EturOroypDmw",
        "outputId": "154fa0b1-7071-48e6-a097-14b7f65617be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.0\n",
            "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.1+cu121\n",
            "    Uninstalling torchaudio-2.3.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 triton-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FROM BELOW ARE NOTES FOR MAIN FILE V2 \"gpt.py\""
      ],
      "metadata": {
        "id": "A9TPnyE5pAQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GK7TCaxJo6Ex"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4FepucrpHuk",
        "outputId": "382dd02a-3978-40b7-97e4-6b12803fb8c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a79f41161b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Q5e4GtpHtL",
        "outputId": "53976a48-c32f-4ed5-9f95-7d09973a9824"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-28 08:36:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-07-28 08:36:25 (175 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text (ALREADY NOTED, REFER TO PREVIOUS V1 FILE FOR WORKINGS)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "27peNT1CpHrP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "bUZKQhabpHps"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generates tensor of random indices, shape tuple defined by batch_size // These indices serves to index the data file. See 2 lines below.\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "cm98UVghpHn5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # disables gradient calculation (used during evaluation)\n",
        "def estimate_loss():\n",
        "    out = {} # empty dict which function will store the average losses for each data split, \"train\" and \"val\"\n",
        "\n",
        "    model.eval() # this method switches the model to evaluation model. Which affects layers like dropout layers that have different behaviours during training vs evaluation (dropout is turned off during evaluation)\n",
        "    for split in ['train', 'val']: # begins a loop over the data splits. The function evaluates the model both on training data and validation data. Allowing you o monitor overfitting and general performance.\n",
        "        losses = torch.zeros(eval_iters) # initialises tensor \"losses\" filled with zeroes of length eval_iters\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split) # uses get_batch with current split to generate a bunch of input data X and target data Y\n",
        "            logits, loss = model(X, Y) # passes the batch through the model. The model returns logits (raw outputs before applying softmax), and the computed \"loss\" for this batch.\n",
        "            losses[k] = loss.item()  # extracts value of loss using \".item()\" and stores it in the losses tensor at index \"k\"\n",
        "        out[split] = losses.mean() # after all iterations for current split is complete. Calculates the mean of all recorded losses. And stores it in dictionary \"out\", where split is the key.\n",
        "    model.train() # switches model back to training mode after evaluating on both splits. Re-enabling training specific behaviours like dropout\n",
        "    return out"
      ],
      "metadata": {
        "id": "UchQrgKepHl_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes for class Head"
      ],
      "metadata": {
        "id": "f5OTtM_4Hyu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're on the right track in understanding the `nn.Linear` layer in PyTorch. Here's a detailed explanation of what it does and how it's configured in your code:\n",
        "\n",
        "### nn.Linear Function\n",
        "`nn.Linear` is a PyTorch module that applies a linear transformation to the incoming data. It's essentially a fully connected neural network layer. Here’s what each parameter in the `nn.Linear` initialization means:\n",
        "\n",
        "- **n_embd (input features):** This is the size of each input sample. For instance, if `n_embd` is 512, each input to the layer should have 512 features.\n",
        "\n",
        "- **head_size (output features):** This is the size of each output sample. The layer transforms the input dimension (`n_embd`) into the `head_size` dimension. For example, if `head_size` is 64, each output from this layer will have 64 features.\n",
        "\n",
        "- **bias (Boolean):** This is a flag that indicates whether a bias vector should be added to the output. If `bias=False`, no bias is added. If `bias=True`, a bias vector (initialized to zero by default) is created and added to the outputs.\n",
        "\n",
        "### Role in the Attention Head\n",
        "In the context of your `Head` class within a transformer, these layers (`self.key`, `self.query`, `self.value`) are used to transform the input into three different representations:\n",
        "- **Keys (k):** Used to interact with queries to compute attention scores.\n",
        "- **Queries (q):** Used to interact with keys to fetch the most relevant information across the sequence.\n",
        "- **Values (v):** Once the relevant positions are identified using keys and queries, the values at these positions are combined to produce the output.\n",
        "\n",
        "### Working Mechanism\n",
        "Here's what happens when you use `nn.Linear` in the context of your self-attention head:\n",
        "\n",
        "1. **Input Dimensionality:** The input `x` to your `forward` function in the `Head` class has the shape `[B, T, C]`, where `B` is the batch size, `T` is the sequence length (number of time steps), and `C` is the number of channels (here, `C` is `n_embd`, the embedding size).\n",
        "\n",
        "2. **Transformation:**\n",
        "   - When `x` is passed through `self.key(x)`, the layer transforms each `[B, T, n_embd]` input into `[B, T, head_size]`. It does this by multiplying `x` by a weight matrix `W` of shape `[n_embd, head_size]`, and since `bias=False`, no bias is added. The same transformation is applied by `self.query(x)` and `self.value(x)` to produce queries and values, respectively.\n",
        "\n",
        "3. **Output Dimensionality:** Each of the transformed outputs (`k`, `q`, `v`) now has the shape `[B, T, head_size]`, aligning with the required dimensions to compute attention scores and ultimately, the weighted sum of values.\n",
        "\n",
        "This transformation allows each head to project the input embeddings into a different subspace, helping the model to focus on different features of the input at different positions, enhancing its ability to capture complex relationships in the data."
      ],
      "metadata": {
        "id": "xqGsUGpIHytW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-S4X7C8LHyq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "EvxYqyqDHyoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) # transformers tensor of [B, T, n_embed] to [B, T, head_size] // after applying a trained linear transformation\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # A lower triangular matrix tril of size (block_size, block_size) is created and registered as\n",
        "                                                                                     # a buffer. This matrix is used later to apply a mask for the attention mechanism, allowing the model to\n",
        "                                                                                     # only attend to previous positions and prevent \"looking ahead.\"\n",
        "        self.dropout = nn.Dropout(dropout) # dropout layer is included to prevent overfitting during training by randomly zeroing out elements of the output tensor with a probability defined by dropout.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "                                                         # this is the attention(Q, K, V) equation. division by sqrt(d_k) can be seen.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ml_VknzZpVkv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes of MultiHeadAttention and nn.Module"
      ],
      "metadata": {
        "id": "eDYZqwvGKhd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! The `MultiHeadAttention` class is a crucial component of Transformer models, extending the concept of self-attention by incorporating multiple heads. Each head can potentially learn to pay attention to different parts of the input, making the model more powerful and versatile. Here’s a detailed breakdown of this class:\n",
        "\n",
        "### Class Definition\n",
        "```python\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "```\n",
        "- The class `MultiHeadAttention` inherits from `nn.Module`, which is a base class for all neural network modules in PyTorch. It manages multiple heads of attention that process the input in parallel, allowing the model to capture different aspects of information simultaneously.\n",
        "\n",
        "### Constructor\n",
        "```python\n",
        "def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # creates x number of parallel self attention heads\n",
        "    self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "```\n",
        "- **ModuleList of Heads:**\n",
        "  - `self.heads` is a `nn.ModuleList` containing several instances of the `Head` class defined earlier. The list comprehension `[Head(head_size) for _ in range(num_heads)]` creates `num_heads` instances of `Head`, each capable of transforming the input independently.\n",
        "  - `head_size` defines the size of each head's output.\n",
        "\n",
        "- **Projection Layer:**\n",
        "  - `self.proj` is a linear layer that projects the concatenated outputs of all attention heads back to the original embedding dimension (`n_embd`). This is necessary because each head outputs `head_size` features, and concatenating `num_heads` of them results in `head_size * num_heads` features.\n",
        "  - This layer maps the combined features back to the expected size for compatibility with other components in the Transformer architecture.\n",
        "\n",
        "- **Dropout:**\n",
        "  - `self.dropout` is a dropout layer that randomly zeroes elements of the output tensor with a probability defined by `dropout`. This regularization technique helps prevent overfitting.\n",
        "\n",
        "### Forward Pass\n",
        "```python\n",
        "def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out)) # projection back into the residual pathway\n",
        "    return out\n",
        "```\n",
        "- **Concatenating Heads Outputs:**\n",
        "  - The forward pass starts by applying each head in `self.heads` to the input `x`. The list comprehension `[h(x) for h in self.heads]` computes the outputs from all heads.\n",
        "  - `torch.cat([...], dim=-1)` concatenates these outputs along the last dimension. If each head's output has dimensions `[B, T, head_size]` and there are `num_heads` heads, the result will have dimensions `[B, T, head_size * num_heads]`.\n",
        "\n",
        "- **Projection and Dropout:**\n",
        "  - The concatenated output is then passed through the `self.proj` linear transformation, which reduces its dimensionality from `head_size * num_heads` back to `n_embd`, aligning it with the rest of the network.\n",
        "  - After the projection, dropout is applied for regularization.\n",
        "\n",
        "The `MultiHeadAttention` module effectively combines information from multiple representational spaces. By doing so, it allows the model to attend to information from different subsequences in different positions, which is a powerful mechanism in many NLP tasks. This architecture is one of the key reasons why Transformers excel in handling complex dependencies in sequence data."
      ],
      "metadata": {
        "id": "-YKvEuPlKp7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.Module` is one of the core building blocks in PyTorch, a popular deep learning framework. It serves as the base class for all neural network modules, and most of the functionality of neural networks in PyTorch is built on top of this class. Here’s a detailed breakdown of what `nn.Module` is, what it contains, and what it does:\n",
        "\n",
        "### Definition of nn.Module\n",
        "- **Base Class for All Networks:** `nn.Module` is the base class for all neural network modules in PyTorch. Any new neural network component should inherit from `nn.Module` to get all its functionality.\n",
        "\n",
        "### Key Features and Functions\n",
        "1. **Parameter Management:**\n",
        "   - **Automatic Parameter Registration:** When you define instance attributes that are `nn.Parameter` or `nn.Module` types, they are automatically added to the list of parameters (or sub-modules) that the module knows about. This includes weights, biases, and other parameters which are used in forward passes and are necessary for backpropagation.\n",
        "   - **Easy Access to Parameters:** `nn.Module` provides methods like `.parameters()` and `.named_parameters()` to iterate over all parameters of the model, which is very useful for optimization, saving, loading, etc.\n",
        "\n",
        "2. **Sub-modules Management:**\n",
        "   - **Hierarchical Structure:** You can nest `nn.Module` instances inside one another. This hierarchical organization allows building complex architectures easily. Methods like `.children()` and `.modules()` help in accessing these sub-modules at different levels of hierarchy.\n",
        "\n",
        "3. **Forward Pass Definition:**\n",
        "   - **Forward Method:** Each `nn.Module` subclass typically implements a `forward()` method. When you call the module (like a function call with `module(input)`), it internally calls `forward()` with the input. This method is where you define the computation performed by the module.\n",
        "\n",
        "4. **Gradient Computation and Backpropagation:**\n",
        "   - **Support for Autograd:** `nn.Module` seamlessly integrates with PyTorch’s autograd system. Parameters of the module are automatically registered for gradient computation. When used in a training loop, gradients are computed when calling `.backward()`, and PyTorch takes care of all the gradient flow calculations through modules.\n",
        "\n",
        "5. **Utilities for Training:**\n",
        "   - **to(device):** You can move all module parameters to a specified device (CPU or GPU) with a single call to `.to(device)`.\n",
        "   - **train() and eval():** Switch between training and evaluation modes. This affects the behavior of certain layers like dropout (active during training and inactive during evaluation) and batch normalization (uses running statistics during evaluation).\n",
        "\n",
        "6. **Serialization and Deserialization:**\n",
        "   - **Save and Load Models:** `nn.Module` provides convenient methods for saving (`torch.save(module.state_dict(), PATH)`) and loading (`module.load_state_dict(torch.load(PATH))`) the parameters of a model, allowing for model persistence and transfer.\n",
        "\n",
        "### Practical Example\n",
        "When you define a new class that inherits from `nn.Module`, you typically:\n",
        "1. Initialize the parent class in your constructor.\n",
        "2. Define any layers or parameters your module needs.\n",
        "3. Implement the `forward()` method to specify how the module processes input.\n",
        "\n",
        "Here is a simple example:\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(10, 5)  # A simple linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "```\n",
        "In this example, `SimpleModel` contains a single linear layer, and the `forward` method defines how the model processes input `x` through that layer.\n",
        "\n",
        "### Summary\n",
        "`nn.Module` is essentially what makes building, training, and using neural networks in PyTorch straightforward and flexible. It provides the infrastructure for assembling layers and parameters into a complete model, managing their states, and using them efficiently during training or inference."
      ],
      "metadata": {
        "id": "8w9g64H9Khbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "OTnW6-kzKhVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module): # inherits from nn.Module <-- base class for all NN modules in PyTorch\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size): # class constructor\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # creates x number of parallel self attention heads\n",
        "                                                                                # contains several instances of the \"Head\" class defined earlier. Each capable of transforming the input independently.\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd) # projects the concatenated outputs of all attention heads back to the original embedding dimension \"n_embeds\". This is\n",
        "                                                             # necessary because each head outputs head_size features, and concatenating num_heads of them results in head_size * num_heads features.\n",
        "        self.dropout = nn.Dropout(dropout) # dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenates the outputs from independent heads along the last dimension. // [B, T, head_size] becomes [B, T, head_size * num_heads]\n",
        "        out = self.dropout(self.proj(out)) # passes through this linear transformation. Which reduces its dimensionality from head_size*num_heads for dim=-1 to n_embed // after projection, dropout is applied for regularization\n",
        "        return out"
      ],
      "metadata": {
        "id": "kKcgaWS2pVhh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes for FeedForward"
      ],
      "metadata": {
        "id": "m13AUTSoTdmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code you've shared defines a class named `FeedForward`, which is a component commonly used in Transformer architectures, especially as specified in the influential paper \"Attention is All You Need\". This class is designed to process the output of the attention mechanism through a two-layer neural network with a ReLU non-linearity and dropout regularization. Here's a breakdown of the class and its functionalities:\n",
        "\n",
        "### Class Definition\n",
        "```python\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "```\n",
        "- **Class Inheritance:** `FeedForward` inherits from `nn.Module`, making it a module in the PyTorch framework.\n",
        "- **Documentation String:** The docstring indicates that this module involves a simple linear transformation followed by a non-linear activation function.\n",
        "\n",
        "### Constructor\n",
        "```python\n",
        "def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd), # both this and 2 lines below had been multiplied by 4 based on FFN implementation in the paper \"Attention is All You Need\"\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd), # this is the projection layer going back into the residual pathway\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "```\n",
        "- **Initialization:**\n",
        "  - `super().__init__()` is called to initialize the base class (`nn.Module`).\n",
        "  - `n_embd` is the embedding dimension (size of each input and output vector of the FeedForward module).\n",
        "\n",
        "- **Network Composition:** `self.net` is defined as a `nn.Sequential` container which automatically creates a network where the output of one layer is passed as the input to the next.\n",
        "  - **First Linear Layer:** Expands the dimensionality of the input from `n_embd` to `4 * n_embd`. This expansion is typical in Transformer models as it allows the network to create a richer representation of the data.\n",
        "  - **ReLU Activation:** Introduces non-linearity into the network, which helps the model learn complex patterns.\n",
        "  - **Second Linear Layer:** Projects the dimensionality back from `4 * n_embd` to `n_embd`, making the output dimension match the input dimension, which is crucial for the addition of the residual connection in the Transformer block.\n",
        "  - **Dropout:** Randomly zeroes some of the elements of the output tensor with a probability equal to `dropout`, which is a regularization method to prevent overfitting.\n",
        "\n",
        "### Forward Pass\n",
        "```python\n",
        "def forward(self, x):\n",
        "    return self.net(x)\n",
        "```\n",
        "- **Input and Output:** The forward method simply passes the input `x` through the `nn.Sequential` network defined in the constructor. It outputs the processed tensor which has the same dimension as the input tensor.\n",
        "\n",
        "### Summary\n",
        "The `FeedForward` module in the Transformer architecture typically follows the multi-head attention component in each layer of the model. It provides additional processing power by transforming the representations learned by the attention mechanism. The inclusion of non-linearity and expansion in dimensionality allows the network to explore more complex transformations before projecting back to the original dimension, which facilitates learning deeper representations in the context of sequences.\n",
        "\n",
        "This module's setup, especially with the expansion and contraction of dimensions and the use of dropout and ReLU, is critical for effective learning in deep Transformer networks."
      ],
      "metadata": {
        "id": "R3qPpqZvTdkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## back to coding"
      ],
      "metadata": {
        "id": "tFoQiyWPTdhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # both this and 2 lines below had been multipied by 4 based on FFN implementation in the\n",
        "                                           # paper \"Attention is All you need\" // transforms dim=-1 to 4*n_embed to have a richer representation before passing it back to ReLu\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # this is the projection layer going back into the residual pathway\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "Pp0Y7AnLIaQo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Method\n",
        "```python\n",
        "def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x)) # residual\n",
        "    x = x + self.ffwd(self.ln2(x)) # from feedforward\n",
        "    return x\n",
        "```\n",
        "- **Processing Flow:**\n",
        "  - The input `x` first passes through the first layer normalization (`self.ln1`). Normalizing the input to the multi-head attention helps in stabilizing the learning.\n",
        "  - The output from `self.ln1(x)` is then passed to the multi-head attention module (`self.sa`). The result is added back to the original input `x` in a residual connection. This is a key feature of the Transformer architecture, promoting easier gradient flow through deep networks and allowing the model to effectively learn identity functions, which can be critical in deep networks.\n",
        "  - The result of the attention and residual addition then passes through the second layer normalization (`self.ln2`). Similar to the first normalization, this helps in preparing the normalized data for subsequent operations.\n",
        "  - The output from `self.ln2(x)` is fed into the feed-forward network (`self.ffwd`). The output from the feed-forward network is added back to the input of the feed-forward network (another residual connection)."
      ],
      "metadata": {
        "id": "YhrBw3uCUlBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head # size of each attention head's output. Calculated by dividing embedding dimension by num_heads // ensures total dimensionality is preserved, when outputs of heads are concatenated\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # instance of multi-head attention class\n",
        "        self.ffwd = FeedFoward(n_embd) # notice how this occurs after attention // feed-forward network component of the trasnformer block.\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd) # 2 instances of nn.LayerNorm for applying layer normalization. // crucial for stabilising training of DNN. Applied before and after the attention and ffwd ops.\n",
        "\n",
        "    def forward(self, x): # REFER TO NOTES ABOVE FOR PROCESS\n",
        "        x = x + self.sa(self.ln1(x)) # residual\n",
        "        x = x + self.ffwd(self.ln2(x)) # from feedforward\n",
        "        return x"
      ],
      "metadata": {
        "id": "99-aB9SdpVfp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "Gy8gvbZwWMwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `GPTLanguageModel` class implements a language model based on the GPT (Generative Pre-trained Transformer) architecture using PyTorch. Here's a breakdown of the different components and functionalities within this class:\n",
        "\n",
        "### Class Definition\n",
        "\n",
        "```python\n",
        "class GPTLanguageModel(nn.Module):\n",
        "```\n",
        "- **Inheritance from nn.Module**: This class inherits from PyTorch’s `nn.Module`, which is the base class for all neural network modules.\n",
        "\n",
        "### Constructor\n",
        "\n",
        "```python\n",
        "def __init__(self):\n",
        "    super().__init__()\n",
        "```\n",
        "- **Initialization of Superclass**: Initializes the superclass `nn.Module`, enabling the functionality of the PyTorch module system.\n",
        "\n",
        "```python\n",
        "self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "```\n",
        "- **Embedding Tables**: Initializes two embedding layers.\n",
        "  - `token_embedding_table`: Maps each token in the vocabulary to a high-dimensional space (`n_embd`).\n",
        "  - `position_embedding_table`: Maps each position in a sequence (up to `block_size`) to the same high-dimensional space, allowing the model to consider the order of tokens.\n",
        "\n",
        "```python\n",
        "self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "```\n",
        "- **Blocks of Transformer**: Initializes a sequence of transformer blocks (`Block`), which handle the complex interactions between different parts of the input through attention mechanisms and feed-forward networks.\n",
        "\n",
        "```python\n",
        "self.ln_f = nn.LayerNorm(n_embd)\n",
        "self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "```\n",
        "- **Final Layer Normalization and Output Linear Layer**:\n",
        "  - `ln_f`: Applies normalization to the output of the last transformer block.\n",
        "  - `lm_head`: Transforms the output from the dimensionality of embeddings (`n_embd`) back to the vocabulary size, producing the logits for each token in the vocabulary.\n",
        "\n",
        "### Weight Initialization\n",
        "\n",
        "```python\n",
        "self.apply(self._init_weights)\n",
        "```\n",
        "- **Custom Weight Initialization**: Applies a custom weight initialization to all parameters in the model.\n",
        "\n",
        "```python\n",
        "def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        ...\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        ...\n",
        "```\n",
        "- **Weight Initialization Logic**: Defines how weights should be initialized for linear and embedding layers, enhancing model training stability and performance.\n",
        "\n",
        "### Forward Method\n",
        "\n",
        "```python\n",
        "def forward(self, idx, targets=None):\n",
        "```\n",
        "- **Processing Input**: Takes a batch of token indices (`idx`) and optionally target indices (`targets`).\n",
        "  - Embeds tokens and adds positional embeddings.\n",
        "  - Processes the combined embeddings through multiple transformer blocks.\n",
        "  - Applies final layer normalization and computes logits for each token in the sequence.\n",
        "\n",
        "```python\n",
        "if targets is not None:\n",
        "    ...\n",
        "```\n",
        "- **Loss Computation**: If targets are provided, computes the cross-entropy loss between logits and targets, which is used for training.\n",
        "\n",
        "### Text Generation Method\n",
        "\n",
        "```python\n",
        "def generate(self, idx, max_new_tokens):\n",
        "```\n",
        "- **Text Generation**: Implements a method to generate text given an initial sequence of token indices (`idx`).\n",
        "  - Continuously predicts the next token until `max_new_tokens` are generated.\n",
        "  - Uses the logits to sample the next token probabilistically.\n",
        "  - Appends the new token to the sequence and repeats.\n",
        "\n",
        "### Summary\n",
        "The `GPTLanguageModel` encapsulates the entire functionality required for a GPT-like language model, including token and position encoding, processing through transformer blocks, and generating predictions. It can be used for tasks like text generation, where the model needs to predict the next token in a sequence based on the previous tokens. The model is designed to be flexible and robust, capable of handling sequences up to a defined block size and being trained on a specified vocabulary."
      ],
      "metadata": {
        "id": "J2AgGydVWMty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: explain to me more clearly what does this do. whats it for and where is it used in the architecture\n",
        "\n",
        "    def _init_weights(self, module):\\\n",
        "        # Weight Initialization Logic: Defines how weights should be initialized for linear and embedding layers, enhancing model training stability and performance.\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "---\n",
        "\n",
        "The method `_init_weights` within your `GPTLanguageModel` class is designed to initialize the weights of the neural network components in a specific manner, which is crucial for improving the stability and performance of training deep neural networks like GPT.\n",
        "\n",
        "### Purpose of Weight Initialization\n",
        "\n",
        "Proper initialization of model weights can have significant impacts on the training process:\n",
        "- **Avoids Vanishing/Exploding Gradients:** Good initialization helps in preventing gradients from becoming too small (vanishing) or too large (exploding) during training, which can hinder the learning process.\n",
        "- **Faster Convergence:** It helps the model to converge faster, reducing the training time.\n",
        "- **Better Overall Performance:** Proper initialization can lead to better model performance by ensuring that neurons initially activate in a regime of their activation functions that makes learning effective.\n",
        "\n",
        "### Specifics of the `_init_weights` Method\n",
        "\n",
        "```python\n",
        "def _init_weights(self, module):\n",
        "    # Checks if the module is an instance of nn.Linear\n",
        "    if isinstance(module, nn.Linear):\n",
        "        # Initializes weights from a normal distribution with mean 0 and standard deviation 0.02\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        # Initializes biases to zero if the bias is not None (i.e., if biases are used in the layer)\n",
        "        if module.bias is not None:\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "    # Checks if the module is an instance of nn.Embedding\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        # Initializes embedding weights from a normal distribution with mean 0 and standard deviation 0.02\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "```\n",
        "\n",
        "### Where It Is Used in the Architecture\n",
        "\n",
        "- **Application on Modules:** This method is applied to every submodule within your `GPTLanguageModel` through the call `self.apply(self._init_weights)` in the constructor. The `apply` function is a built-in method in `nn.Module` that recursively applies a function to every submodule (including itself).\n",
        "- **Coverage:** It targets two types of layers—linear layers (`nn.Linear`) and embedding layers (`nn.Embedding`). These layers are fundamental components of the GPT architecture:\n",
        "  - **Linear Layers:** Used in various places like in the feedforward network (part of each transformer block) and in the output layer to generate logits for each vocabulary token.\n",
        "  - **Embedding Layers:** Used for converting token indices into embeddings and for adding positional encodings to these embeddings, which are crucial for maintaining the sequence context in models like GPT.\n",
        "\n",
        "### Summary\n",
        "The `_init_weights` function is a crucial setup step in preparing the GPT model for training, ensuring that all components start from a consistent, well-considered initial state that promotes effective and stable learning. This step is particularly important in deep learning models, where poor initialization can severely hamper the model's ability to learn."
      ],
      "metadata": {
        "id": "lcNOl2jqWMrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "can you explain to me how does the methods work themselves?\n",
        "\n",
        "\n",
        "torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "torch.nn.init.zeros_(module.bias)\n",
        "torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "---\n",
        "\n",
        "Certainly! The methods you've mentioned are from PyTorch's initialization module (`torch.nn.init`), which provides several functions to set the initial weights and biases of neural network layers. Let's dive into how each of these methods works and why they are used:\n",
        "\n",
        "### 1. `torch.nn.init.normal_`\n",
        "```python\n",
        "torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "```\n",
        "- **Purpose**: This function initializes the weights of a given tensor (usually a layer's weights) with values drawn from a normal (Gaussian) distribution.\n",
        "- **Parameters**:\n",
        "  - `module.weight`: The tensor (weights of a neural network layer) that you want to initialize.\n",
        "  - `mean=0.0`: The mean (μ) of the normal distribution.\n",
        "  - `std=0.02`: The standard deviation (σ) of the normal distribution. A standard deviation of 0.02 is often used in transformer models based on empirical results showing it helps in stabilizing training.\n",
        "- **Operation**: The function directly modifies the tensor `module.weight` in-place by filling it with values sampled from a normal distribution with the specified mean and standard deviation.\n",
        "- **Usage**: This initialization is typically used for weights to ensure they are neither too small nor too large, avoiding the vanishing or exploding gradients problem in deep networks. It's a common choice for layers in transformer architectures, like GPT.\n",
        "\n",
        "### 2. `torch.nn.init.zeros_`\n",
        "```python\n",
        "torch.nn.init.zeros_(module.bias)\n",
        "```\n",
        "- **Purpose**: This function initializes the biases of a given tensor with zero values.\n",
        "- **Parameters**:\n",
        "  - `module.bias`: The tensor (biases of a neural network layer) that you want to initialize.\n",
        "- **Operation**: The function modifies the tensor `module.bias` in-place by setting all its elements to zero.\n",
        "- **Usage**: Initializing biases to zero is a common practice as it does not add any initial prejudice to the outputs. It ensures that the initial outputs of neurons are solely driven by the weights and not influenced by biases. This can lead to a more stable and predictable start to training.\n",
        "\n",
        "### 3. Repeated Example\n",
        "```python\n",
        "torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "```\n",
        "- This line is identical to the first example and operates in the same way. It’s repeated usage across different modules in a model helps maintain consistency in how the weights are initialized, supporting overall model stability and convergence during training.\n",
        "\n",
        "### Summary\n",
        "These initialization methods are crucial for setting the starting conditions of a neural network:\n",
        "- **Weights**: Properly initializing weights with `normal_` can greatly influence the learning dynamics, especially important in deep and complex architectures like transformers, where different layers need to start with weights that neither dominate the propagated signals nor vanish them.\n",
        "- **Biases**: Setting biases to zero with `zeros_` is a safe starting point that contributes to the initial neutrality in decision boundaries, allowing the learning process to evolve naturally based on the data and loss gradients.\n",
        "\n",
        "By carefully choosing these initialization schemes, you can ensure that your model begins training under conditions that promote effective learning and convergence."
      ],
      "metadata": {
        "id": "wrrtD0VwW_g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "tzxYFeDpWMoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "                # not just encoding identity of token here. but also its position!\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # initialises 2 embedding layers. REFER TO NOTES ABOVE\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # initialises a sequence of transformer blocks. Which handles the complex interactions between different parts of the input\n",
        "                                                                                             # through attention mechanisms and feed-forward networks.\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm // applies normalisation to the output of last transformer block\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # short for language model head // Transforms the output from the dimensionality of embeddings (n_embd) back to the vocabulary size, producing the logits\n",
        "                                                     # for each token in the vocabulary.\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights) # Customg Weight Initialization: Applies a custom weight initialization to all parameters in the model.\n",
        "\n",
        "    def _init_weights(self, module):\\\n",
        "        # Weight Initialization Logic: Defines how weights should be initialized for linear and embedding layers, enhancing model training stability and performance.\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # .normal_ means normal distribution\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias) # initialises the bias to 0 if bias is used.\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape # input dimensions\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "                                                                                # Generates positional embeddings for each position in the sequence up to T. torch.arange(T) creates a tensor from 0 to T-1,\n",
        "                                                                                # which is then used to fetch positional embeddings. This is crucial as it helps the model understand the order of tokens in the sequence.\n",
        "        x = tok_emb + pos_emb # (B,T,C) // at this point x not only contains token identity. but also\n",
        "                                        #  positional identity from position embedding (combines semantic and positional information)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "                         #  Applies a normalization layer (self.ln_f) to the output of the last transformer block, stabilizing the learning process by normalizing the layer outputs.\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "                                 # The final output is transformed by self.lm_head, a linear layer that maps the high-dimensional transformer block outputs to the size of the vocabulary. This produces logits, where each\n",
        "                                 # element at position [b, t] represents the logit scores (unnormalized probabilities) for predicting each vocabulary token at position t in batch b.\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else: # if target is provided. Or loss needs to be calculated\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # reshaping for matmul operations\n",
        "            targets = targets.view(B*T) # reshaping for matmul operations\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens // crops input context to the maximum context size that the model can handle.\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "                    # torch.multinomial treats each row in probs as a separate distributon and samples num_samples=1 index from each distribution\n",
        "                    # based on probabilities distribution from softmax // multinomial basically just picks options based on how likely they are to be picked\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "V5866rzLqXuD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3PH-R2DqXrw",
        "outputId": "d3b8ebc9-56fb-440f-8401-d7864dbd81ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "dhbemYOIqXpp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "LAzJ_z_Bd0Sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you provided outlines a typical training loop for a machine learning model using PyTorch. This loop iteratively updates the model's parameters to minimize the loss function over a dataset. Let's break down each part of the loop and explain how it works:\n",
        "\n",
        "### The Training Loop\n",
        "```python\n",
        "for iter in range(max_iters):\n",
        "```\n",
        "- **Iteration Control**: The loop iterates over a predefined number of iterations `max_iters`. Each iteration represents a single step of training where the model parameters are updated.\n",
        "\n",
        "### Periodic Evaluation\n",
        "```python\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "```\n",
        "- **Conditional Evaluation**: At regular intervals (`eval_interval`), and at the last iteration, the model's performance is evaluated on both the training and validation datasets.\n",
        "- **Loss Calculation**: The `estimate_loss()` function is called, which likely computes the average loss over several batches of both training and validation data without affecting the model’s parameters (as it should be run in evaluation mode).\n",
        "- **Logging**: The losses are printed out, giving a snapshot of the model's performance at that point in training. This helps in monitoring the training progress and debugging if necessary.\n",
        "\n",
        "### Sampling a Batch of Data\n",
        "```python\n",
        "    xb, yb = get_batch('train')\n",
        "```\n",
        "- **Data Fetching**: A batch of data (`xb` for inputs, `yb` for targets) is fetched from the training dataset using the `get_batch` function. This function probably handles the data loading and batching operations, ensuring that the model sees different parts of the dataset in each epoch.\n",
        "\n",
        "### Loss Calculation\n",
        "```python\n",
        "    logits, loss = model(xb, yb)\n",
        "```\n",
        "- **Forward Pass**: The model processes the batch of inputs (`xb`) to produce outputs (`logits`) and computes the loss (`loss`) by comparing the outputs to the actual targets (`yb`).\n",
        "\n",
        "### Gradient Computation and Parameter Update\n",
        "```python\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "- **Zeroing Gradients**: Before computing the gradients, the gradients of all model parameters are set to zero (or `None`, which can be slightly more efficient). This is necessary because gradients are accumulated by default, for cases involving RNNs or when multiple sub-parts of the dataset are processed before a step of the optimizer.\n",
        "- **Backward Pass**: The `loss.backward()` computes the gradients of the loss with respect to the model parameters. This is where PyTorch's autograd system comes into play, calculating gradients efficiently through the computation graph.\n",
        "- **Parameter Update**: `optimizer.step()` updates the model parameters using the computed gradients. The type of update depends on the optimizer used (e.g., SGD, Adam).\n",
        "\n",
        "### Summary\n",
        "This training loop is fundamental for training deep learning models. It repeatedly adjusts the model's weights to minimize the loss on a training set, evaluates performance on a validation set to monitor overfitting, and provides feedback on the training progress. Properly managing these steps ensures effective training and is crucial for developing accurate models."
      ],
      "metadata": {
        "id": "q6PdYABbd0Qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## back to coding"
      ],
      "metadata": {
        "id": "T0vN5oxhd0OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD4YY2ADqXnZ",
        "outputId": "e69b43cc-cdb4-4dfc-93da-aa0ff2f30487"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2221, val loss 4.2306\n",
            "step 500: train loss 1.7526, val loss 1.9053\n",
            "step 1000: train loss 1.3949, val loss 1.6050\n",
            "step 1500: train loss 1.2625, val loss 1.5219\n",
            "step 2000: train loss 1.1860, val loss 1.5046\n",
            "step 2500: train loss 1.1254, val loss 1.4972\n",
            "step 3000: train loss 1.0694, val loss 1.4849\n",
            "step 3500: train loss 1.0211, val loss 1.5048\n",
            "step 4000: train loss 0.9643, val loss 1.5160\n",
            "step 4500: train loss 0.9121, val loss 1.5396\n",
            "step 5000: train loss 0.8673, val loss 1.5552\n",
            "step 5500: train loss 0.8052, val loss 1.5988\n",
            "step 6000: train loss 0.7611, val loss 1.6231\n",
            "step 6500: train loss 0.7087, val loss 1.6706\n",
            "step 7000: train loss 0.6644, val loss 1.7000\n",
            "step 7500: train loss 0.6187, val loss 1.7484\n",
            "step 8000: train loss 0.5818, val loss 1.7882\n",
            "step 8500: train loss 0.5350, val loss 1.8304\n",
            "step 9000: train loss 0.4973, val loss 1.8688\n",
            "step 9500: train loss 0.4638, val loss 1.9050\n",
            "step 9999: train loss 0.4333, val loss 1.9475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device) # basically inputtng nothing as context for model\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdp5gw-8qeVP",
        "outputId": "64b02281-4486-416b-a89f-0683b78f3f4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KING RICHARD III:\n",
            "Mistakest me haste; become enough that I was.\n",
            "She what hast thou torments now, thou rose\n",
            "And here should not shame thee says a strength shore,\n",
            "To thiry by his country of smiles' tongue\n",
            "And seem chase them ere in parchasing toward the shards,\n",
            "As doth aid office and I am corn\n",
            "Till it from the end hath my hazard.\n",
            "He hath at said throws for the time butcherly,\n",
            "That he had nothing but wrong'd, made hauntings\n",
            "And all these eyes did eal his despitions,\n",
            "Kill dis the enemies regain a cr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'full_model.pth')"
      ],
      "metadata": {
        "id": "SOL8qEt0rbtK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftmRQBt-rcyW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}