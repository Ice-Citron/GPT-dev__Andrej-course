{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eDYZqwvGKhd6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0"
      ],
      "metadata": {
        "id": "EturOroypDmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FROM BELOW ARE NOTES FOR MAIN FILE V2 \"gpt.py\""
      ],
      "metadata": {
        "id": "A9TPnyE5pAQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK7TCaxJo6Ex"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "G4FepucrpHuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "F7Q5e4GtpHtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text (ALREADY NOTED, REFER TO PREVIOUS V1 FILE FOR WORKINGS)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "27peNT1CpHrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "bUZKQhabpHps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generates tensor of random indices, shape tuple defined by batch_size // These indices serves to index the data file. See 2 lines below.\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "cm98UVghpHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # disables gradient calculation (used during evaluation)\n",
        "def estimate_loss():\n",
        "    out = {} # empty dict which function will store the average losses for each data split, \"train\" and \"val\"\n",
        "\n",
        "    model.eval() # this method switches the model to evaluation model. Which affects layers like dropout layers that have different behaviours during training vs evaluation (dropout is turned off during evaluation)\n",
        "    for split in ['train', 'val']: # begins a loop over the data splits. The function evaluates the model both on training data and validation data. Allowing you o monitor overfitting and general performance.\n",
        "        losses = torch.zeros(eval_iters) # initialises tensor \"losses\" filled with zeroes of length eval_iters\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split) # uses get_batch with current split to generate a bunch of input data X and target data Y\n",
        "            logits, loss = model(X, Y) # passes the batch through the model. The model returns logits (raw outputs before applying softmax), and the computed \"loss\" for this batch.\n",
        "            losses[k] = loss.item()  # extracts value of loss using \".item()\" and stores it in the losses tensor at index \"k\"\n",
        "        out[split] = losses.mean() # after all iterations for current split is complete. Calculates the mean of all recorded losses. And stores it in dictionary \"out\", where split is the key.\n",
        "    model.train() # switches model back to training mode after evaluating on both splits. Re-enabling training specific behaviours like dropout\n",
        "    return out"
      ],
      "metadata": {
        "id": "UchQrgKepHl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes for class Head"
      ],
      "metadata": {
        "id": "f5OTtM_4Hyu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're on the right track in understanding the `nn.Linear` layer in PyTorch. Here's a detailed explanation of what it does and how it's configured in your code:\n",
        "\n",
        "### nn.Linear Function\n",
        "`nn.Linear` is a PyTorch module that applies a linear transformation to the incoming data. It's essentially a fully connected neural network layer. Here’s what each parameter in the `nn.Linear` initialization means:\n",
        "\n",
        "- **n_embd (input features):** This is the size of each input sample. For instance, if `n_embd` is 512, each input to the layer should have 512 features.\n",
        "\n",
        "- **head_size (output features):** This is the size of each output sample. The layer transforms the input dimension (`n_embd`) into the `head_size` dimension. For example, if `head_size` is 64, each output from this layer will have 64 features.\n",
        "\n",
        "- **bias (Boolean):** This is a flag that indicates whether a bias vector should be added to the output. If `bias=False`, no bias is added. If `bias=True`, a bias vector (initialized to zero by default) is created and added to the outputs.\n",
        "\n",
        "### Role in the Attention Head\n",
        "In the context of your `Head` class within a transformer, these layers (`self.key`, `self.query`, `self.value`) are used to transform the input into three different representations:\n",
        "- **Keys (k):** Used to interact with queries to compute attention scores.\n",
        "- **Queries (q):** Used to interact with keys to fetch the most relevant information across the sequence.\n",
        "- **Values (v):** Once the relevant positions are identified using keys and queries, the values at these positions are combined to produce the output.\n",
        "\n",
        "### Working Mechanism\n",
        "Here's what happens when you use `nn.Linear` in the context of your self-attention head:\n",
        "\n",
        "1. **Input Dimensionality:** The input `x` to your `forward` function in the `Head` class has the shape `[B, T, C]`, where `B` is the batch size, `T` is the sequence length (number of time steps), and `C` is the number of channels (here, `C` is `n_embd`, the embedding size).\n",
        "\n",
        "2. **Transformation:**\n",
        "   - When `x` is passed through `self.key(x)`, the layer transforms each `[B, T, n_embd]` input into `[B, T, head_size]`. It does this by multiplying `x` by a weight matrix `W` of shape `[n_embd, head_size]`, and since `bias=False`, no bias is added. The same transformation is applied by `self.query(x)` and `self.value(x)` to produce queries and values, respectively.\n",
        "\n",
        "3. **Output Dimensionality:** Each of the transformed outputs (`k`, `q`, `v`) now has the shape `[B, T, head_size]`, aligning with the required dimensions to compute attention scores and ultimately, the weighted sum of values.\n",
        "\n",
        "This transformation allows each head to project the input embeddings into a different subspace, helping the model to focus on different features of the input at different positions, enhancing its ability to capture complex relationships in the data."
      ],
      "metadata": {
        "id": "xqGsUGpIHytW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-S4X7C8LHyq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "EvxYqyqDHyoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) # transformers tensor of [B, T, n_embed] to [B, T, head_size] // after applying a trained linear transformation\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # A lower triangular matrix tril of size (block_size, block_size) is created and registered as\n",
        "                                                                                     # a buffer. This matrix is used later to apply a mask for the attention mechanism, allowing the model to\n",
        "                                                                                     # only attend to previous positions and prevent \"looking ahead.\"\n",
        "        self.dropout = nn.Dropout(dropout) # dropout layer is included to prevent overfitting during training by randomly zeroing out elements of the output tensor with a probability defined by dropout.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "                                                         # this is the attention(Q, K, V) equation. division by sqrt(d_k) can be seen.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ml_VknzZpVkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes of MultiHeadAttention and nn.Module"
      ],
      "metadata": {
        "id": "eDYZqwvGKhd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! The `MultiHeadAttention` class is a crucial component of Transformer models, extending the concept of self-attention by incorporating multiple heads. Each head can potentially learn to pay attention to different parts of the input, making the model more powerful and versatile. Here’s a detailed breakdown of this class:\n",
        "\n",
        "### Class Definition\n",
        "```python\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "```\n",
        "- The class `MultiHeadAttention` inherits from `nn.Module`, which is a base class for all neural network modules in PyTorch. It manages multiple heads of attention that process the input in parallel, allowing the model to capture different aspects of information simultaneously.\n",
        "\n",
        "### Constructor\n",
        "```python\n",
        "def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # creates x number of parallel self attention heads\n",
        "    self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "```\n",
        "- **ModuleList of Heads:**\n",
        "  - `self.heads` is a `nn.ModuleList` containing several instances of the `Head` class defined earlier. The list comprehension `[Head(head_size) for _ in range(num_heads)]` creates `num_heads` instances of `Head`, each capable of transforming the input independently.\n",
        "  - `head_size` defines the size of each head's output.\n",
        "\n",
        "- **Projection Layer:**\n",
        "  - `self.proj` is a linear layer that projects the concatenated outputs of all attention heads back to the original embedding dimension (`n_embd`). This is necessary because each head outputs `head_size` features, and concatenating `num_heads` of them results in `head_size * num_heads` features.\n",
        "  - This layer maps the combined features back to the expected size for compatibility with other components in the Transformer architecture.\n",
        "\n",
        "- **Dropout:**\n",
        "  - `self.dropout` is a dropout layer that randomly zeroes elements of the output tensor with a probability defined by `dropout`. This regularization technique helps prevent overfitting.\n",
        "\n",
        "### Forward Pass\n",
        "```python\n",
        "def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out)) # projection back into the residual pathway\n",
        "    return out\n",
        "```\n",
        "- **Concatenating Heads Outputs:**\n",
        "  - The forward pass starts by applying each head in `self.heads` to the input `x`. The list comprehension `[h(x) for h in self.heads]` computes the outputs from all heads.\n",
        "  - `torch.cat([...], dim=-1)` concatenates these outputs along the last dimension. If each head's output has dimensions `[B, T, head_size]` and there are `num_heads` heads, the result will have dimensions `[B, T, head_size * num_heads]`.\n",
        "\n",
        "- **Projection and Dropout:**\n",
        "  - The concatenated output is then passed through the `self.proj` linear transformation, which reduces its dimensionality from `head_size * num_heads` back to `n_embd`, aligning it with the rest of the network.\n",
        "  - After the projection, dropout is applied for regularization.\n",
        "\n",
        "The `MultiHeadAttention` module effectively combines information from multiple representational spaces. By doing so, it allows the model to attend to information from different subsequences in different positions, which is a powerful mechanism in many NLP tasks. This architecture is one of the key reasons why Transformers excel in handling complex dependencies in sequence data."
      ],
      "metadata": {
        "id": "-YKvEuPlKp7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.Module` is one of the core building blocks in PyTorch, a popular deep learning framework. It serves as the base class for all neural network modules, and most of the functionality of neural networks in PyTorch is built on top of this class. Here’s a detailed breakdown of what `nn.Module` is, what it contains, and what it does:\n",
        "\n",
        "### Definition of nn.Module\n",
        "- **Base Class for All Networks:** `nn.Module` is the base class for all neural network modules in PyTorch. Any new neural network component should inherit from `nn.Module` to get all its functionality.\n",
        "\n",
        "### Key Features and Functions\n",
        "1. **Parameter Management:**\n",
        "   - **Automatic Parameter Registration:** When you define instance attributes that are `nn.Parameter` or `nn.Module` types, they are automatically added to the list of parameters (or sub-modules) that the module knows about. This includes weights, biases, and other parameters which are used in forward passes and are necessary for backpropagation.\n",
        "   - **Easy Access to Parameters:** `nn.Module` provides methods like `.parameters()` and `.named_parameters()` to iterate over all parameters of the model, which is very useful for optimization, saving, loading, etc.\n",
        "\n",
        "2. **Sub-modules Management:**\n",
        "   - **Hierarchical Structure:** You can nest `nn.Module` instances inside one another. This hierarchical organization allows building complex architectures easily. Methods like `.children()` and `.modules()` help in accessing these sub-modules at different levels of hierarchy.\n",
        "\n",
        "3. **Forward Pass Definition:**\n",
        "   - **Forward Method:** Each `nn.Module` subclass typically implements a `forward()` method. When you call the module (like a function call with `module(input)`), it internally calls `forward()` with the input. This method is where you define the computation performed by the module.\n",
        "\n",
        "4. **Gradient Computation and Backpropagation:**\n",
        "   - **Support for Autograd:** `nn.Module` seamlessly integrates with PyTorch’s autograd system. Parameters of the module are automatically registered for gradient computation. When used in a training loop, gradients are computed when calling `.backward()`, and PyTorch takes care of all the gradient flow calculations through modules.\n",
        "\n",
        "5. **Utilities for Training:**\n",
        "   - **to(device):** You can move all module parameters to a specified device (CPU or GPU) with a single call to `.to(device)`.\n",
        "   - **train() and eval():** Switch between training and evaluation modes. This affects the behavior of certain layers like dropout (active during training and inactive during evaluation) and batch normalization (uses running statistics during evaluation).\n",
        "\n",
        "6. **Serialization and Deserialization:**\n",
        "   - **Save and Load Models:** `nn.Module` provides convenient methods for saving (`torch.save(module.state_dict(), PATH)`) and loading (`module.load_state_dict(torch.load(PATH))`) the parameters of a model, allowing for model persistence and transfer.\n",
        "\n",
        "### Practical Example\n",
        "When you define a new class that inherits from `nn.Module`, you typically:\n",
        "1. Initialize the parent class in your constructor.\n",
        "2. Define any layers or parameters your module needs.\n",
        "3. Implement the `forward()` method to specify how the module processes input.\n",
        "\n",
        "Here is a simple example:\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(10, 5)  # A simple linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "```\n",
        "In this example, `SimpleModel` contains a single linear layer, and the `forward` method defines how the model processes input `x` through that layer.\n",
        "\n",
        "### Summary\n",
        "`nn.Module` is essentially what makes building, training, and using neural networks in PyTorch straightforward and flexible. It provides the infrastructure for assembling layers and parameters into a complete model, managing their states, and using them efficiently during training or inference."
      ],
      "metadata": {
        "id": "8w9g64H9Khbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "OTnW6-kzKhVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module): # inherits from nn.Module <-- base class for all NN modules in PyTorch\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size): # class constructor\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # creates x number of parallel self attention heads\n",
        "                                                                                # contains several instances of the \"Head\" class defined earlier. Each capable of transforming the input independently.\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd) # projects the concatenated outputs of all attention heads back to the original embedding dimension \"n_embeds\". This is\n",
        "                                                             # necessary because each head outputs head_size features, and concatenating num_heads of them results in head_size * num_heads features.\n",
        "        self.dropout = nn.Dropout(dropout) # dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenates the outputs from independent heads along the last dimension. // [B, T, head_size] becomes [B, T, head_size * num_heads]\n",
        "        out = self.dropout(self.proj(out)) # passes through this linear transformation. Which reduces its dimensionality from head_size*num_heads for dim=-1 to n_embed // after projection, dropout is applied for regularization\n",
        "        return out"
      ],
      "metadata": {
        "id": "kKcgaWS2pVhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # both this and 2 lines below had been multipied by 4 based on FFN implementation in the\n",
        "                                           # paper \"Attention is All you need\"\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # this is the projection layer going back into the residual pathway\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "Pp0Y7AnLIaQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # multi-head attention class initialisation\n",
        "        self.ffwd = FeedFoward(n_embd) # notice how this occurs after attention\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # residual\n",
        "        x = x + self.ffwd(self.ln2(x)) # from feedforward\n",
        "        return x"
      ],
      "metadata": {
        "id": "99-aB9SdpVfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "                # not just encoding identity of token here. but also its position!\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # multiple blocks can be seen here\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # short for language model head\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C) // at this point x not only contains token identity. but also\n",
        "                                        #  positional identity from position embedding\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "V5866rzLqXuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "id": "v3PH-R2DqXrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "dhbemYOIqXpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "xD4YY2ADqXnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "id": "qdp5gw-8qeVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}