{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lLIdlN5gJTlu",
        "outputId": "86804b21-7e2c-4059-d69c-0a952261233d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.0\n",
            "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.1+cu121\n",
            "    Uninstalling torchaudio-2.3.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 triton-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYq5ZhzOJtSx",
        "outputId": "903a13f9-c754-4be7-bbeb-869678a675b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-26 10:10:15--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-26 10:10:15 (27.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "pq2Bp6IdJzDT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRVszaUrLq7s",
        "outputId": "2d09dc7a-91e0-4717-8d90-c84673ccf7a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QbEtTGlLq5P",
        "outputId": "07fd4d3f-18f6-4e44-88ea-5a39f3542c51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This block identifies all unique character in string or tet stored in variable 'text'\n",
        "chars = sorted(list(set(text))) # Extracts unique characters from the string text, converts the set to a list, and sorts it alphabetically.\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars)) # prints sorted list\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56OK0KWkLq3H",
        "outputId": "d4ff5f38-ddd2-4b00-c2bc-3955ec7a776b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers + defines function for encoding and decoding strings\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # create a dictionary mapping characters to integers // i is index, ch is character in enumerate (this is fashion of enumerate)\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CbWXBiPLq1A",
        "outputId": "bee555b4-5210-4404-d83f-2b5eff1de24e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the first 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V60tyBgDLqy5",
        "outputId": "6909c653-82fd-4599-f360-6782c3519a03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "myMh6RYkLqw1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn3olFVRLquj",
        "outputId": "5443db7b-785f-4531-e45f-face5ead419f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # x represents the input sequence // takes the first block_size number of elements\n",
        "y = train_data[1:block_size+1] # takes elements from 2nd elements up to block_size+1 number of elements // is essentially x but shifted one position to the right\n",
        "                               # This is used as the target sequence for training. Where each element in y is the sequence that model should predict after seeing sequence in 'x'\n",
        "\n",
        "for t in range(block_size): # This for loop iterates over each position in the block_size, effectively allowing you to demonstrate or train the model on increasing amounts of context.\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")\n",
        "\n",
        "# note: as seen below, when provided a sequence in training set. The model actually tries to train the data batch as just [1] to [9], to predict the\n",
        "#       token. So that the model are able to capture semantic meaning better, and also is able to deal with small inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOoMmCZgL64z",
        "outputId": "5b01c71d-85b6-4a0b-9bd6-8f71bdffa0cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Notes"
      ],
      "metadata": {
        "id": "18ObmqafP6zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: What is byte-pair encoding?\n",
        "\n",
        "Byte-Pair Encoding (BPE) is a data compression technique originally designed to efficiently compress text data. It has been adapted in natural language processing (NLP) to perform subword tokenization, which involves breaking down words into more manageable units (subwords) for models to process. This technique is especially useful in handling languages with rich morphology or in managing the vocabulary size in neural network models.\n",
        "\n",
        "### How Byte-Pair Encoding Works in NLP\n",
        "\n",
        "1. **Initialization**: Start with a vocabulary of individual characters or other base units (like bytes in its original application) from the training data.\n",
        "\n",
        "2. **Frequency Count**: Count the frequency of pairs of adjacent symbols or characters in the text data.\n",
        "\n",
        "3. **Merge Operations**: Identify the most frequently occurring pair of symbols and merge them into a single new symbol. This merged symbol is then added to the vocabulary.\n",
        "\n",
        "4. **Iterative Process**: Repeat the merging process for a predetermined number of steps or until some criterion is met (like a desired vocabulary size). Each iteration merges the next most frequent pair.\n",
        "\n",
        "5. **Tokenization**: Once the vocabulary of symbols (including merged symbols) is created, tokenize the text by greedily replacing the longest possible sequences in the text with the corresponding symbols in the vocabulary.\n",
        "\n",
        "### Advantages of BPE in NLP\n",
        "\n",
        "- **Efficiency in Unknown Words**: BPE allows a model to handle words not seen during training, by breaking them into known subwords. This is particularly useful in languages with compounding or agglutination where new word forms are frequent.\n",
        "\n",
        "- **Balance Between Character and Word Level**: BPE provides a balance between character-level and word-level representations. It avoids the sparsity of character-level processing and the vocabulary explosion of word-level processing.\n",
        "\n",
        "- **Contextual Awareness**: Subwords provide more contextual information than individual characters, helping models understand suffixes, prefixes, and roots, which are crucial for semantic and syntactic understanding.\n",
        "\n",
        "### Usage in Modern NLP\n",
        "\n",
        "BPE has been employed in several state-of-the-art NLP models and frameworks, notably in models like GPT (from OpenAI), BERT, and other Transformer-based architectures. These models use BPE or its variants (like SentencePiece or WordPiece) to preprocess text into subwords, which are then used for tasks like translation, text summarization, and question-answering.\n",
        "\n",
        "Overall, BPE is a pivotal component in the preprocessing pipeline for many NLP tasks, helping to improve both the efficiency and effectiveness of models in handling diverse and complex language data."
      ],
      "metadata": {
        "id": "QhMuDwmUPkBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `torch.stack` in PyTorch is used to concatenate a sequence of tensors along a new dimension. This is different from functions like `torch.cat`, which concatenate tensors along an existing dimension. `torch.stack` effectively combines tensors in such a way that the resulting tensor has an additional dimension compared to the input tensors.\n",
        "\n",
        "### Usage of `torch.stack`\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "torch.stack(tensors, dim=0)\n",
        "```\n",
        "\n",
        "- **tensors**: a sequence of tensors to concatenate. All tensors must have the same shape.\n",
        "- **dim**: the dimension along which to concatenate the tensors. This dimension is added to the resulting tensor, and each tensor in the sequence will be sliced along this new dimension.\n",
        "\n",
        "### Example\n",
        "\n",
        "Let's consider you have three 2x2 tensors:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Define three 2x2 tensors\n",
        "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
        "tensor3 = torch.tensor([[9, 10], [11, 12]])\n",
        "\n",
        "# Stack these tensors along a new dimension\n",
        "result = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "tensor([[[ 1,  2],\n",
        "         [ 3,  4]],\n",
        "\n",
        "        [[ 5,  6],\n",
        "         [ 7,  8]],\n",
        "\n",
        "        [[ 9, 10],\n",
        "         [11, 12]]])\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The original tensors are 2x2 matrices.\n",
        "- The `torch.stack` function stacks them along a new first dimension (`dim=0`), resulting in a 3x2x2 tensor. This means we now have 3 matrices of size 2x2 stacked on top of each other.\n",
        "\n",
        "### Practical Uses in Machine Learning\n",
        "\n",
        "`torch.stack` is often used in machine learning and deep learning when you need to batch together multiple tensors of the same size, such as when combining individual feature tensors into a batch tensor that can be passed to a model. This operation is critical for efficient computation, allowing batch processing of data rather than processing one item at a time."
      ],
      "metadata": {
        "id": "B0FGRMfMS45l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax `(batch_size,)` in Python is used to create a tuple containing a single element, which in this context is the `batch_size`. This notation is necessary because simply writing `batch_size` without the comma and parentheses would be interpreted as an integer, not a tuple. In Python, the trailing comma is what distinguishes a tuple from a regular parenthesis-wrapped expression.\n",
        "\n",
        "### Context in PyTorch:\n",
        "\n",
        "When using PyTorch, especially in functions like `torch.randint`, specifying dimensions as a tuple is common. For example, if you want to generate random integers within a certain range and shape the output as a tensor with dimensions defined by that tuple, you'd use:\n",
        "\n",
        "```python\n",
        "torch.randint(high, (shape_tuple))\n",
        "```\n",
        "\n",
        "- **high**: The upper boundary (exclusive) for the random integers.\n",
        "- **shape_tuple**: A tuple defining the shape of the output tensor.\n",
        "\n",
        "### Example with `torch.randint`:\n",
        "\n",
        "If you have `batch_size = 4` and you write:\n",
        "\n",
        "```python\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "```\n",
        "\n",
        "This code is generating a tensor of shape `(4,)`—meaning it contains four elements. Each element is a random integer between 0 and `len(data) - block_size`. This is particularly useful for selecting starting indices for sequences in data processing, where each index needs to be independently sampled.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "1. **Tuple Notation**: `(batch_size,)` ensures that the argument is passed as a tuple, which is often required by PyTorch functions for defining dimensions.\n",
        "2. **Usage in PyTorch**: Many PyTorch functions that generate or manipulate tensors require dimensions to be specified as tuples to correctly interpret the intended shape of the tensor.\n",
        "\n",
        "This method is widely used in data manipulation and batching processes in machine learning workflows to ensure that tensor operations are applied over the correct dimensions."
      ],
      "metadata": {
        "id": "LVmUbLkgTRi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "5Ef_W8F3P83Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # Number of independent sequences to process simultaneously\n",
        "block_size = 8 # Length of each sequence or the context used for predicting the next item\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data # determines which dataset to use\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # ix: Randomly selects starting indices for sequences within data, avoiding the last block_size\n",
        "                                                              # characters to prevent index overflow.\n",
        "                                                                  # (batch_size,) in the end: explanation found above. It's because it needs a shape_tuple to determine its shape :)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension // iterates over each independent sequences in the batch\n",
        "    for t in range(block_size): # time dimension // iterates over each element within each independent sequence\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUPY2tfEL62W",
        "outputId": "9fa4a375-513b-4b26-cdd3-482dfe1e0521"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W--QK3B9L60O",
        "outputId": "0773a75f-ebe3-4aa6-a47a-cf095dcc214c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note Taking"
      ],
      "metadata": {
        "id": "n6SU4bsiUYT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An **embedding** in the context of machine learning, specifically in natural language processing (NLP), is a representation of data where similar data points are close to each other in a high-dimensional space. More simply, it's a way of converting categorical data, like words or tokens, into vectors of real numbers which are easier to process while retaining semantic meaning.\n",
        "\n",
        "### Key Points about Embeddings:\n",
        "\n",
        "- **Dimensionality Reduction**: While the vocabulary of a language can be vast, embeddings allow each word or token to be represented by a much smaller vector. This is more efficient than one-hot encoding, where the vector length equals the number of words in the vocabulary, with a single '1' and the rest '0's.\n",
        "\n",
        "- **Semantic Information**: Embeddings capture more than just the identity of words. They can also capture semantic relationships between words. For example, synonyms might be positioned closer together in the embedding space.\n",
        "\n",
        "- **Training**: Embeddings can be pre-trained on large text corpora using models like Word2Vec, GloVe, or FastText, or they can be trained from scratch along with the neural network's other parameters tailored to a specific task.\n",
        "\n",
        "### Specifics from Your Code:\n",
        "\n",
        "In your provided code snippet, the embedding layer (`self.token_embedding_table`) maps each token (represented as an integer) to a vector of size equal to the vocabulary size. Each position in the vector represents a potential feature or characteristic of the token, learned during training. Although using an embedding size equal to the vocabulary size is unusual (as it can lead to high dimensionality and overfitting), it might be useful in certain contexts or for experimental purposes.\n",
        "\n",
        "This embedding process is crucial for models to effectively learn and make predictions from textual data, allowing them to handle the vast variability of language with a more manageable and meaningful representation."
      ],
      "metadata": {
        "id": "4-i9Bpz3UYR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query:\n",
        "so in my case im creating a lookup table where each token point to its respective vector in high dimensional space (which is also called embedding?)?\n",
        "\n",
        "and what does nn.Embedding(vocab_size, vocab_size) do? what does the input parameter changes?\n",
        "\n",
        "---\n",
        "\n",
        "Yes, you've got it right! In your case, the embedding layer acts as a lookup table where each token (represented as an integer index) is mapped to its respective vector in a high-dimensional space. This vector space, where each vector represents a token, is indeed referred to as an \"embedding.\"\n",
        "\n",
        "### Breaking Down `nn.Embedding(vocab_size, vocab_size)`\n",
        "\n",
        "The `nn.Embedding` layer in PyTorch is specifically designed to handle such mappings. When you instantiate an `nn.Embedding` with parameters like `(vocab_size, vocab_size)`, you're setting up:\n",
        "\n",
        "1. **Number of Embeddings (`num_embeddings`)**: The first `vocab_size` parameter specifies the number of distinct tokens that the embedding layer can handle. This is essentially the size of your vocabulary.\n",
        "\n",
        "2. **Dimension of Each Embedding Vector (`embedding_dim`)**: The second `vocab_size` parameter sets the size of each embedding vector. In typical use cases, this parameter is much smaller than the vocabulary size because embedding vectors are meant to be a dense, information-rich representation of the tokens.\n",
        "\n",
        "### Function of `nn.Embedding(vocab_size, vocab_size)`\n",
        "By using `vocab_size` for both parameters, you are creating an embedding layer where:\n",
        "- There are as many embedding vectors as there are tokens in your vocabulary.\n",
        "- Each embedding vector has a dimensionality equal to the number of tokens in your vocabulary.\n",
        "\n",
        "### Practical Implications\n",
        "- **Dimensionality**: Normally, the dimension of the embedding vectors (`embedding_dim`) is chosen to be much smaller than the vocabulary size to ensure that the embeddings capture the most important features of the data without being redundantly large. A large embedding dimension (especially equal to the vocabulary size) can lead to overfitting where the model learns the training data too well, including noise and details that do not generalize well to new, unseen data.\n",
        "- **Efficiency**: Large embeddings consume more memory and computational resources, which can make training slower and less efficient.\n",
        "\n",
        "### Example Use\n",
        "A common setup might be something like `nn.Embedding(10000, 300)`, where 10,000 represents a vocabulary of 10,000 words, and each word is represented by a 300-dimensional vector. This setup efficiently compresses information while retaining enough complexity to model linguistic nuances.\n",
        "\n",
        "In your specific case, using `vocab_size` for both might be more experimental or illustrative, helping to explore or demonstrate certain concepts, but it's generally advisable to use a smaller dimension for the embeddings in practical applications."
      ],
      "metadata": {
        "id": "lc6ImIcUVk84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: why do i need to zero_grad first? why not just calculate gradient right away\n",
        "\n",
        "---\n",
        "\n",
        "In PyTorch, when you use `.backward()` to calculate gradients during the training of a neural network, the gradients for each parameter are accumulated by default. This means that each time `.backward()` is executed, gradients from the current pass are added to those already stored from previous backward passes, unless they are explicitly cleared.\n",
        "\n",
        "### Why Zeroing Gradients is Necessary\n",
        "1. **Accumulation by Default**: By design, PyTorch accumulates gradients because this feature supports several advanced optimization tasks, such as training parts of a model (like in transfer learning) or accumulating gradients over multiple batches or sub-batches. However, in standard training loops, you typically want to update weights based on the gradients computed from just the current batch, not a combination of the current and previous batches.\n",
        "\n",
        "2. **Correct Gradient Values**: If you don't reset the gradients, the gradient computations would mix gradients from different batches. This would lead to incorrect updates in the weights, potentially causing the model to converge poorly or diverge.\n",
        "\n",
        "3. **Isolation of Gradient Computation**: Zeroing out the gradients ensures that each batch's gradient computation is independent of others. Each training step's update should reflect only the current batch's data, ensuring that the learning process accurately represents the model's performance on the latest data.\n",
        "\n",
        "### Usage of `optimizer.zero_grad(set_to_none=True)`\n",
        "- **Efficiency**: Setting gradients to `None` (`set_to_none=True`) rather than explicitly zeroing them can be more efficient in some cases. When you set gradients to `None`, PyTorch understands that the gradients need to be recalculated afresh, avoiding the allocation of zero tensors before storing the new gradients. This can reduce some overhead, particularly in terms of memory operations, because you skip the explicit zeroing step and go straight to calculating new gradients.\n",
        "\n",
        "In summary, zeroing out the gradients (or setting them to `None`) before calculating them afresh each time ensures that the updates you make to the model parameters reflect only the most recent data's influence, maintaining the integrity and independence of each training step. This is crucial for the model to learn effectively."
      ],
      "metadata": {
        "id": "9WFJD5EXe4HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "_s7v7_ifUYPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): # inherits from nn.Module <-- PyTorch's base class for all nn modules.\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # first parameter specifies the number of distinct tokens that the embedding layer can handle.\n",
        "                                                                          # or essentially the size of your vocabulary\n",
        "                                                                          # second parameter specifies the size of each embedding vector. This is normally much smaller than vocab size itself\n",
        "                                                                          # because embedding vectors are meant to be a dense, information-rich representation of tokens\n",
        "\n",
        "    def forward(self, idx, targets=None): # method is part of the model's forward pass. Used for traiing + inference to compute the outputs/logits. And optionally loss if targets are provided\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers // B is batch size, and  is sequence length\n",
        "        # targets = optional tensor of same shape as 'idx' containing target indices for training\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C) // C is the embedding dimension\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # uses torch.view to merge batch and sequence length dimensions // preparing it for loss compuation\n",
        "            targets = targets.view(B*T) # simillarly reshapes for loss compuation\n",
        "            loss = F.cross_entropy(logits, targets) # computes cross-entropy loss between logits and training. Used for training the model.\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context // idx is initial sequence of indices to start generating from // max_new_tokens = number of tokens to generate\n",
        "        for _ in range(max_new_tokens): # repeats process for max_new_tokens number of iterations\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx) # model's 'forward' method is called with current indices idx as input\n",
        "            # focus only on the last time step (last layer?)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution // this line samples from probabilitiy distribution \"probs\"\n",
        "                  # torch.multinomial treats each row in probs as a separate distributon and samples num_samples=1 index from each distribution\n",
        "                  # based on probabilities distribution from softmax // multinomial basically just picks options based on how likely they are to be picked\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) # concatenated.\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size) # creates instance of BigramLanguageModel with specified vocabulary size. Which determines how many different tokens the model can handle.\n",
        "logits, loss = m(xb, yb) # runs a forward pass of model. Input batch + target batch inputted.\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())) # calls the 'generate' method of the model to produce a sequence of new\n",
        "                                                                                                       # tokens. This method starts with an initial token. In this case, torch.zeros\n",
        "                                                                                                       # or just 0s, which typically corresponds to a \"start\" token or padding token.\n",
        "\n",
        "                                                                                                       # idx = torch.zeros specifies starting token\n",
        "\n",
        "                                                                                                       # [0].tolist(): The 'generate' method returns a tensor, where the 1st dimension\n",
        "                                                                                                       # is batch size (1 in this case). The [0] selects the first sequence form batch.\n",
        "                                                                                                       # And toList() converts the tensor of token indices to a list of integers.\n",
        "\n",
        "                                                                                                       # decode() is defined previously."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txFa8CFJL6yT",
        "outputId": "9b18501c-5321-4bf5-d645-11d76d94a44f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "mAYpDRV8L6wP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # sets the size of each batch to 32. But variable isn't used here.\n",
        "for steps in range(100): # increase number of steps for good results... // below is training loop.\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train') # retrieves a batch of training data (previously defined function)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True) # resets gradients of all model parameter to 0 before backpropagation. \"set_to_none=True\" arg optimises this process by setting gradients to\n",
        "                                          # 'None' instead of zeroing them. Which can be more efficient\n",
        "    loss.backward() # performs backpropagation to compute gradient of loss with respect to model parameters\n",
        "    optimizer.step() # updates the model's parameters using the computed gradients.\n",
        "\n",
        "print(loss.item()) # .item() converts a single-value tensor to a Python number."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXfymlxybAyr",
        "outputId": "9eef2051-2b6d-42ce-bac3-1de0f309224d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.587916374206543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())) # regenerating again. 500 tokens this time instead. way better!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYJHvUGcbAxO",
        "outputId": "93076736-d9ae-4368-eb4d-98dec65e2f49"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
            "3Q&sGlvHQ?mqSq-eON\n",
            "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
            "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
            "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
            "&WDdP!Ko,px\n",
            "x\n",
            "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
            "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "IJX0vyLIfa8B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDnRLfAfbAvs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLizI4z6bAuN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ybyNtStbAsp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6ePFAOqbArJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n29BN6UebAp8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTVZwOMYbAn-"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}