{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "vpz_PjV5YYSK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lLIdlN5gJTlu",
        "outputId": "2b98264d-d103-4d6a-a593-c1d242b461de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.0\n",
            "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.1+cu121\n",
            "    Uninstalling torchaudio-2.3.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 triton-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYq5ZhzOJtSx",
        "outputId": "01dc9598-1c13-4c8b-eda4-508a6ffc193d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-26 14:15:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-07-26 14:15:05 (17.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "pq2Bp6IdJzDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRVszaUrLq7s",
        "outputId": "28c292af-7eb0-44b5-f42e-da78fb952083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QbEtTGlLq5P",
        "outputId": "94e2e8ef-881e-461f-b05c-da731cfa1160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This block identifies all unique character in string or tet stored in variable 'text'\n",
        "chars = sorted(list(set(text))) # Extracts unique characters from the string text, converts the set to a list, and sorts it alphabetically.\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars)) # prints sorted list\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56OK0KWkLq3H",
        "outputId": "713435e2-9228-49c1-a0fe-e52172591c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers + defines function for encoding and decoding strings\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # create a dictionary mapping characters to integers // i is index, ch is character in enumerate (this is fashion of enumerate)\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CbWXBiPLq1A",
        "outputId": "2cfe2491-e51a-4a50-c742-e797c01d9a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the first 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V60tyBgDLqy5",
        "outputId": "1e25aa88-f310-4d6f-e6fd-3ff3584345a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "myMh6RYkLqw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn3olFVRLquj",
        "outputId": "51104a6e-fdaa-4f06-81bc-e2fe03d9820a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # x represents the input sequence // takes the first block_size number of elements\n",
        "y = train_data[1:block_size+1] # takes elements from 2nd elements up to block_size+1 number of elements // is essentially x but shifted one position to the right\n",
        "                               # This is used as the target sequence for training. Where each element in y is the sequence that model should predict after seeing sequence in 'x'\n",
        "\n",
        "for t in range(block_size): # This for loop iterates over each position in the block_size, effectively allowing you to demonstrate or train the model on increasing amounts of context.\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")\n",
        "\n",
        "# note: as seen below, when provided a sequence in training set. The model actually tries to train the data batch as just [1] to [9], to predict the\n",
        "#       token. So that the model are able to capture semantic meaning better, and also is able to deal with small inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOoMmCZgL64z",
        "outputId": "8e785d63-1d2c-45c7-d20d-82237839a0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Notes"
      ],
      "metadata": {
        "id": "18ObmqafP6zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: What is byte-pair encoding?\n",
        "\n",
        "Byte-Pair Encoding (BPE) is a data compression technique originally designed to efficiently compress text data. It has been adapted in natural language processing (NLP) to perform subword tokenization, which involves breaking down words into more manageable units (subwords) for models to process. This technique is especially useful in handling languages with rich morphology or in managing the vocabulary size in neural network models.\n",
        "\n",
        "### How Byte-Pair Encoding Works in NLP\n",
        "\n",
        "1. **Initialization**: Start with a vocabulary of individual characters or other base units (like bytes in its original application) from the training data.\n",
        "\n",
        "2. **Frequency Count**: Count the frequency of pairs of adjacent symbols or characters in the text data.\n",
        "\n",
        "3. **Merge Operations**: Identify the most frequently occurring pair of symbols and merge them into a single new symbol. This merged symbol is then added to the vocabulary.\n",
        "\n",
        "4. **Iterative Process**: Repeat the merging process for a predetermined number of steps or until some criterion is met (like a desired vocabulary size). Each iteration merges the next most frequent pair.\n",
        "\n",
        "5. **Tokenization**: Once the vocabulary of symbols (including merged symbols) is created, tokenize the text by greedily replacing the longest possible sequences in the text with the corresponding symbols in the vocabulary.\n",
        "\n",
        "### Advantages of BPE in NLP\n",
        "\n",
        "- **Efficiency in Unknown Words**: BPE allows a model to handle words not seen during training, by breaking them into known subwords. This is particularly useful in languages with compounding or agglutination where new word forms are frequent.\n",
        "\n",
        "- **Balance Between Character and Word Level**: BPE provides a balance between character-level and word-level representations. It avoids the sparsity of character-level processing and the vocabulary explosion of word-level processing.\n",
        "\n",
        "- **Contextual Awareness**: Subwords provide more contextual information than individual characters, helping models understand suffixes, prefixes, and roots, which are crucial for semantic and syntactic understanding.\n",
        "\n",
        "### Usage in Modern NLP\n",
        "\n",
        "BPE has been employed in several state-of-the-art NLP models and frameworks, notably in models like GPT (from OpenAI), BERT, and other Transformer-based architectures. These models use BPE or its variants (like SentencePiece or WordPiece) to preprocess text into subwords, which are then used for tasks like translation, text summarization, and question-answering.\n",
        "\n",
        "Overall, BPE is a pivotal component in the preprocessing pipeline for many NLP tasks, helping to improve both the efficiency and effectiveness of models in handling diverse and complex language data."
      ],
      "metadata": {
        "id": "QhMuDwmUPkBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `torch.stack` in PyTorch is used to concatenate a sequence of tensors along a new dimension. This is different from functions like `torch.cat`, which concatenate tensors along an existing dimension. `torch.stack` effectively combines tensors in such a way that the resulting tensor has an additional dimension compared to the input tensors.\n",
        "\n",
        "### Usage of `torch.stack`\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "torch.stack(tensors, dim=0)\n",
        "```\n",
        "\n",
        "- **tensors**: a sequence of tensors to concatenate. All tensors must have the same shape.\n",
        "- **dim**: the dimension along which to concatenate the tensors. This dimension is added to the resulting tensor, and each tensor in the sequence will be sliced along this new dimension.\n",
        "\n",
        "### Example\n",
        "\n",
        "Let's consider you have three 2x2 tensors:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Define three 2x2 tensors\n",
        "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
        "tensor3 = torch.tensor([[9, 10], [11, 12]])\n",
        "\n",
        "# Stack these tensors along a new dimension\n",
        "result = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "tensor([[[ 1,  2],\n",
        "         [ 3,  4]],\n",
        "\n",
        "        [[ 5,  6],\n",
        "         [ 7,  8]],\n",
        "\n",
        "        [[ 9, 10],\n",
        "         [11, 12]]])\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The original tensors are 2x2 matrices.\n",
        "- The `torch.stack` function stacks them along a new first dimension (`dim=0`), resulting in a 3x2x2 tensor. This means we now have 3 matrices of size 2x2 stacked on top of each other.\n",
        "\n",
        "### Practical Uses in Machine Learning\n",
        "\n",
        "`torch.stack` is often used in machine learning and deep learning when you need to batch together multiple tensors of the same size, such as when combining individual feature tensors into a batch tensor that can be passed to a model. This operation is critical for efficient computation, allowing batch processing of data rather than processing one item at a time."
      ],
      "metadata": {
        "id": "B0FGRMfMS45l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax `(batch_size,)` in Python is used to create a tuple containing a single element, which in this context is the `batch_size`. This notation is necessary because simply writing `batch_size` without the comma and parentheses would be interpreted as an integer, not a tuple. In Python, the trailing comma is what distinguishes a tuple from a regular parenthesis-wrapped expression.\n",
        "\n",
        "### Context in PyTorch:\n",
        "\n",
        "When using PyTorch, especially in functions like `torch.randint`, specifying dimensions as a tuple is common. For example, if you want to generate random integers within a certain range and shape the output as a tensor with dimensions defined by that tuple, you'd use:\n",
        "\n",
        "```python\n",
        "torch.randint(high, (shape_tuple))\n",
        "```\n",
        "\n",
        "- **high**: The upper boundary (exclusive) for the random integers.\n",
        "- **shape_tuple**: A tuple defining the shape of the output tensor.\n",
        "\n",
        "### Example with `torch.randint`:\n",
        "\n",
        "If you have `batch_size = 4` and you write:\n",
        "\n",
        "```python\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "```\n",
        "\n",
        "This code is generating a tensor of shape `(4,)`—meaning it contains four elements. Each element is a random integer between 0 and `len(data) - block_size`. This is particularly useful for selecting starting indices for sequences in data processing, where each index needs to be independently sampled.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "1. **Tuple Notation**: `(batch_size,)` ensures that the argument is passed as a tuple, which is often required by PyTorch functions for defining dimensions.\n",
        "2. **Usage in PyTorch**: Many PyTorch functions that generate or manipulate tensors require dimensions to be specified as tuples to correctly interpret the intended shape of the tensor.\n",
        "\n",
        "This method is widely used in data manipulation and batching processes in machine learning workflows to ensure that tensor operations are applied over the correct dimensions."
      ],
      "metadata": {
        "id": "LVmUbLkgTRi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "5Ef_W8F3P83Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # Number of independent sequences to process simultaneously\n",
        "block_size = 8 # Length of each sequence or the context used for predicting the next item\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data # determines which dataset to use\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # ix: Randomly selects starting indices for sequences within data, avoiding the last block_size\n",
        "                                                              # characters to prevent index overflow.\n",
        "                                                                  # (batch_size,) in the end: explanation found above. It's because it needs a shape_tuple to determine its shape :)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension // iterates over each independent sequences in the batch\n",
        "    for t in range(block_size): # time dimension // iterates over each element within each independent sequence\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUPY2tfEL62W",
        "outputId": "3c27dcc3-fa91-4d9a-f680-f998483cbf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W--QK3B9L60O",
        "outputId": "8faa1860-2c72-4aee-bbe4-99bb2b60095c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note Taking"
      ],
      "metadata": {
        "id": "n6SU4bsiUYT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An **embedding** in the context of machine learning, specifically in natural language processing (NLP), is a representation of data where similar data points are close to each other in a high-dimensional space. More simply, it's a way of converting categorical data, like words or tokens, into vectors of real numbers which are easier to process while retaining semantic meaning.\n",
        "\n",
        "### Key Points about Embeddings:\n",
        "\n",
        "- **Dimensionality Reduction**: While the vocabulary of a language can be vast, embeddings allow each word or token to be represented by a much smaller vector. This is more efficient than one-hot encoding, where the vector length equals the number of words in the vocabulary, with a single '1' and the rest '0's.\n",
        "\n",
        "- **Semantic Information**: Embeddings capture more than just the identity of words. They can also capture semantic relationships between words. For example, synonyms might be positioned closer together in the embedding space.\n",
        "\n",
        "- **Training**: Embeddings can be pre-trained on large text corpora using models like Word2Vec, GloVe, or FastText, or they can be trained from scratch along with the neural network's other parameters tailored to a specific task.\n",
        "\n",
        "### Specifics from Your Code:\n",
        "\n",
        "In your provided code snippet, the embedding layer (`self.token_embedding_table`) maps each token (represented as an integer) to a vector of size equal to the vocabulary size. Each position in the vector represents a potential feature or characteristic of the token, learned during training. Although using an embedding size equal to the vocabulary size is unusual (as it can lead to high dimensionality and overfitting), it might be useful in certain contexts or for experimental purposes.\n",
        "\n",
        "This embedding process is crucial for models to effectively learn and make predictions from textual data, allowing them to handle the vast variability of language with a more manageable and meaningful representation."
      ],
      "metadata": {
        "id": "4-i9Bpz3UYR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query:\n",
        "so in my case im creating a lookup table where each token point to its respective vector in high dimensional space (which is also called embedding?)?\n",
        "\n",
        "and what does nn.Embedding(vocab_size, vocab_size) do? what does the input parameter changes?\n",
        "\n",
        "---\n",
        "\n",
        "Yes, you've got it right! In your case, the embedding layer acts as a lookup table where each token (represented as an integer index) is mapped to its respective vector in a high-dimensional space. This vector space, where each vector represents a token, is indeed referred to as an \"embedding.\"\n",
        "\n",
        "### Breaking Down `nn.Embedding(vocab_size, vocab_size)`\n",
        "\n",
        "The `nn.Embedding` layer in PyTorch is specifically designed to handle such mappings. When you instantiate an `nn.Embedding` with parameters like `(vocab_size, vocab_size)`, you're setting up:\n",
        "\n",
        "1. **Number of Embeddings (`num_embeddings`)**: The first `vocab_size` parameter specifies the number of distinct tokens that the embedding layer can handle. This is essentially the size of your vocabulary.\n",
        "\n",
        "2. **Dimension of Each Embedding Vector (`embedding_dim`)**: The second `vocab_size` parameter sets the size of each embedding vector. In typical use cases, this parameter is much smaller than the vocabulary size because embedding vectors are meant to be a dense, information-rich representation of the tokens.\n",
        "\n",
        "### Function of `nn.Embedding(vocab_size, vocab_size)`\n",
        "By using `vocab_size` for both parameters, you are creating an embedding layer where:\n",
        "- There are as many embedding vectors as there are tokens in your vocabulary.\n",
        "- Each embedding vector has a dimensionality equal to the number of tokens in your vocabulary.\n",
        "\n",
        "### Practical Implications\n",
        "- **Dimensionality**: Normally, the dimension of the embedding vectors (`embedding_dim`) is chosen to be much smaller than the vocabulary size to ensure that the embeddings capture the most important features of the data without being redundantly large. A large embedding dimension (especially equal to the vocabulary size) can lead to overfitting where the model learns the training data too well, including noise and details that do not generalize well to new, unseen data.\n",
        "- **Efficiency**: Large embeddings consume more memory and computational resources, which can make training slower and less efficient.\n",
        "\n",
        "### Example Use\n",
        "A common setup might be something like `nn.Embedding(10000, 300)`, where 10,000 represents a vocabulary of 10,000 words, and each word is represented by a 300-dimensional vector. This setup efficiently compresses information while retaining enough complexity to model linguistic nuances.\n",
        "\n",
        "In your specific case, using `vocab_size` for both might be more experimental or illustrative, helping to explore or demonstrate certain concepts, but it's generally advisable to use a smaller dimension for the embeddings in practical applications."
      ],
      "metadata": {
        "id": "lc6ImIcUVk84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: why do i need to zero_grad first? why not just calculate gradient right away\n",
        "\n",
        "---\n",
        "\n",
        "In PyTorch, when you use `.backward()` to calculate gradients during the training of a neural network, the gradients for each parameter are accumulated by default. This means that each time `.backward()` is executed, gradients from the current pass are added to those already stored from previous backward passes, unless they are explicitly cleared.\n",
        "\n",
        "### Why Zeroing Gradients is Necessary\n",
        "1. **Accumulation by Default**: By design, PyTorch accumulates gradients because this feature supports several advanced optimization tasks, such as training parts of a model (like in transfer learning) or accumulating gradients over multiple batches or sub-batches. However, in standard training loops, you typically want to update weights based on the gradients computed from just the current batch, not a combination of the current and previous batches.\n",
        "\n",
        "2. **Correct Gradient Values**: If you don't reset the gradients, the gradient computations would mix gradients from different batches. This would lead to incorrect updates in the weights, potentially causing the model to converge poorly or diverge.\n",
        "\n",
        "3. **Isolation of Gradient Computation**: Zeroing out the gradients ensures that each batch's gradient computation is independent of others. Each training step's update should reflect only the current batch's data, ensuring that the learning process accurately represents the model's performance on the latest data.\n",
        "\n",
        "### Usage of `optimizer.zero_grad(set_to_none=True)`\n",
        "- **Efficiency**: Setting gradients to `None` (`set_to_none=True`) rather than explicitly zeroing them can be more efficient in some cases. When you set gradients to `None`, PyTorch understands that the gradients need to be recalculated afresh, avoiding the allocation of zero tensors before storing the new gradients. This can reduce some overhead, particularly in terms of memory operations, because you skip the explicit zeroing step and go straight to calculating new gradients.\n",
        "\n",
        "In summary, zeroing out the gradients (or setting them to `None`) before calculating them afresh each time ensures that the updates you make to the model parameters reflect only the most recent data's influence, maintaining the integrity and independence of each training step. This is crucial for the model to learn effectively."
      ],
      "metadata": {
        "id": "9WFJD5EXe4HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "_s7v7_ifUYPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): # inherits from nn.Module <-- PyTorch's base class for all nn modules.\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # first parameter specifies the number of distinct tokens that the embedding layer can handle.\n",
        "                                                                          # or essentially the size of your vocabulary\n",
        "                                                                          # second parameter specifies the size of each embedding vector. This is normally much smaller than vocab size itself\n",
        "                                                                          # because embedding vectors are meant to be a dense, information-rich representation of tokens\n",
        "\n",
        "    def forward(self, idx, targets=None): # method is part of the model's forward pass. Used for traiing + inference to compute the outputs/logits. And optionally loss if targets are provided\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers // B is batch size, and  is sequence length\n",
        "        # targets = optional tensor of same shape as 'idx' containing target indices for training\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C) // C is the embedding dimension\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # uses torch.view to merge batch and sequence length dimensions // preparing it for loss compuation\n",
        "            targets = targets.view(B*T) # simillarly reshapes for loss compuation\n",
        "            loss = F.cross_entropy(logits, targets) # computes cross-entropy loss between logits and training. Used for training the model.\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context // idx is initial sequence of indices to start generating from // max_new_tokens = number of tokens to generate\n",
        "        for _ in range(max_new_tokens): # repeats process for max_new_tokens number of iterations\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx) # model's 'forward' method is called with current indices idx as input\n",
        "            # focus only on the last time step (last layer?)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution // this line samples from probabilitiy distribution \"probs\"\n",
        "                  # torch.multinomial treats each row in probs as a separate distributon and samples num_samples=1 index from each distribution\n",
        "                  # based on probabilities distribution from softmax // multinomial basically just picks options based on how likely they are to be picked\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) # concatenated.\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size) # creates instance of BigramLanguageModel with specified vocabulary size. Which determines how many different tokens the model can handle.\n",
        "logits, loss = m(xb, yb) # runs a forward pass of model. Input batch + target batch inputted.\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())) # calls the 'generate' method of the model to produce a sequence of new\n",
        "                                                                                                       # tokens. This method starts with an initial token. In this case, torch.zeros\n",
        "                                                                                                       # or just 0s, which typically corresponds to a \"start\" token or padding token.\n",
        "\n",
        "                                                                                                       # idx = torch.zeros specifies starting token\n",
        "\n",
        "                                                                                                       # [0].tolist(): The 'generate' method returns a tensor, where the 1st dimension\n",
        "                                                                                                       # is batch size (1 in this case). The [0] selects the first sequence form batch.\n",
        "                                                                                                       # And toList() converts the tensor of token indices to a list of integers.\n",
        "\n",
        "                                                                                                       # decode() is defined previously."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txFa8CFJL6yT",
        "outputId": "90a7c0f4-4d46-40c0-a084-7ea50f5ec6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "mAYpDRV8L6wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # sets the size of each batch to 32. But variable isn't used here.\n",
        "for steps in range(100): # increase number of steps for good results... // below is training loop.\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train') # retrieves a batch of training data (previously defined function)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True) # resets gradients of all model parameter to 0 before backpropagation. \"set_to_none=True\" arg optimises this process by setting gradients to\n",
        "                                          # 'None' instead of zeroing them. Which can be more efficient\n",
        "    loss.backward() # performs backpropagation to compute gradient of loss with respect to model parameters\n",
        "    optimizer.step() # updates the model's parameters using the computed gradients.\n",
        "\n",
        "print(loss.item()) # .item() converts a single-value tensor to a Python number."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXfymlxybAyr",
        "outputId": "e26a3ceb-a516-4fe8-a996-f8d4a543124f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.587916374206543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())) # regenerating again. 500 tokens this time instead. way better!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYJHvUGcbAxO",
        "outputId": "d0dca081-5cb9-47af-9dd7-53e50989bd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
            "3Q&sGlvHQ?mqSq-eON\n",
            "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
            "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
            "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
            "&WDdP!Ko,px\n",
            "x\n",
            "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
            "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "IJX0vyLIfa8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "vpz_PjV5YYSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's break down the code snippet step by step to understand its functionality. This snippet demonstrates the use of matrix operations to achieve weighted aggregation, which is foundational in the mechanism of self-attention.\n",
        "\n",
        "### Code Breakdown\n",
        "\n",
        "```python\n",
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "```\n",
        "\n",
        "1. **Set Random Seed:**\n",
        "   ```python\n",
        "   torch.manual_seed(42)\n",
        "   ```\n",
        "   - This line sets the seed for generating random numbers, ensuring reproducibility of the random results (such as tensor values) for debugging and testing purposes.\n",
        "\n",
        "2. **Create Matrix `a`:**\n",
        "   ```python\n",
        "   a = torch.tril(torch.ones(3, 3))\n",
        "   ```\n",
        "   - `torch.tril`: Generates a lower triangular matrix from a 3x3 matrix of ones. The result is a matrix where all elements above the diagonal are zero, and those on and below the diagonal are one.\n",
        "   - Example output of `a` at this point:\n",
        "     ```\n",
        "     [[1., 0., 0.],\n",
        "      [1., 1., 0.],\n",
        "      [1., 1., 1.]]\n",
        "     ```\n",
        "\n",
        "3. **Normalize Matrix `a`:**\n",
        "   ```python\n",
        "   a = a / torch.sum(a, 1, keepdim=True)\n",
        "   ```\n",
        "   - This line normalizes each row of the matrix `a` so that the sum of each row equals 1. This is achieved by dividing each element in a row by the sum of the elements in that row (`torch.sum(a, 1, keepdim=True)` calculates the sum of each row and retains the original number of dimensions for broadcasting).\n",
        "   - Post-normalization, matrix `a` serves as a set of weights for aggregation, with each row summing to 1.\n",
        "\n",
        "4. **Create Matrix `b`:**\n",
        "   ```python\n",
        "   b = torch.randint(0,10,(3,2)).float()\n",
        "   ```\n",
        "   - Generates a 3x2 matrix with random integers between 0 and 9, then converts these integers to floating-point numbers. This matrix `b` might represent data points or features that will be aggregated.\n",
        "\n",
        "5. **Matrix Multiplication (`a @ b`):**\n",
        "   ```python\n",
        "   c = a @ b\n",
        "   ```\n",
        "   - Performs matrix multiplication between the normalized weight matrix `a` and the data matrix `b`. This operation aggregates data in `b` using the weighted sums specified in `a`. The `(i, j)` element of the result matrix `c` is the weighted sum of the elements in the `j`-th column of `b`, using the weights from the `i`-th row of `a`.\n",
        "\n",
        "6. **Printing Results:**\n",
        "   ```python\n",
        "   print('a=')\n",
        "   print(a)\n",
        "   print('--')\n",
        "   print('b=')\n",
        "   print(b)\n",
        "   print('--')\n",
        "   print('c=')\n",
        "   print(c)\n",
        "   ```\n",
        "   - These lines print the matrices `a`, `b`, and the result `c` to show the input weights, the data being aggregated, and the outcome of the weighted aggregation, respectively.\n",
        "\n",
        "### Summary\n",
        "This code illustrates a fundamental operation in neural networks, especially in mechanisms like self-attention where weighted sums are computed as a combination of elements. The use of normalization and matrix multiplication here is a simple yet powerful demonstration of how weights can control the influence of different elements (or time steps in sequences) when aggregating data."
      ],
      "metadata": {
        "id": "ZDoo_26zYYPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: not very clear with this one. show me an example of how does this work\n",
        "\n",
        "can you explain to me what is the code here meant to do overall\n",
        "\n",
        "---\n",
        "\n",
        "Let's clarify how the normalization of matrix `a` and the subsequent matrix multiplication with matrix `b` works in the given code by walking through a detailed example. This will demonstrate the concept of weighted aggregation, which is a fundamental operation in many neural network architectures, including transformers for self-attention.\n",
        "\n",
        "### Detailed Example with Matrix `a` and `b`\n",
        "\n",
        "#### 1. Initialization and Normalization of Matrix `a`\n",
        "Matrix `a` is initially created as a lower triangular matrix of ones. This kind of matrix will have ones on and below the diagonal, and zeros above it. Here's what it looks like initially:\n",
        "\n",
        "```\n",
        "[[1, 0, 0],\n",
        " [1, 1, 0],\n",
        " [1, 1, 1]]\n",
        "```\n",
        "\n",
        "Normalization means adjusting the rows of `a` such that each row sums to 1. This makes each row of `a` represent a set of weights for aggregating elements of `b`. The normalization is performed by dividing each element of a row by the sum of that row:\n",
        "\n",
        "- Row 1: \\( [1, 0, 0] \\) sums to 1. \\( \\frac{[1, 0, 0]}{1} = [1, 0, 0] \\)\n",
        "- Row 2: \\( [1, 1, 0] \\) sums to 2. \\( \\frac{[1, 1, 0]}{2} = [0.5, 0.5, 0] \\)\n",
        "- Row 3: \\( [1, 1, 1] \\) sums to 3. \\( \\frac{[1, 1, 1]}{3} = [0.33, 0.33, 0.33] \\)\n",
        "\n",
        "Normalized matrix `a`:\n",
        "```\n",
        "[[1.00, 0.00, 0.00],\n",
        " [0.50, 0.50, 0.00],\n",
        " [0.33, 0.33, 0.33]]\n",
        "```\n",
        "\n",
        "#### 2. Creation of Matrix `b`\n",
        "Suppose matrix `b` is randomly generated as:\n",
        "```\n",
        "[[3, 7],\n",
        " [2, 1],\n",
        " [5, 4]]\n",
        "```\n",
        "\n",
        "#### 3. Matrix Multiplication (`a @ b`)\n",
        "The operation `a @ b` performs a matrix multiplication where each element of the result matrix `c` is a weighted sum of elements from `b`, with weights defined by `a`.\n",
        "\n",
        "- **First row of `c`**: Only the first element of `b` contributes because all weights for other elements are zero (due to `a`).\n",
        "  - \\( c[0] = 1.0 \\times [3, 7] + 0.0 \\times [2, 1] + 0.0 \\times [5, 4] = [3, 7] \\)\n",
        "- **Second row of `c`**: Elements from the first two rows of `b` contribute equally.\n",
        "  - \\( c[1] = 0.5 \\times [3, 7] + 0.5 \\times [2, 1] = [2.5, 4] \\)\n",
        "- **Third row of `c`**: All elements of `b` contribute equally.\n",
        "  - \\( c[2] = 0.33 \\times [3, 7] + 0.33 \\times [2, 1] + 0.33 \\times [5, 4] = [3.33, 4] \\)\n",
        "\n",
        "Resultant matrix `c`:\n",
        "```\n",
        "[[3.00, 7.00],\n",
        " [2.50, 4.00],\n",
        " [3.33, 4.00]]\n",
        "```\n",
        "\n",
        "### Overall Purpose\n",
        "The operation demonstrated in the code uses the concept of **weighted aggregation** where matrix `a` serves as a set of weights to combine or blend elements of matrix `b`. This kind of operation is fundamental in neural networks, particularly in mechanisms like attention where inputs are combined based on learned or predefined importance weights. Here, it simplifies to averaging or summing with weights that consider positional relations (like in the lower triangular form, which could mimic causality or sequence in data)."
      ],
      "metadata": {
        "id": "SHEI5ee9aqOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: explain to me each of the parameter here. and please go into more detail what weighted aggregation is\n",
        "\n",
        "sum(a, 1, keepdim=True)\n",
        "\n",
        "---\n",
        "\n",
        "The function `torch.sum(a, 1, keepdim=True)` from PyTorch is used to sum elements of a tensor along a specified dimension, with options to maintain the number of dimensions in the output. This function is key in various tensor manipulations, especially in operations like normalization where relative scaling of tensor elements based on their sums is required.\n",
        "\n",
        "### Parameters Explained:\n",
        "\n",
        "- **a**: This is the tensor on which the summation operation is performed. In your context, `a` is the lower triangular matrix of ones that has been modified in some way (like normalizing each row).\n",
        "\n",
        "- **1**: This is the dimension index over which to perform the summation. PyTorch tensors are zero-indexed, so an index of `1` refers to the second dimension of the tensor. In a 2D matrix, this would be summing across rows (i.e., summing elements within each column of a row). For the matrix `a`, which is of size `[3, 3]`, summing over dimension `1` results in summing all elements across each row.\n",
        "\n",
        "- **keepdim=True**: This parameter decides whether the output tensor should retain the number of dimensions as the input tensor. If set to `True`, the resulting tensor will have the same number of dimensions as the input tensor, but with size `1` along the summed dimension. If it were `False`, the summed dimension would be removed from the output, reducing the dimensionality of the result.\n",
        "\n",
        "### Example:\n",
        "Consider the matrix `a`:\n",
        "```\n",
        "[[1, 0, 0],\n",
        " [1, 1, 0],\n",
        " [1, 1, 1]]\n",
        "```\n",
        "Running `torch.sum(a, 1, keepdim=True)` on this matrix would produce:\n",
        "```\n",
        "[[1],\n",
        " [2],\n",
        " [3]]\n",
        "```\n",
        "Here’s what happened:\n",
        "- For the first row, only the first element is non-zero, so the sum is `1`.\n",
        "- For the second row, the sum of `1 + 1` is `2`.\n",
        "- For the third row, the sum of `1 + 1 + 1` is `3`.\n",
        "\n",
        "The output retains its two-dimensional structure due to `keepdim=True`, but each row's sum is encapsulated in its own row, facilitating direct division for normalization purposes without needing to reshape.\n",
        "\n",
        "### Weighted Aggregation Explained:\n",
        "Weighted aggregation refers to the process of combining multiple elements into a single value where each element contributes according to a specified weight. This is a fundamental concept in data processing, statistics, and machine learning, particularly in fields like neural networks where it forms the basis of operations like calculating weighted averages, performing convolutions, and, more complexly, computing attention mechanisms in models like transformers.\n",
        "\n",
        "In the context of the matrix `a` and `b` from your code:\n",
        "- **Matrix `a`** acts as the weight matrix where each row's values determine the relative contribution of corresponding elements in matrix `b` during the aggregation process.\n",
        "- **Matrix `b`** contains data points or features to be aggregated.\n",
        "\n",
        "When you perform `a @ b` (matrix multiplication between `a` and `b`), you effectively apply the weighted aggregation:\n",
        "- Each row in `a` specifies how to combine elements of `b` into a single row in the resulting matrix `c`.\n",
        "- Each entry in the result is a dot product of a row from `a` and a column from `b`, which mathematically represents a weighted sum where weights are given by `a`.\n",
        "\n",
        "This process allows the model to highlight or diminish the influence of certain features dynamically, based on the training or the specific operation's goals, such as focusing on more recent information in sequences or blending features in image processing."
      ],
      "metadata": {
        "id": "t9Ez2AA1aqM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: oh wait. is it trying to give all the 1 values equal attention?\n",
        "\n",
        "ah i see. and by relying on matrix multiplication. if the value there is 0. or no token yet. then nothing is there. fair enough.\n",
        "\n",
        "---\n",
        "\n",
        "Yes, exactly! Your understanding is spot-on. Let's dive a bit deeper into these concepts:\n",
        "\n",
        "### Equal Attention and Matrix Weights\n",
        "\n",
        "The matrix `a`, after normalization, gives equal weight to all \"1\" values within each row, provided they exist. This is a way of distributing attention or influence equally across all available values or tokens up to that point in the sequence. For each row in `a`, the \"1\" values are adjusted to sum up to one, distributing equal attention to each existing token (non-zero entries). Here's what it does:\n",
        "\n",
        "- **Uniform Distribution of Influence**: When each row of `a` is normalized such that the sum equals 1, it means each non-zero entry (or each past token, if you relate it to sequences) contributes equally to the resultant value during the matrix multiplication. This ensures that there's no undue bias towards any particular position within the limits set by the triangular structure.\n",
        "\n",
        "- **Handling Zeros**: The zeros in `a` effectively ensure that no attention or weight is given to future tokens (entries above the diagonal), adhering to a causal structure. This is crucial in models where the future cannot depend on past information, such as when generating text or processing time-series data in a forward-only manner.\n",
        "\n",
        "### Matrix Multiplication and Its Role\n",
        "\n",
        "Matrix multiplication (`a @ b`) leverages the structure and values of `a` to aggregate information in `b`. Here’s what happens:\n",
        "\n",
        "- **If a Row in `a` has a \"0\"**: Any position in `b` corresponding to a zero in `a` does not contribute to the output in `c`. This aligns with the principle that only available (past) tokens should influence the current output, mirroring scenarios in sequence processing where you only want to consider previously seen or available data up to the current time step.\n",
        "\n",
        "- **Efficient Aggregation**: By performing matrix multiplication, you’re efficiently aggregating data across possibly large matrices in a highly optimized manner. This is far more computationally efficient than manually iterating through each element, especially with hardware accelerators like GPUs.\n",
        "\n",
        "### Practical Implications in Models\n",
        "\n",
        "This kind of weighted aggregation, particularly with a structured matrix like a lower triangular one, is a fundamental concept in many deep learning architectures, such as RNNs and Transformers:\n",
        "\n",
        "- **Recurrent Neural Networks (RNNs)**: They inherently process sequences by considering past information up to the current step, often manually managing how past states influence the current state.\n",
        "\n",
        "- **Transformers and Self-Attention**: They use a more sophisticated form of this concept where the attention mechanism dynamically determines weights (`a` matrix) based on the content and relationship between sequences, often without the strict triangular limitation unless specifically modeling causality (as in GPT-style architectures).\n",
        "\n",
        "Understanding these matrix operations and their implications helps in grasping more advanced topics in deep learning, especially in understanding how data flows and is processed within neural network layers."
      ],
      "metadata": {
        "id": "zJLWlfP2aqLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: show me what happens in this case when keepdim=False\n",
        "\n",
        "---\n",
        "\n",
        "Setting `keepdim=False` in the `torch.sum()` function affects how the output is shaped after the summation operation. This parameter influences whether the summed dimension is retained as a singleton dimension.\n",
        "\n",
        "### Example to Illustrate `keepdim=False`\n",
        "\n",
        "Let’s use a simple example to demonstrate the effect of setting `keepdim=False`. Consider a matrix `a` as follows:\n",
        "\n",
        "```\n",
        "a = [[1, 0, 0],\n",
        "     [1, 1, 0],\n",
        "     [1, 1, 1]]\n",
        "```\n",
        "\n",
        "#### Summation with `keepdim=False`\n",
        "Using `torch.sum(a, 1, keepdim=False)`, we sum across each row (dimension 1) but do not keep the dimension:\n",
        "\n",
        "```python\n",
        "a_sum = torch.sum(a, 1, keepdim=False)\n",
        "```\n",
        "For the matrix `a`:\n",
        "- The sum of the first row: `1 + 0 + 0 = 1`\n",
        "- The sum of the second row: `1 + 1 + 0 = 2`\n",
        "- The sum of the third row: `1 + 1 + 1 = 3`\n",
        "\n",
        "The resulting tensor `a_sum` will be:\n",
        "```\n",
        "[1, 2, 3]\n",
        "```\n",
        "\n",
        "#### Output Shape\n",
        "The shape of `a_sum` will be `[3]`, a one-dimensional tensor (vector), because we collapsed the row dimension by summing over it and did not keep it in the output.\n",
        "\n",
        "#### Contrast with `keepdim=True`\n",
        "If we had used `keepdim=True`:\n",
        "```python\n",
        "a_sum_keepdim = torch.sum(a, 1, keepdim=True)\n",
        "```\n",
        "The result would have been:\n",
        "```\n",
        "[[1],\n",
        " [2],\n",
        " [3]]\n",
        "```\n",
        "Here, the output retains its two-dimensional structure, but the summed dimension is reduced to a single element, maintaining compatibility for subsequent operations that might rely on tensor dimensions, such as broadcasting in matrix operations.\n",
        "\n",
        "### Practical Implication\n",
        "When `keepdim=False` is used, the resulting tensor after a summation operation loses a dimension. This is useful when you want a flattened or reduced result directly without extra singleton dimensions. However, this might require additional reshaping if the tensor needs to be used in subsequent operations expecting a specific number of dimensions.\n",
        "\n",
        "For operations that involve normalization or weighted computations (like in your matrix `a`), keeping dimensions (using `keepdim=True`) simplifies further processing by maintaining the structure necessary for direct, element-wise operations without additional reshaping."
      ],
      "metadata": {
        "id": "33HqPaq_aqIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "LMBI7S9MYYNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# torch.tril: Generates a lower triangular matrix from a 3x3 matrix of ones. The result is a matrix where all elements above the diagonal\n",
        "# are zero, and those on and below the diagonal are one. (REFER TO MAIN NOTES ABOVE)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "\n",
        "\n",
        "a = a / torch.sum(a, 1, keepdim=True) # second parameter \"1\" means to sum from the 2nd dimension (since pytorch starts from 0)\n",
        "                                      # for keepdim=True parameter, it basically keeps dimension. REFER TO MAIN NOTES ABOVE for what happens if not\n",
        "b = torch.randint(0,10,(3,2)).float() # (3,2) is shape tuple\n",
        "c = a @ b # matrix multiplication <-- giving final results here\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDnRLfAfbAvs",
        "outputId": "91e092b8-cdb7-4356-8631-80810248442f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels (features per time step) // defines dimensions of 3D tensor\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLizI4z6bAuN",
        "outputId": "29d4ff3e-5ace-4610-a733-4c2caccb44e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "7ybyNtStbAsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ePFAOqbArJ",
        "outputId": "88bcb446-e9ef-41c3-cc5b-daaa153e4edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n29BN6UebAp8",
        "outputId": "3cedf818-c4ab-4d22-e6a1-98f6bc5ecd37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTVZwOMYbAn-",
        "outputId": "252de2e8-4e33-48e0-85d1-3baaefe168e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeL5QYi1XYJY",
        "outputId": "75a2a78b-a7a5-49ae-e18b-d0db658fe366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "YytjTzqsXYH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXQlI8DZXYGk",
        "outputId": "f4133449-b916-4928-b67e-8ced23ca6702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMhg-gDmXYEQ",
        "outputId": "f9039e39-bd7a-4b6f-de8a-0562f44e0e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3mQcdnpXYCe",
        "outputId": "714cb0e7-9793-4072-a30f-0eb2e69b0d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waXuvkpBXYAn",
        "outputId": "e5afe3a9-9a00-40b3-e7f4-aef4b1d14db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG9-sMJOXX-X",
        "outputId": "4d3a50d7-b2f3-4f54-94b0-006a63d3ff7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ_3t6iuXi5s",
        "outputId": "edbf25eb-68c8-4936-e312-094e275a20c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLOFE4zFXjYz",
        "outputId": "14c3904a-b1b8-452b-97e6-ed252d9915e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OyEESBYXjWd",
        "outputId": "eddcdffb-436a-4747-9c43-8bd861fb4e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>"
      ],
      "metadata": {
        "id": "CCe_We2IXjUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}