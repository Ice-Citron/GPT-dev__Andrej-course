{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "vpz_PjV5YYSK",
        "nrvntjV-IaQs",
        "2AXcvDKtLDGz",
        "WM_giRxgRCN7",
        "g-t5IAqS8fx1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lLIdlN5gJTlu",
        "outputId": "2b98264d-d103-4d6a-a593-c1d242b461de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.0\n",
            "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.1+cu121\n",
            "    Uninstalling torchaudio-2.3.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 triton-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYq5ZhzOJtSx",
        "outputId": "01dc9598-1c13-4c8b-eda4-508a6ffc193d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-26 14:15:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-07-26 14:15:05 (17.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "pq2Bp6IdJzDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRVszaUrLq7s",
        "outputId": "28c292af-7eb0-44b5-f42e-da78fb952083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QbEtTGlLq5P",
        "outputId": "94e2e8ef-881e-461f-b05c-da731cfa1160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This block identifies all unique character in string or tet stored in variable 'text'\n",
        "chars = sorted(list(set(text))) # Extracts unique characters from the string text, converts the set to a list, and sorts it alphabetically.\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars)) # prints sorted list\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56OK0KWkLq3H",
        "outputId": "713435e2-9228-49c1-a0fe-e52172591c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers + defines function for encoding and decoding strings\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # create a dictionary mapping characters to integers // i is index, ch is character in enumerate (this is fashion of enumerate)\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CbWXBiPLq1A",
        "outputId": "2cfe2491-e51a-4a50-c742-e797c01d9a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the first 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V60tyBgDLqy5",
        "outputId": "1e25aa88-f310-4d6f-e6fd-3ff3584345a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "myMh6RYkLqw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn3olFVRLquj",
        "outputId": "51104a6e-fdaa-4f06-81bc-e2fe03d9820a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # x represents the input sequence // takes the first block_size number of elements\n",
        "y = train_data[1:block_size+1] # takes elements from 2nd elements up to block_size+1 number of elements // is essentially x but shifted one position to the right\n",
        "                               # This is used as the target sequence for training. Where each element in y is the sequence that model should predict after seeing sequence in 'x'\n",
        "\n",
        "for t in range(block_size): # This for loop iterates over each position in the block_size, effectively allowing you to demonstrate or train the model on increasing amounts of context.\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")\n",
        "\n",
        "# note: as seen below, when provided a sequence in training set. The model actually tries to train the data batch as just [1] to [9], to predict the\n",
        "#       token. So that the model are able to capture semantic meaning better, and also is able to deal with small inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOoMmCZgL64z",
        "outputId": "8e785d63-1d2c-45c7-d20d-82237839a0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Notes"
      ],
      "metadata": {
        "id": "18ObmqafP6zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: What is byte-pair encoding?\n",
        "\n",
        "Byte-Pair Encoding (BPE) is a data compression technique originally designed to efficiently compress text data. It has been adapted in natural language processing (NLP) to perform subword tokenization, which involves breaking down words into more manageable units (subwords) for models to process. This technique is especially useful in handling languages with rich morphology or in managing the vocabulary size in neural network models.\n",
        "\n",
        "### How Byte-Pair Encoding Works in NLP\n",
        "\n",
        "1. **Initialization**: Start with a vocabulary of individual characters or other base units (like bytes in its original application) from the training data.\n",
        "\n",
        "2. **Frequency Count**: Count the frequency of pairs of adjacent symbols or characters in the text data.\n",
        "\n",
        "3. **Merge Operations**: Identify the most frequently occurring pair of symbols and merge them into a single new symbol. This merged symbol is then added to the vocabulary.\n",
        "\n",
        "4. **Iterative Process**: Repeat the merging process for a predetermined number of steps or until some criterion is met (like a desired vocabulary size). Each iteration merges the next most frequent pair.\n",
        "\n",
        "5. **Tokenization**: Once the vocabulary of symbols (including merged symbols) is created, tokenize the text by greedily replacing the longest possible sequences in the text with the corresponding symbols in the vocabulary.\n",
        "\n",
        "### Advantages of BPE in NLP\n",
        "\n",
        "- **Efficiency in Unknown Words**: BPE allows a model to handle words not seen during training, by breaking them into known subwords. This is particularly useful in languages with compounding or agglutination where new word forms are frequent.\n",
        "\n",
        "- **Balance Between Character and Word Level**: BPE provides a balance between character-level and word-level representations. It avoids the sparsity of character-level processing and the vocabulary explosion of word-level processing.\n",
        "\n",
        "- **Contextual Awareness**: Subwords provide more contextual information than individual characters, helping models understand suffixes, prefixes, and roots, which are crucial for semantic and syntactic understanding.\n",
        "\n",
        "### Usage in Modern NLP\n",
        "\n",
        "BPE has been employed in several state-of-the-art NLP models and frameworks, notably in models like GPT (from OpenAI), BERT, and other Transformer-based architectures. These models use BPE or its variants (like SentencePiece or WordPiece) to preprocess text into subwords, which are then used for tasks like translation, text summarization, and question-answering.\n",
        "\n",
        "Overall, BPE is a pivotal component in the preprocessing pipeline for many NLP tasks, helping to improve both the efficiency and effectiveness of models in handling diverse and complex language data."
      ],
      "metadata": {
        "id": "QhMuDwmUPkBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `torch.stack` in PyTorch is used to concatenate a sequence of tensors along a new dimension. This is different from functions like `torch.cat`, which concatenate tensors along an existing dimension. `torch.stack` effectively combines tensors in such a way that the resulting tensor has an additional dimension compared to the input tensors.\n",
        "\n",
        "### Usage of `torch.stack`\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "torch.stack(tensors, dim=0)\n",
        "```\n",
        "\n",
        "- **tensors**: a sequence of tensors to concatenate. All tensors must have the same shape.\n",
        "- **dim**: the dimension along which to concatenate the tensors. This dimension is added to the resulting tensor, and each tensor in the sequence will be sliced along this new dimension.\n",
        "\n",
        "### Example\n",
        "\n",
        "Let's consider you have three 2x2 tensors:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Define three 2x2 tensors\n",
        "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
        "tensor3 = torch.tensor([[9, 10], [11, 12]])\n",
        "\n",
        "# Stack these tensors along a new dimension\n",
        "result = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "tensor([[[ 1,  2],\n",
        "         [ 3,  4]],\n",
        "\n",
        "        [[ 5,  6],\n",
        "         [ 7,  8]],\n",
        "\n",
        "        [[ 9, 10],\n",
        "         [11, 12]]])\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The original tensors are 2x2 matrices.\n",
        "- The `torch.stack` function stacks them along a new first dimension (`dim=0`), resulting in a 3x2x2 tensor. This means we now have 3 matrices of size 2x2 stacked on top of each other.\n",
        "\n",
        "### Practical Uses in Machine Learning\n",
        "\n",
        "`torch.stack` is often used in machine learning and deep learning when you need to batch together multiple tensors of the same size, such as when combining individual feature tensors into a batch tensor that can be passed to a model. This operation is critical for efficient computation, allowing batch processing of data rather than processing one item at a time."
      ],
      "metadata": {
        "id": "B0FGRMfMS45l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax `(batch_size,)` in Python is used to create a tuple containing a single element, which in this context is the `batch_size`. This notation is necessary because simply writing `batch_size` without the comma and parentheses would be interpreted as an integer, not a tuple. In Python, the trailing comma is what distinguishes a tuple from a regular parenthesis-wrapped expression.\n",
        "\n",
        "### Context in PyTorch:\n",
        "\n",
        "When using PyTorch, especially in functions like `torch.randint`, specifying dimensions as a tuple is common. For example, if you want to generate random integers within a certain range and shape the output as a tensor with dimensions defined by that tuple, you'd use:\n",
        "\n",
        "```python\n",
        "torch.randint(high, (shape_tuple))\n",
        "```\n",
        "\n",
        "- **high**: The upper boundary (exclusive) for the random integers.\n",
        "- **shape_tuple**: A tuple defining the shape of the output tensor.\n",
        "\n",
        "### Example with `torch.randint`:\n",
        "\n",
        "If you have `batch_size = 4` and you write:\n",
        "\n",
        "```python\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "```\n",
        "\n",
        "This code is generating a tensor of shape `(4,)`—meaning it contains four elements. Each element is a random integer between 0 and `len(data) - block_size`. This is particularly useful for selecting starting indices for sequences in data processing, where each index needs to be independently sampled.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "1. **Tuple Notation**: `(batch_size,)` ensures that the argument is passed as a tuple, which is often required by PyTorch functions for defining dimensions.\n",
        "2. **Usage in PyTorch**: Many PyTorch functions that generate or manipulate tensors require dimensions to be specified as tuples to correctly interpret the intended shape of the tensor.\n",
        "\n",
        "This method is widely used in data manipulation and batching processes in machine learning workflows to ensure that tensor operations are applied over the correct dimensions."
      ],
      "metadata": {
        "id": "LVmUbLkgTRi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "5Ef_W8F3P83Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # Number of independent sequences to process simultaneously\n",
        "block_size = 8 # Length of each sequence or the context used for predicting the next item\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data # determines which dataset to use\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # ix: Randomly selects starting indices for sequences within data, avoiding the last block_size\n",
        "                                                              # characters to prevent index overflow.\n",
        "                                                                  # (batch_size,) in the end: explanation found above. It's because it needs a shape_tuple to determine its shape :)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension // iterates over each independent sequences in the batch\n",
        "    for t in range(block_size): # time dimension // iterates over each element within each independent sequence\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUPY2tfEL62W",
        "outputId": "3c27dcc3-fa91-4d9a-f680-f998483cbf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W--QK3B9L60O",
        "outputId": "8faa1860-2c72-4aee-bbe4-99bb2b60095c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note Taking"
      ],
      "metadata": {
        "id": "n6SU4bsiUYT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An **embedding** in the context of machine learning, specifically in natural language processing (NLP), is a representation of data where similar data points are close to each other in a high-dimensional space. More simply, it's a way of converting categorical data, like words or tokens, into vectors of real numbers which are easier to process while retaining semantic meaning.\n",
        "\n",
        "### Key Points about Embeddings:\n",
        "\n",
        "- **Dimensionality Reduction**: While the vocabulary of a language can be vast, embeddings allow each word or token to be represented by a much smaller vector. This is more efficient than one-hot encoding, where the vector length equals the number of words in the vocabulary, with a single '1' and the rest '0's.\n",
        "\n",
        "- **Semantic Information**: Embeddings capture more than just the identity of words. They can also capture semantic relationships between words. For example, synonyms might be positioned closer together in the embedding space.\n",
        "\n",
        "- **Training**: Embeddings can be pre-trained on large text corpora using models like Word2Vec, GloVe, or FastText, or they can be trained from scratch along with the neural network's other parameters tailored to a specific task.\n",
        "\n",
        "### Specifics from Your Code:\n",
        "\n",
        "In your provided code snippet, the embedding layer (`self.token_embedding_table`) maps each token (represented as an integer) to a vector of size equal to the vocabulary size. Each position in the vector represents a potential feature or characteristic of the token, learned during training. Although using an embedding size equal to the vocabulary size is unusual (as it can lead to high dimensionality and overfitting), it might be useful in certain contexts or for experimental purposes.\n",
        "\n",
        "This embedding process is crucial for models to effectively learn and make predictions from textual data, allowing them to handle the vast variability of language with a more manageable and meaningful representation."
      ],
      "metadata": {
        "id": "4-i9Bpz3UYR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query:\n",
        "so in my case im creating a lookup table where each token point to its respective vector in high dimensional space (which is also called embedding?)?\n",
        "\n",
        "and what does nn.Embedding(vocab_size, vocab_size) do? what does the input parameter changes?\n",
        "\n",
        "---\n",
        "\n",
        "Yes, you've got it right! In your case, the embedding layer acts as a lookup table where each token (represented as an integer index) is mapped to its respective vector in a high-dimensional space. This vector space, where each vector represents a token, is indeed referred to as an \"embedding.\"\n",
        "\n",
        "### Breaking Down `nn.Embedding(vocab_size, vocab_size)`\n",
        "\n",
        "The `nn.Embedding` layer in PyTorch is specifically designed to handle such mappings. When you instantiate an `nn.Embedding` with parameters like `(vocab_size, vocab_size)`, you're setting up:\n",
        "\n",
        "1. **Number of Embeddings (`num_embeddings`)**: The first `vocab_size` parameter specifies the number of distinct tokens that the embedding layer can handle. This is essentially the size of your vocabulary.\n",
        "\n",
        "2. **Dimension of Each Embedding Vector (`embedding_dim`)**: The second `vocab_size` parameter sets the size of each embedding vector. In typical use cases, this parameter is much smaller than the vocabulary size because embedding vectors are meant to be a dense, information-rich representation of the tokens.\n",
        "\n",
        "### Function of `nn.Embedding(vocab_size, vocab_size)`\n",
        "By using `vocab_size` for both parameters, you are creating an embedding layer where:\n",
        "- There are as many embedding vectors as there are tokens in your vocabulary.\n",
        "- Each embedding vector has a dimensionality equal to the number of tokens in your vocabulary.\n",
        "\n",
        "### Practical Implications\n",
        "- **Dimensionality**: Normally, the dimension of the embedding vectors (`embedding_dim`) is chosen to be much smaller than the vocabulary size to ensure that the embeddings capture the most important features of the data without being redundantly large. A large embedding dimension (especially equal to the vocabulary size) can lead to overfitting where the model learns the training data too well, including noise and details that do not generalize well to new, unseen data.\n",
        "- **Efficiency**: Large embeddings consume more memory and computational resources, which can make training slower and less efficient.\n",
        "\n",
        "### Example Use\n",
        "A common setup might be something like `nn.Embedding(10000, 300)`, where 10,000 represents a vocabulary of 10,000 words, and each word is represented by a 300-dimensional vector. This setup efficiently compresses information while retaining enough complexity to model linguistic nuances.\n",
        "\n",
        "In your specific case, using `vocab_size` for both might be more experimental or illustrative, helping to explore or demonstrate certain concepts, but it's generally advisable to use a smaller dimension for the embeddings in practical applications."
      ],
      "metadata": {
        "id": "lc6ImIcUVk84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: why do i need to zero_grad first? why not just calculate gradient right away\n",
        "\n",
        "---\n",
        "\n",
        "In PyTorch, when you use `.backward()` to calculate gradients during the training of a neural network, the gradients for each parameter are accumulated by default. This means that each time `.backward()` is executed, gradients from the current pass are added to those already stored from previous backward passes, unless they are explicitly cleared.\n",
        "\n",
        "### Why Zeroing Gradients is Necessary\n",
        "1. **Accumulation by Default**: By design, PyTorch accumulates gradients because this feature supports several advanced optimization tasks, such as training parts of a model (like in transfer learning) or accumulating gradients over multiple batches or sub-batches. However, in standard training loops, you typically want to update weights based on the gradients computed from just the current batch, not a combination of the current and previous batches.\n",
        "\n",
        "2. **Correct Gradient Values**: If you don't reset the gradients, the gradient computations would mix gradients from different batches. This would lead to incorrect updates in the weights, potentially causing the model to converge poorly or diverge.\n",
        "\n",
        "3. **Isolation of Gradient Computation**: Zeroing out the gradients ensures that each batch's gradient computation is independent of others. Each training step's update should reflect only the current batch's data, ensuring that the learning process accurately represents the model's performance on the latest data.\n",
        "\n",
        "### Usage of `optimizer.zero_grad(set_to_none=True)`\n",
        "- **Efficiency**: Setting gradients to `None` (`set_to_none=True`) rather than explicitly zeroing them can be more efficient in some cases. When you set gradients to `None`, PyTorch understands that the gradients need to be recalculated afresh, avoiding the allocation of zero tensors before storing the new gradients. This can reduce some overhead, particularly in terms of memory operations, because you skip the explicit zeroing step and go straight to calculating new gradients.\n",
        "\n",
        "In summary, zeroing out the gradients (or setting them to `None`) before calculating them afresh each time ensures that the updates you make to the model parameters reflect only the most recent data's influence, maintaining the integrity and independence of each training step. This is crucial for the model to learn effectively."
      ],
      "metadata": {
        "id": "9WFJD5EXe4HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "_s7v7_ifUYPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): # inherits from nn.Module <-- PyTorch's base class for all nn modules.\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # first parameter specifies the number of distinct tokens that the embedding layer can handle.\n",
        "                                                                          # or essentially the size of your vocabulary\n",
        "                                                                          # second parameter specifies the size of each embedding vector. This is normally much smaller than vocab size itself\n",
        "                                                                          # because embedding vectors are meant to be a dense, information-rich representation of tokens\n",
        "\n",
        "    def forward(self, idx, targets=None): # method is part of the model's forward pass. Used for traiing + inference to compute the outputs/logits. And optionally loss if targets are provided\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers // B is batch size, and  is sequence length\n",
        "        # targets = optional tensor of same shape as 'idx' containing target indices for training\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C) // C is the embedding dimension\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # uses torch.view to merge batch and sequence length dimensions // preparing it for loss compuation\n",
        "            targets = targets.view(B*T) # simillarly reshapes for loss compuation\n",
        "            loss = F.cross_entropy(logits, targets) # computes cross-entropy loss between logits and training. Used for training the model.\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context // idx is initial sequence of indices to start generating from // max_new_tokens = number of tokens to generate\n",
        "        for _ in range(max_new_tokens): # repeats process for max_new_tokens number of iterations\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx) # model's 'forward' method is called with current indices idx as input\n",
        "            # focus only on the last time step (last layer?)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution // this line samples from probabilitiy distribution \"probs\"\n",
        "                  # torch.multinomial treats each row in probs as a separate distributon and samples num_samples=1 index from each distribution\n",
        "                  # based on probabilities distribution from softmax // multinomial basically just picks options based on how likely they are to be picked\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) # concatenated.\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size) # creates instance of BigramLanguageModel with specified vocabulary size. Which determines how many different tokens the model can handle.\n",
        "logits, loss = m(xb, yb) # runs a forward pass of model. Input batch + target batch inputted.\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())) # calls the 'generate' method of the model to produce a sequence of new\n",
        "                                                                                                       # tokens. This method starts with an initial token. In this case, torch.zeros\n",
        "                                                                                                       # or just 0s, which typically corresponds to a \"start\" token or padding token.\n",
        "\n",
        "                                                                                                       # idx = torch.zeros specifies starting token\n",
        "\n",
        "                                                                                                       # [0].tolist(): The 'generate' method returns a tensor, where the 1st dimension\n",
        "                                                                                                       # is batch size (1 in this case). The [0] selects the first sequence form batch.\n",
        "                                                                                                       # And toList() converts the tensor of token indices to a list of integers.\n",
        "\n",
        "                                                                                                       # decode() is defined previously."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txFa8CFJL6yT",
        "outputId": "90a7c0f4-4d46-40c0-a084-7ea50f5ec6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "mAYpDRV8L6wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # sets the size of each batch to 32. But variable isn't used here.\n",
        "for steps in range(100): # increase number of steps for good results... // below is training loop.\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train') # retrieves a batch of training data (previously defined function)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True) # resets gradients of all model parameter to 0 before backpropagation. \"set_to_none=True\" arg optimises this process by setting gradients to\n",
        "                                          # 'None' instead of zeroing them. Which can be more efficient\n",
        "    loss.backward() # performs backpropagation to compute gradient of loss with respect to model parameters\n",
        "    optimizer.step() # updates the model's parameters using the computed gradients.\n",
        "\n",
        "print(loss.item()) # .item() converts a single-value tensor to a Python number."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXfymlxybAyr",
        "outputId": "e26a3ceb-a516-4fe8-a996-f8d4a543124f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.587916374206543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())) # regenerating again. 500 tokens this time instead. way better!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYJHvUGcbAxO",
        "outputId": "d0dca081-5cb9-47af-9dd7-53e50989bd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
            "3Q&sGlvHQ?mqSq-eON\n",
            "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
            "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
            "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
            "&WDdP!Ko,px\n",
            "x\n",
            "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
            "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "IJX0vyLIfa8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes"
      ],
      "metadata": {
        "id": "vpz_PjV5YYSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's break down the code snippet step by step to understand its functionality. This snippet demonstrates the use of matrix operations to achieve weighted aggregation, which is foundational in the mechanism of self-attention.\n",
        "\n",
        "### Code Breakdown\n",
        "\n",
        "```python\n",
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "```\n",
        "\n",
        "1. **Set Random Seed:**\n",
        "   ```python\n",
        "   torch.manual_seed(42)\n",
        "   ```\n",
        "   - This line sets the seed for generating random numbers, ensuring reproducibility of the random results (such as tensor values) for debugging and testing purposes.\n",
        "\n",
        "2. **Create Matrix `a`:**\n",
        "   ```python\n",
        "   a = torch.tril(torch.ones(3, 3))\n",
        "   ```\n",
        "   - `torch.tril`: Generates a lower triangular matrix from a 3x3 matrix of ones. The result is a matrix where all elements above the diagonal are zero, and those on and below the diagonal are one.\n",
        "   - Example output of `a` at this point:\n",
        "     ```\n",
        "     [[1., 0., 0.],\n",
        "      [1., 1., 0.],\n",
        "      [1., 1., 1.]]\n",
        "     ```\n",
        "\n",
        "3. **Normalize Matrix `a`:**\n",
        "   ```python\n",
        "   a = a / torch.sum(a, 1, keepdim=True)\n",
        "   ```\n",
        "   - This line normalizes each row of the matrix `a` so that the sum of each row equals 1. This is achieved by dividing each element in a row by the sum of the elements in that row (`torch.sum(a, 1, keepdim=True)` calculates the sum of each row and retains the original number of dimensions for broadcasting).\n",
        "   - Post-normalization, matrix `a` serves as a set of weights for aggregation, with each row summing to 1.\n",
        "\n",
        "4. **Create Matrix `b`:**\n",
        "   ```python\n",
        "   b = torch.randint(0,10,(3,2)).float()\n",
        "   ```\n",
        "   - Generates a 3x2 matrix with random integers between 0 and 9, then converts these integers to floating-point numbers. This matrix `b` might represent data points or features that will be aggregated.\n",
        "\n",
        "5. **Matrix Multiplication (`a @ b`):**\n",
        "   ```python\n",
        "   c = a @ b\n",
        "   ```\n",
        "   - Performs matrix multiplication between the normalized weight matrix `a` and the data matrix `b`. This operation aggregates data in `b` using the weighted sums specified in `a`. The `(i, j)` element of the result matrix `c` is the weighted sum of the elements in the `j`-th column of `b`, using the weights from the `i`-th row of `a`.\n",
        "\n",
        "6. **Printing Results:**\n",
        "   ```python\n",
        "   print('a=')\n",
        "   print(a)\n",
        "   print('--')\n",
        "   print('b=')\n",
        "   print(b)\n",
        "   print('--')\n",
        "   print('c=')\n",
        "   print(c)\n",
        "   ```\n",
        "   - These lines print the matrices `a`, `b`, and the result `c` to show the input weights, the data being aggregated, and the outcome of the weighted aggregation, respectively.\n",
        "\n",
        "### Summary\n",
        "This code illustrates a fundamental operation in neural networks, especially in mechanisms like self-attention where weighted sums are computed as a combination of elements. The use of normalization and matrix multiplication here is a simple yet powerful demonstration of how weights can control the influence of different elements (or time steps in sequences) when aggregating data."
      ],
      "metadata": {
        "id": "ZDoo_26zYYPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: not very clear with this one. show me an example of how does this work\n",
        "\n",
        "can you explain to me what is the code here meant to do overall\n",
        "\n",
        "---\n",
        "\n",
        "Let's clarify how the normalization of matrix `a` and the subsequent matrix multiplication with matrix `b` works in the given code by walking through a detailed example. This will demonstrate the concept of weighted aggregation, which is a fundamental operation in many neural network architectures, including transformers for self-attention.\n",
        "\n",
        "### Detailed Example with Matrix `a` and `b`\n",
        "\n",
        "#### 1. Initialization and Normalization of Matrix `a`\n",
        "Matrix `a` is initially created as a lower triangular matrix of ones. This kind of matrix will have ones on and below the diagonal, and zeros above it. Here's what it looks like initially:\n",
        "\n",
        "```\n",
        "[[1, 0, 0],\n",
        " [1, 1, 0],\n",
        " [1, 1, 1]]\n",
        "```\n",
        "\n",
        "Normalization means adjusting the rows of `a` such that each row sums to 1. This makes each row of `a` represent a set of weights for aggregating elements of `b`. The normalization is performed by dividing each element of a row by the sum of that row:\n",
        "\n",
        "- Row 1: \\( [1, 0, 0] \\) sums to 1. \\( \\frac{[1, 0, 0]}{1} = [1, 0, 0] \\)\n",
        "- Row 2: \\( [1, 1, 0] \\) sums to 2. \\( \\frac{[1, 1, 0]}{2} = [0.5, 0.5, 0] \\)\n",
        "- Row 3: \\( [1, 1, 1] \\) sums to 3. \\( \\frac{[1, 1, 1]}{3} = [0.33, 0.33, 0.33] \\)\n",
        "\n",
        "Normalized matrix `a`:\n",
        "```\n",
        "[[1.00, 0.00, 0.00],\n",
        " [0.50, 0.50, 0.00],\n",
        " [0.33, 0.33, 0.33]]\n",
        "```\n",
        "\n",
        "#### 2. Creation of Matrix `b`\n",
        "Suppose matrix `b` is randomly generated as:\n",
        "```\n",
        "[[3, 7],\n",
        " [2, 1],\n",
        " [5, 4]]\n",
        "```\n",
        "\n",
        "#### 3. Matrix Multiplication (`a @ b`)\n",
        "The operation `a @ b` performs a matrix multiplication where each element of the result matrix `c` is a weighted sum of elements from `b`, with weights defined by `a`.\n",
        "\n",
        "- **First row of `c`**: Only the first element of `b` contributes because all weights for other elements are zero (due to `a`).\n",
        "  - \\( c[0] = 1.0 \\times [3, 7] + 0.0 \\times [2, 1] + 0.0 \\times [5, 4] = [3, 7] \\)\n",
        "- **Second row of `c`**: Elements from the first two rows of `b` contribute equally.\n",
        "  - \\( c[1] = 0.5 \\times [3, 7] + 0.5 \\times [2, 1] = [2.5, 4] \\)\n",
        "- **Third row of `c`**: All elements of `b` contribute equally.\n",
        "  - \\( c[2] = 0.33 \\times [3, 7] + 0.33 \\times [2, 1] + 0.33 \\times [5, 4] = [3.33, 4] \\)\n",
        "\n",
        "Resultant matrix `c`:\n",
        "```\n",
        "[[3.00, 7.00],\n",
        " [2.50, 4.00],\n",
        " [3.33, 4.00]]\n",
        "```\n",
        "\n",
        "### Overall Purpose\n",
        "The operation demonstrated in the code uses the concept of **weighted aggregation** where matrix `a` serves as a set of weights to combine or blend elements of matrix `b`. This kind of operation is fundamental in neural networks, particularly in mechanisms like attention where inputs are combined based on learned or predefined importance weights. Here, it simplifies to averaging or summing with weights that consider positional relations (like in the lower triangular form, which could mimic causality or sequence in data)."
      ],
      "metadata": {
        "id": "SHEI5ee9aqOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: explain to me each of the parameter here. and please go into more detail what weighted aggregation is\n",
        "\n",
        "sum(a, 1, keepdim=True)\n",
        "\n",
        "---\n",
        "\n",
        "The function `torch.sum(a, 1, keepdim=True)` from PyTorch is used to sum elements of a tensor along a specified dimension, with options to maintain the number of dimensions in the output. This function is key in various tensor manipulations, especially in operations like normalization where relative scaling of tensor elements based on their sums is required.\n",
        "\n",
        "### Parameters Explained:\n",
        "\n",
        "- **a**: This is the tensor on which the summation operation is performed. In your context, `a` is the lower triangular matrix of ones that has been modified in some way (like normalizing each row).\n",
        "\n",
        "- **1**: This is the dimension index over which to perform the summation. PyTorch tensors are zero-indexed, so an index of `1` refers to the second dimension of the tensor. In a 2D matrix, this would be summing across rows (i.e., summing elements within each column of a row). For the matrix `a`, which is of size `[3, 3]`, summing over dimension `1` results in summing all elements across each row.\n",
        "\n",
        "- **keepdim=True**: This parameter decides whether the output tensor should retain the number of dimensions as the input tensor. If set to `True`, the resulting tensor will have the same number of dimensions as the input tensor, but with size `1` along the summed dimension. If it were `False`, the summed dimension would be removed from the output, reducing the dimensionality of the result.\n",
        "\n",
        "### Example:\n",
        "Consider the matrix `a`:\n",
        "```\n",
        "[[1, 0, 0],\n",
        " [1, 1, 0],\n",
        " [1, 1, 1]]\n",
        "```\n",
        "Running `torch.sum(a, 1, keepdim=True)` on this matrix would produce:\n",
        "```\n",
        "[[1],\n",
        " [2],\n",
        " [3]]\n",
        "```\n",
        "Here’s what happened:\n",
        "- For the first row, only the first element is non-zero, so the sum is `1`.\n",
        "- For the second row, the sum of `1 + 1` is `2`.\n",
        "- For the third row, the sum of `1 + 1 + 1` is `3`.\n",
        "\n",
        "The output retains its two-dimensional structure due to `keepdim=True`, but each row's sum is encapsulated in its own row, facilitating direct division for normalization purposes without needing to reshape.\n",
        "\n",
        "### Weighted Aggregation Explained:\n",
        "Weighted aggregation refers to the process of combining multiple elements into a single value where each element contributes according to a specified weight. This is a fundamental concept in data processing, statistics, and machine learning, particularly in fields like neural networks where it forms the basis of operations like calculating weighted averages, performing convolutions, and, more complexly, computing attention mechanisms in models like transformers.\n",
        "\n",
        "In the context of the matrix `a` and `b` from your code:\n",
        "- **Matrix `a`** acts as the weight matrix where each row's values determine the relative contribution of corresponding elements in matrix `b` during the aggregation process.\n",
        "- **Matrix `b`** contains data points or features to be aggregated.\n",
        "\n",
        "When you perform `a @ b` (matrix multiplication between `a` and `b`), you effectively apply the weighted aggregation:\n",
        "- Each row in `a` specifies how to combine elements of `b` into a single row in the resulting matrix `c`.\n",
        "- Each entry in the result is a dot product of a row from `a` and a column from `b`, which mathematically represents a weighted sum where weights are given by `a`.\n",
        "\n",
        "This process allows the model to highlight or diminish the influence of certain features dynamically, based on the training or the specific operation's goals, such as focusing on more recent information in sequences or blending features in image processing."
      ],
      "metadata": {
        "id": "t9Ez2AA1aqM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: oh wait. is it trying to give all the 1 values equal attention?\n",
        "\n",
        "ah i see. and by relying on matrix multiplication. if the value there is 0. or no token yet. then nothing is there. fair enough.\n",
        "\n",
        "---\n",
        "\n",
        "Yes, exactly! Your understanding is spot-on. Let's dive a bit deeper into these concepts:\n",
        "\n",
        "### Equal Attention and Matrix Weights\n",
        "\n",
        "The matrix `a`, after normalization, gives equal weight to all \"1\" values within each row, provided they exist. This is a way of distributing attention or influence equally across all available values or tokens up to that point in the sequence. For each row in `a`, the \"1\" values are adjusted to sum up to one, distributing equal attention to each existing token (non-zero entries). Here's what it does:\n",
        "\n",
        "- **Uniform Distribution of Influence**: When each row of `a` is normalized such that the sum equals 1, it means each non-zero entry (or each past token, if you relate it to sequences) contributes equally to the resultant value during the matrix multiplication. This ensures that there's no undue bias towards any particular position within the limits set by the triangular structure.\n",
        "\n",
        "- **Handling Zeros**: The zeros in `a` effectively ensure that no attention or weight is given to future tokens (entries above the diagonal), adhering to a causal structure. This is crucial in models where the future cannot depend on past information, such as when generating text or processing time-series data in a forward-only manner.\n",
        "\n",
        "### Matrix Multiplication and Its Role\n",
        "\n",
        "Matrix multiplication (`a @ b`) leverages the structure and values of `a` to aggregate information in `b`. Here’s what happens:\n",
        "\n",
        "- **If a Row in `a` has a \"0\"**: Any position in `b` corresponding to a zero in `a` does not contribute to the output in `c`. This aligns with the principle that only available (past) tokens should influence the current output, mirroring scenarios in sequence processing where you only want to consider previously seen or available data up to the current time step.\n",
        "\n",
        "- **Efficient Aggregation**: By performing matrix multiplication, you’re efficiently aggregating data across possibly large matrices in a highly optimized manner. This is far more computationally efficient than manually iterating through each element, especially with hardware accelerators like GPUs.\n",
        "\n",
        "### Practical Implications in Models\n",
        "\n",
        "This kind of weighted aggregation, particularly with a structured matrix like a lower triangular one, is a fundamental concept in many deep learning architectures, such as RNNs and Transformers:\n",
        "\n",
        "- **Recurrent Neural Networks (RNNs)**: They inherently process sequences by considering past information up to the current step, often manually managing how past states influence the current state.\n",
        "\n",
        "- **Transformers and Self-Attention**: They use a more sophisticated form of this concept where the attention mechanism dynamically determines weights (`a` matrix) based on the content and relationship between sequences, often without the strict triangular limitation unless specifically modeling causality (as in GPT-style architectures).\n",
        "\n",
        "Understanding these matrix operations and their implications helps in grasping more advanced topics in deep learning, especially in understanding how data flows and is processed within neural network layers."
      ],
      "metadata": {
        "id": "zJLWlfP2aqLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: show me what happens in this case when keepdim=False\n",
        "\n",
        "---\n",
        "\n",
        "Setting `keepdim=False` in the `torch.sum()` function affects how the output is shaped after the summation operation. This parameter influences whether the summed dimension is retained as a singleton dimension.\n",
        "\n",
        "### Example to Illustrate `keepdim=False`\n",
        "\n",
        "Let’s use a simple example to demonstrate the effect of setting `keepdim=False`. Consider a matrix `a` as follows:\n",
        "\n",
        "```\n",
        "a = [[1, 0, 0],\n",
        "     [1, 1, 0],\n",
        "     [1, 1, 1]]\n",
        "```\n",
        "\n",
        "#### Summation with `keepdim=False`\n",
        "Using `torch.sum(a, 1, keepdim=False)`, we sum across each row (dimension 1) but do not keep the dimension:\n",
        "\n",
        "```python\n",
        "a_sum = torch.sum(a, 1, keepdim=False)\n",
        "```\n",
        "For the matrix `a`:\n",
        "- The sum of the first row: `1 + 0 + 0 = 1`\n",
        "- The sum of the second row: `1 + 1 + 0 = 2`\n",
        "- The sum of the third row: `1 + 1 + 1 = 3`\n",
        "\n",
        "The resulting tensor `a_sum` will be:\n",
        "```\n",
        "[1, 2, 3]\n",
        "```\n",
        "\n",
        "#### Output Shape\n",
        "The shape of `a_sum` will be `[3]`, a one-dimensional tensor (vector), because we collapsed the row dimension by summing over it and did not keep it in the output.\n",
        "\n",
        "#### Contrast with `keepdim=True`\n",
        "If we had used `keepdim=True`:\n",
        "```python\n",
        "a_sum_keepdim = torch.sum(a, 1, keepdim=True)\n",
        "```\n",
        "The result would have been:\n",
        "```\n",
        "[[1],\n",
        " [2],\n",
        " [3]]\n",
        "```\n",
        "Here, the output retains its two-dimensional structure, but the summed dimension is reduced to a single element, maintaining compatibility for subsequent operations that might rely on tensor dimensions, such as broadcasting in matrix operations.\n",
        "\n",
        "### Practical Implication\n",
        "When `keepdim=False` is used, the resulting tensor after a summation operation loses a dimension. This is useful when you want a flattened or reduced result directly without extra singleton dimensions. However, this might require additional reshaping if the tensor needs to be used in subsequent operations expecting a specific number of dimensions.\n",
        "\n",
        "For operations that involve normalization or weighted computations (like in your matrix `a`), keeping dimensions (using `keepdim=True`) simplifies further processing by maintaining the structure necessary for direct, element-wise operations without additional reshaping."
      ],
      "metadata": {
        "id": "33HqPaq_aqIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "LMBI7S9MYYNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# torch.tril: Generates a lower triangular matrix from a 3x3 matrix of ones. The result is a matrix where all elements above the diagonal\n",
        "# are zero, and those on and below the diagonal are one. (REFER TO MAIN NOTES ABOVE)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "\n",
        "\n",
        "a = a / torch.sum(a, 1, keepdim=True) # second parameter \"1\" means to sum from the 2nd dimension (since pytorch starts from 0)\n",
        "                                      # for keepdim=True parameter, it basically keeps dimension. REFER TO MAIN NOTES ABOVE for what happens if not\n",
        "b = torch.randint(0,10,(3,2)).float() # (3,2) is shape tuple\n",
        "c = a @ b # matrix multiplication <-- giving final results here\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDnRLfAfbAvs",
        "outputId": "91e092b8-cdb7-4356-8631-80810248442f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels (features per time step) // defines dimensions of 3D tensor\n",
        "x = torch.randn(B,T,C) # initialises tensor with shape BxTxC. With random values drawn from normal distribution.\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLizI4z6bAuN",
        "outputId": "45b379ab-8e5e-4363-d3ec-226750c0aca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## xbow Notes"
      ],
      "metadata": {
        "id": "nrvntjV-IaQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query: cant visualise this. can you show me an example? and whats cumulative average again? and why. why do this\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's clarify the concept of **cumulative average** and provide an illustrative example using simple numbers to demonstrate how it's calculated and why it might be used in the context of your Python code.\n",
        "\n",
        "### What is Cumulative Average?\n",
        "A cumulative average, also known as a running or moving average, is calculated by averaging a set of values up to a certain point in a sequence. For each new point in the sequence, the cumulative average includes all previous values up to that point. It is commonly used to smooth out short-term fluctuations and highlight longer-term trends in time series data.\n",
        "\n",
        "### Why Use Cumulative Average?\n",
        "- **Smoothing**: It smooths out noise and fluctuations in data, making trends more visible.\n",
        "- **Real-time Analysis**: Useful in real-time analysis since it can be updated incrementally as new data arrives.\n",
        "- **Simplifying Data**: Reduces data complexity, making it easier to perform further analysis or modeling.\n",
        "\n",
        "### Example with Detailed Explanation\n",
        "\n",
        "Suppose we have a tensor `x` of shape `(B, T, C)` where:\n",
        "- **B** (batch size) = 1 (simplify by considering only one batch)\n",
        "- **T** (time steps) = 5\n",
        "- **C** (channels/features per time step) = 1\n",
        "\n",
        "Let's assume the tensor `x` for a single batch and single channel looks like this:\n",
        "```\n",
        "x = [[1, 2, 3, 4, 5]]\n",
        "```\n",
        "\n",
        "#### Calculation of Cumulative Averages\n",
        "Here's how the cumulative averages would be computed manually for this tensor:\n",
        "\n",
        "1. **At time step 1**: Only the first value (1).\n",
        "   - Average = \\(1/1 = 1\\)\n",
        "2. **At time step 2**: Includes values (1, 2).\n",
        "   - Average = \\((1 + 2)/2 = 1.5\\)\n",
        "3. **At time step 3**: Includes values (1, 2, 3).\n",
        "   - Average = \\((1 + 2 + 3)/3 = 2\\)\n",
        "4. **At time step 4**: Includes values (1, 2, 3, 4).\n",
        "   - Average = \\((1 + 2 + 3 + 4)/4 = 2.5\\)\n",
        "5. **At time step 5**: Includes all values (1, 2, 3, 4, 5).\n",
        "   - Average = \\((1 + 2 + 3 + 4 + 5)/5 = 3\\)\n",
        "\n",
        "This results in a new tensor of cumulative averages:\n",
        "```\n",
        "cumulative_averages = [[1, 1.5, 2, 2.5, 3]]\n",
        "```"
      ],
      "metadata": {
        "id": "k5QnrFLbIaOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "oh wait nvm xbow is simply needed because version2, 3, 4 etc. other attention mechanisms relies on cumulative mean.\n",
        "\n",
        "---\n",
        "\n",
        "Exactly, the purpose of `xbow` in your code examples is to establish a baseline or reference implementation using a straightforward method (cumulative mean) for accumulating and averaging information across time steps. This cumulative mean is then compared to more sophisticated methods like matrix multiplication and softmax-based attention in the subsequent versions. Each method attempts to replicate or improve upon this fundamental operation but using different techniques that offer various advantages, particularly in terms of efficiency and alignment with modern neural network architectures.\n",
        "\n",
        "### Recap of Each Method:\n",
        "\n",
        "1. **Version 1 (`xbow` Calculation via Loops)**:\n",
        "   - This version directly computes the cumulative average using nested loops. It serves as a simple and clear implementation but is computationally inefficient, especially as the size of data (B, T, C) increases.\n",
        "\n",
        "2. **Version 2 (Matrix Multiply for Weighted Aggregation)**:\n",
        "   - This version uses matrix operations to achieve the same result as Version 1 but does so more efficiently. It leverages the parallelism of matrix operations, which are well-optimized on modern hardware (like GPUs), thus speeding up the calculations.\n",
        "\n",
        "3. **Version 3 (Softmax for Dynamic Weighting)**:\n",
        "   - Implements a form of attention by using softmax to dynamically allocate weights based on a trainable or predefined criterion (though in your specific example, it uses a static lower triangular matrix to simulate attention over previous and current time steps only). This method adds the flexibility of focusing differently on various parts of the sequence, akin to how attention mechanisms in neural networks like Transformers work.\n",
        "\n",
        "4. **Further Versions (Incorporating Self-Attention Concepts)**:\n",
        "   - These would incorporate even more complex and dynamically learned interactions between different parts of the input data, moving beyond static or predefined weighting schemes.\n",
        "\n",
        "### Why Use `xbow` or Cumulative Mean in Attention Mechanisms?\n",
        "\n",
        "In the context of your examples, `xbow` acts as a foundational concept to demonstrate how information from previous time steps can be aggregated to influence or determine the output at the current time step. This is crucial in many sequence processing tasks where past context influences current decisions or outputs.\n",
        "\n",
        "In more sophisticated models like Transformers:\n",
        "- **Self-attention** mechanisms dynamically compute weights (similar to those in your Version 3 with softmax), which allows the model to focus more on relevant parts of the input data based on the task at hand (e.g., focusing more on a specific word or feature that is crucial for understanding the sentence or image).\n",
        "\n",
        "- **Transformers and Cumulative Averages**: While transformers don't use cumulative averages per se, the concept of accumulating information across positions is generalized and optimized in the form of attention scores. These scores are computed in a way that each position potentially influences every other position, based on the learned importance rather than a simple chronological average.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Your implementations effectively showcase the evolution from simple cumulative averages to more complex and dynamic methods like weighted averages and attention, demonstrating the progression in techniques used for processing sequential data in neural networks. Each method has its place, with the more advanced techniques offering significant advantages in learning and performance for complex tasks involving large-scale and high-dimensional data."
      ],
      "metadata": {
        "id": "A7MAU9NkIxUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to coding"
      ],
      "metadata": {
        "id": "CuqJ7j4oIaMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "#Purpose: To compute the cumulative average of the data up to each time step for each batch and channel.\n",
        "    # MAIN PURPOSE: cumulative mean is calculated to be used for downstream V2, V3, V4 attention mechanism task\n",
        "\n",
        "xbow = torch.zeros((B,T,C)) # bow = bag of words // line here iniialises a tensore to store the cumulative averages\n",
        "for b in range(B):\n",
        "    for t in range(T): # loops over each batch and time step.\n",
        "        xprev = x[b,:t+1] # (t,C) // slices tensor x up to current time step.\n",
        "        xbow[b,t] = torch.mean(xprev, 0) # now computes a mean across these time steps for each channel and stores it in xbow"
      ],
      "metadata": {
        "id": "7ybyNtStbAsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 2 notes"
      ],
      "metadata": {
        "id": "2AXcvDKtLDGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dissect and clarify the provided code snippet, which demonstrates using matrix multiplication for weighted aggregation, often used in deep learning to combine features or apply learned importance metrics. I'll also provide a detailed example to help visualize the process step by step.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Create Weight Matrix:**\n",
        "   ```python\n",
        "   wei = torch.tril(torch.ones(T, T))\n",
        "   ```\n",
        "   - This line generates a lower triangular matrix (`torch.tril`) of size \\(T \\times T\\), where \\(T\\) is the number of time steps. All elements above the diagonal are zero, which effectively means only current and past time steps will influence the output at any given time step, simulating a causal structure (no future data peeking).\n",
        "\n",
        "2. **Normalize the Weight Matrix:**\n",
        "   ```python\n",
        "   wei = wei / wei.sum(1, keepdim=True)\n",
        "   ```\n",
        "   - Each row of the weight matrix `wei` is normalized to sum to 1. This ensures that the weighted sum used later genuinely reflects a weighted average, rather than disproportionately scaling the data. Normalization makes the sum of weights for each time step equal to 1, which is crucial for maintaining the scale of the aggregated output.\n",
        "\n",
        "3. **Weighted Aggregation via Matrix Multiplication:**\n",
        "   ```python\n",
        "   xbow2 = wei @ x  # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "   ```\n",
        "   - This line performs the matrix multiplication between the weight matrix `wei` and the data tensor `x`. The result `xbow2` is a tensor where each time step in each batch has been transformed by the weighted sum of all preceding and current time steps' data.\n",
        "\n",
        "4. **Check Close Equality:**\n",
        "   ```python\n",
        "   torch.allclose(xbow, xbow2)\n",
        "   ```\n",
        "   - This function checks if the tensors `xbow` and `xbow2` are element-wise close within a tolerance. This is typically used to verify that different methods of computing the same transformation yield consistent results.\n",
        "\n",
        "### Step-by-Step Example:\n",
        "\n",
        "Let's assume \\(T = 3\\) for simplicity, and our input tensor `x` has size \\(B=1\\) (one batch) and \\(C=1\\) (one channel), with values:\n",
        "\n",
        "```\n",
        "x = [[1],\n",
        "     [2],\n",
        "     [3]]\n",
        "```\n",
        "\n",
        "#### Step 1: Create and Normalize Weight Matrix\n",
        "\n",
        "1. **Create `wei`:**\n",
        "   - Lower triangular matrix of ones (size 3x3):\n",
        "     ```\n",
        "     [[1, 0, 0],\n",
        "      [1, 1, 0],\n",
        "      [1, 1, 1]]\n",
        "     ```\n",
        "\n",
        "2. **Normalize `wei`:**\n",
        "   - Sum of each row:\n",
        "     ```\n",
        "     [1, 2, 3]\n",
        "     ```\n",
        "   - Normalized `wei`:\n",
        "     ```\n",
        "     [[1.0, 0.0, 0.0],\n",
        "      [0.5, 0.5, 0.0],\n",
        "      [0.33, 0.33, 0.33]]\n",
        "     ```\n",
        "\n",
        "#### Step 2: Perform Weighted Aggregation\n",
        "\n",
        "1. **Matrix Multiplication `wei @ x`:**\n",
        "   - Calculating `xbow2`:\n",
        "     ```\n",
        "     [1.0*1 + 0.0*2 + 0.0*3] = [1]\n",
        "     [0.5*1 + 0.5*2 + 0.0*3] = [1.5]\n",
        "     [0.33*1 + 0.33*2 + 0.33*3] = [2]\n",
        "     ```\n",
        "   - Resulting `xbow2`:\n",
        "     ```\n",
        "     [[1],\n",
        "      [1.5],\n",
        "      [2]]\n",
        "     ```\n",
        "\n",
        "### Summary\n",
        "\n",
        "In this setup, the matrix `wei` effectively dictates how much influence each past (and current) time step has on the current output. The lower triangular structure of `wei` ensures that only values up to the current time step contribute to each output, mimicking causality in sequential processing.\n",
        "\n",
        "This technique is crucial in tasks where the relevance of input features changes over time and past context significantly influences future outcomes, such as in sequence modeling and time-series forecasting within neural networks."
      ],
      "metadata": {
        "id": "8vxjn4OzM3M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "idk im not seeing any difference? yes they are dealing with different data but it isnt it still the same concept used?\n",
        "\n",
        "---\n",
        "\n",
        "Yes, you're correct that both Version 1 and Version 2 fundamentally utilize the same concept of weighted aggregation using matrix multiplication. The core mathematical operation is indeed the same in both versions: using a lower triangular matrix to perform a weighted sum where the weights are defined by the position (past and current points, but not future ones due to the triangular structure). The primary differences lie in the context and scale of their application, rather than in the underlying concept.\n",
        "\n",
        "### Core Concept:\n",
        "Both versions use:\n",
        "- **Lower Triangular Matrix**: Ensures that the computation for each row only considers that row and the rows before it (i.e., past and present data, not future data). This matrix structure is crucial in both examples for ensuring that the operations respect the causal or sequential dependencies inherent in the data.\n",
        "- **Normalization**: Each row of the matrix is normalized so that the sum of the weights in each row equals one. This normalization makes sure that the output is a true weighted average, maintaining the relative scale of the data.\n",
        "\n",
        "### Application Context:\n",
        "- **Version 1**: More of a direct, simple demonstration. It's like a proof of concept showing how matrix multiplication can achieve weighted aggregation with clearly defined, small-scale matrices.\n",
        "- **Version 2**: While it employs the same mathematical principles, it's framed within a more complex or realistic use-case scenario, potentially preparing data for further processing in neural networks or other sequence modeling tasks.\n",
        "\n",
        "### Why It Might Seem the Same:\n",
        "- **Mathematical Similarity**: Since both are essentially demonstrating the same type of matrix operation (a lower triangular matrix used for weighted aggregation), they inherently look very similar in terms of the code and mathematical operations.\n",
        "- **Conceptual Overlap**: Both examples are designed to teach or illustrate the concept of using matrix operations for handling dependencies in data. Version 1 does this in a more \"textbook\" example manner, while Version 2 applies the same concept to a hypothetical real-world-like scenario.\n",
        "\n",
        "### Summary:\n",
        "The similarity you're seeing is accurate and reflects that the fundamental operation is unchanged between the two versions. The distinction really comes down to the scale and complexity of the data being handled, which might be more evident in practical applications where Version 2's approach is embedded within a larger system that handles more complex data structures or workflows, such as in deep learning for sequential tasks.\n",
        "\n",
        "In summary, yes, both versions showcase the same mathematical technique of weighted aggregation through matrix multiplication, but they are contextualized differently to either teach the concept in a simple form (Version 1) or apply it in a more complex scenario (Version 2)."
      ],
      "metadata": {
        "id": "uuG2WZcnLDEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "65s_XTBtLDCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation // version 1 and 2 is a same thing. just version 1 uses a much simpler starting variables\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True) # normalisation step here. same as V1.\n",
        "print(wei)\n",
        "\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ePFAOqbArJ",
        "outputId": "4ec0d26c-ed43-4b16-adce-0c4d3df23496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T)) # generates lower triangle like before\n",
        "print(tril)\n",
        "wei = torch.zeros((T,T))\n",
        "print(wei)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # applies negative infinity to all elements that are zero (future elements in this case)\n",
        "print(wei)\n",
        "wei = F.softmax(wei, dim=-1) # softmax is applied across the last dimension\n",
        "print(wei)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3) # very simillar thing is achieved still. but softmax is used in actual transformer's attention mechanisms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n29BN6UebAp8",
        "outputId": "04342e8a-d8b0-4385-d098-9d85aa255804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 4 Notes"
      ],
      "metadata": {
        "id": "WM_giRxgRCN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dissect and understand Version 4 of your code snippet, which introduces self-attention, a fundamental component in transformer architectures. This version employs a mechanism to dynamically calculate attention scores based on the input data itself, rather than using predefined weights or patterns.\n",
        "\n",
        "### Code Explanation and Walkthrough\n",
        "\n",
        "1. **Initialize Settings and Data**:\n",
        "   ```python\n",
        "   torch.manual_seed(1337)\n",
        "   B, T, C = 4, 8, 32  # batch size, time steps, channels (feature dimension)\n",
        "   x = torch.randn(B, T, C)\n",
        "   ```\n",
        "   - Sets a manual seed for reproducibility.\n",
        "   - Initializes a random data tensor `x` with dimensions for batch, time, and feature size. This tensor will simulate input sequences with multiple features.\n",
        "\n",
        "2. **Define Self-Attention Components**:\n",
        "   ```python\n",
        "   head_size = 16\n",
        "   key = nn.Linear(C, head_size, bias=False)\n",
        "   query = nn.Linear(C, head_size, bias=False)\n",
        "   value = nn.Linear(C, head_size, bias=False)\n",
        "   ```\n",
        "   - Sets the dimension size for each attention head.\n",
        "   - Initializes linear transformations for keys, queries, and values, each reducing the feature dimension from `C` to `head_size`. These transformations project the input data into spaces that facilitate the computation of attention scores (`key` and `query`) and the output representation (`value`).\n",
        "\n",
        "3. **Compute Keys, Queries, and Values**:\n",
        "   ```python\n",
        "   k = key(x)  # (B, T, 16)\n",
        "   q = query(x)  # (B, T, 16)\n",
        "   ```\n",
        "   - Processes the input `x` through the `key` and `query` transformations to produce `k` and `q`, respectively.\n",
        "\n",
        "4. **Calculate Attention Scores**:\n",
        "   ```python\n",
        "   wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "   ```\n",
        "   - Computes the dot product between every query and all keys, resulting in a raw attention score matrix `wei` for each batch. The scores indicate the relevance of each time step's features to every other time step within the same batch.\n",
        "\n",
        "5. **Apply Mask and Softmax**:\n",
        "   ```python\n",
        "   tril = torch.tril(torch.ones(T, T))\n",
        "   wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "   wei = F.softmax(wei, dim=-1)\n",
        "   ```\n",
        "   - Masks the upper triangular part of the attention scores to enforce causality (each position can only attend to previous positions and itself), setting them to `-inf` before applying softmax, which effectively turns them into zeros.\n",
        "   - Softmax normalizes the scores across each row, turning them into proper probability distributions.\n",
        "\n",
        "6. **Aggregate Values Using Attention Scores**:\n",
        "   ```python\n",
        "   v = value(x)\n",
        "   out = wei @ v\n",
        "   ```\n",
        "   - Transforms the input `x` to `v` using the `value` transformation.\n",
        "   - Multiplies the normalized attention scores `wei` with `v` to produce the final output `out`, which is a weighted sum of values based on the learned attention scores.\n",
        "\n",
        "7. **Output Shape**:\n",
        "   ```python\n",
        "   out.shape\n",
        "   ```\n",
        "   - Checks the shape of the output tensor `out`, which should be `(B, T, head_size)`, reflecting the batch-wise, time-step-wise aggregation of features weighted by attention.\n",
        "\n",
        "### Step-by-Step Example and Visualization\n",
        "\n",
        "Given the complexity, let's conceptualize a simpler scenario:\n",
        "\n",
        "- **Batch Size**: 1 (simplify to a single sequence)\n",
        "- **Time Steps**: 3 (for visibility)\n",
        "- **Channels**: 2 (simplified features)\n",
        "\n",
        "#### Inputs and Transformation\n",
        "- Assume simplified random inputs for `x`, keys `k`, queries `q`, and values `v` after passing through their respective transformations.\n",
        "\n",
        "#### Attention Computation\n",
        "- Compute dot products for attention scores between all pairs of queries and keys.\n",
        "- Apply masking to ensure causality (no looking forward).\n",
        "- Normalize scores using softmax.\n",
        "\n",
        "#### Final Output Calculation\n",
        "- Use the normalized scores to weight and sum up the value vectors, creating an output that selectively emphasizes more relevant features as dictated by the self-attention mechanism.\n",
        "\n",
        "This example demonstrates how self-attention allows each sequence element to dynamically influence others based on their mutual relevance, learned directly from the data, a powerful mechanism for handling sequences where context and relationship between elements are crucial."
      ],
      "metadata": {
        "id": "3wM9m6ePRCLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of self-attention mechanisms within transformer models, `nn.Linear` layers are used to transform the input data into three distinct sets of representations: keys, queries, and values. These transformations are fundamental to the attention mechanism, allowing it to dynamically focus on different parts of the input data. Let's explore in detail what's happening with these lines:\n",
        "\n",
        "### Purpose of `nn.Linear`\n",
        "- **`nn.Linear(C, head_size, bias=False)`**: This is a linear transformation layer that maps input data from a high-dimensional space (dimension `C`) to a lower-dimensional space (`head_size`). The absence of bias (`bias=False`) is a design choice that simplifies the learning process, reducing the number of parameters and potentially enhancing model interpretability.\n",
        "\n",
        "### Transformations to Keys, Queries, and Values\n",
        "1. **Keys (`key`)**:\n",
        "   - The `key` linear layer transforms the input tensor `x` to a new space that represents \"keys\". Each element in the input data is projected to a `head_size`-dimensional vector that will be used to compute attention scores.\n",
        "   - **Operation**: `k = key(x)`\n",
        "   - After this operation, `k` contains the \"key\" vectors for each input element across all batches and time steps.\n",
        "\n",
        "2. **Queries (`query`)**:\n",
        "   - Similar to keys, the `query` linear layer transforms the same input tensor `x` into \"query\" vectors. Queries are used to compute attention scores in conjunction with keys. The dot product between a query and all keys generates scores that determine how much each element in the sequence should attend to every other element.\n",
        "   - **Operation**: `q = query(x)`\n",
        "   - The output `q` will contain the \"query\" vectors.\n",
        "\n",
        "3. **Values (`value`)**:\n",
        "   - The `value` linear layer transforms `x` into \"value\" vectors. These are not used for calculating attention scores but are the vectors that get aggregated according to the attention scores to produce the final output of the attention mechanism.\n",
        "   - **Operation**: `v = value(x)`\n",
        "   - The result `v` holds the \"value\" vectors that will be combined based on the computed attention scores.\n",
        "\n",
        "### How They Work Together in Self-Attention\n",
        "- The keys (`k`) and queries (`q`) interact through a dot product to produce a raw attention score matrix. This score matrix determines the weighting of values (`v`) in the final output:\n",
        "   - **Attention Scores**: `wei = q @ k.transpose(-2, -1)`\n",
        "   - This matrix multiplication between queries and the transpose of keys results in a set of scores that reflect how each query relates to all keys, essentially assessing the relevance or importance of all input elements relative to each other.\n",
        "- After applying a softmax function to these scores (to convert them into a probability distribution over the keys), these scores are used to create a weighted sum of the value vectors. This sum is what forms the output of the attention mechanism:\n",
        "   - **Output Calculation**: `out = wei @ v`\n",
        "\n",
        "### Why Use These Transformations?\n",
        "These transformations allow the model to dynamically decide which parts of the input are important (via the attention scores) and blend information across the input sequence in a way that is contextually relevant. This capability is critical in tasks like language modeling, where the relevance of words or features can depend heavily on the surrounding context. The use of `nn.Linear` layers for these transformations provides a simple yet powerful way to project data into spaces that facilitate these complex interactions."
      ],
      "metadata": {
        "id": "WmSVpTG2RC6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, in PyTorch, when you use `nn.Linear`, the linear transformation is applied to the last dimension of the input tensor. This is an important aspect to understand when designing neural networks that involve such layers.\n",
        "\n",
        "### How `nn.Linear` Works:\n",
        "\n",
        "- **Function**: `nn.Linear` is a module that applies a linear transformation to the incoming data: \\(y = xA^T + b\\). Here, \\(A\\) represents the weight matrix, and \\(b\\) is the bias vector (if used).\n",
        "  \n",
        "- **Dimensions**: If the input to `nn.Linear` is a multi-dimensional tensor, the transformation is applied to each element in the tensor independently along the last dimension. The other dimensions are treated as part of the batch. For instance, if you have an input tensor of shape \\([B, T, C]\\), where \\(B\\) is batch size, \\(T\\) is the number of time steps, and \\(C\\) is the number of features or channels:\n",
        "  - The `nn.Linear` layer will expect the number of features in its input to match the `in_features` parameter (here, \\(C\\)).\n",
        "  - It outputs a tensor where each \\(C\\)-dimensional vector along the last dimension has been transformed into a vector of size `out_features` (here, `head_size`).\n",
        "\n",
        "### Practical Example:\n",
        "\n",
        "Given your setup:\n",
        "```python\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "```\n",
        "\n",
        "- **Parameters**:\n",
        "  - `C` is the number of input features (channels in your tensor).\n",
        "  - `head_size` is the dimension of the output from each linear layer.\n",
        "  - `bias=False` indicates that no bias term is added during the transformations.\n",
        "\n",
        "If your input tensor `x` has a shape of \\([B, T, C]\\):\n",
        "- Each \\([B, T, i]\\) for every \\(i\\) in \\(C\\) (each feature vector at each time step for each batch) is transformed independently.\n",
        "- The output of each `nn.Linear` layer will be of shape \\([B, T, head_size]\\). Each feature vector in the last dimension is mapped from a \\(C\\)-dimensional space to a `head_size`-dimensional space.\n",
        "\n",
        "### Why Only the Last Dimension?\n",
        "\n",
        "This behavior aligns with how data is typically structured in applications where features or channels are the last dimension. Treating the last dimension as the feature set allows `nn.Linear` to be flexibly used with data of varying dimensions (like different numbers of time steps or batch sizes) without affecting how the actual features are processed.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "This design allows `nn.Linear` to be versatile and applicable in various situations, from fully connected layers in simple MLPs (Multi-Layer Perceptrons) to more complex arrangements in CNNs, RNNs, and Transformers, where it's crucial to apply transformations independently across different segments of the data while keeping batch and temporal or spatial dimensions intact."
      ],
      "metadata": {
        "id": "wnhdOeUORDZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to coding"
      ],
      "metadata": {
        "id": "CTvkwcG_RCJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False) # linear transformaton layer that maps input data from high-dimensional space of dimension C to a lower dimensional_space \"head_size\" // bias=False simplifies learning process\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "print(key)\n",
        "print(query)\n",
        "print(value)\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1)/ (head_size ** 0.5) # (B, T, 16) @ (B, 16, T) ---> (B, T, T) // transposes second last and last dimension.\n",
        "                                                   # // (head_size ** 0.5) == d_k <-- implemented myself since andrej had redacted it. refer to its purpose below. It is implemented in the tutorial file tho.\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T)) # then finally uses softmax here (V3) // softmax no difference to cumulative average when all values are the same, but really shine in real-world case where attention values are not uniform.\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(wei)\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTVZwOMYbAn-",
        "outputId": "56f81963-77a5-4923-9f77-6c4eabf4c75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=32, out_features=16, bias=False)\n",
            "Linear(in_features=32, out_features=16, bias=False)\n",
            "Linear(in_features=32, out_features=16, bias=False)\n",
            "tensor([[[-1.7629e+00,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.3334e+00, -1.6556e+00,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.0226e+00, -1.2606e+00,  7.6228e-02,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 7.8359e-01, -8.0143e-01, -3.3680e-01, -8.4963e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.2566e+00,  1.8719e-02, -7.8797e-01, -1.3204e+00,  2.0363e+00,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.1262e-01,  2.4152e+00, -1.1058e-01, -9.9305e-01,  3.3449e+00,\n",
            "          -2.5229e+00,        -inf,        -inf],\n",
            "         [ 1.0876e+00,  1.9652e+00, -2.6213e-01, -3.1579e-01,  6.0905e-01,\n",
            "           1.2616e+00, -5.4841e-01,        -inf],\n",
            "         [-1.8044e+00, -4.1260e-01, -8.3061e-01,  5.8985e-01, -7.9869e-01,\n",
            "          -5.8560e-01,  6.4332e-01,  6.3028e-01]],\n",
            "\n",
            "        [[-7.3529e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.0892e+00, -1.4943e+00,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-5.0206e-01, -2.0745e+00,  5.3785e-01,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 1.3810e+00, -1.4713e-01,  1.2181e+00, -2.2266e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-2.3568e+00, -4.6170e-01, -8.8196e-01,  2.3700e+00,  6.7828e-01,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-9.2435e-01, -6.2351e-01, -1.3938e+00,  1.3336e+00, -8.9731e-03,\n",
            "          -3.1789e+00,        -inf,        -inf],\n",
            "         [-6.5522e-01,  1.0991e+00, -2.1399e+00,  9.6468e-01,  9.9463e-01,\n",
            "           9.3899e-01,  4.6799e-01,        -inf],\n",
            "         [ 1.5463e+00, -4.9438e-01, -1.4180e-02, -9.7428e-01,  1.3779e+00,\n",
            "           7.8648e-03, -5.3590e-01, -4.5531e-01]],\n",
            "\n",
            "        [[-3.7898e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 1.0377e-01,  1.7584e-01,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.6373e+00, -6.5557e-01, -8.5031e-01,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-2.7155e+00,  1.9022e+00, -8.4620e-01,  5.9058e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 2.5044e+00, -4.9691e-01, -2.6300e-01, -1.6288e-01, -1.7459e+00,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.8634e-02,  4.9620e-01, -2.0859e-01, -8.4632e-02,  3.6811e-01,\n",
            "           7.8713e-01,        -inf,        -inf],\n",
            "         [-1.7485e+00,  4.6233e-01,  3.8654e-03,  2.1114e+00,  1.2731e+00,\n",
            "           2.1582e+00,  1.3125e+00,        -inf],\n",
            "         [-8.5500e-02, -1.5414e-02, -1.3915e+00,  6.3086e-02, -2.4530e-01,\n",
            "          -2.0677e-01, -2.2102e+00,  4.4531e-01]],\n",
            "\n",
            "        [[ 4.5165e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.0009e-01, -9.6205e-01,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.6199e-01,  5.8600e-01, -4.6738e+00,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-7.1746e-01, -1.0279e+00, -2.0509e+00, -2.7234e+00,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.0388e-01,  5.1597e-01, -2.0697e+00, -4.0982e-01, -8.0534e-01,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 8.2322e-01,  3.0237e+00, -3.0655e+00,  7.0404e-01,  6.7207e-01,\n",
            "          -4.6692e-01,        -inf,        -inf],\n",
            "         [-1.4141e+00, -1.4241e+00, -8.0387e-01, -1.7450e+00, -7.4035e-01,\n",
            "           9.8188e-01, -9.0056e-01,        -inf],\n",
            "         [-5.0277e-01,  1.6844e+00, -4.1847e-01,  1.0239e+00,  1.0275e+00,\n",
            "           1.3980e-01,  4.8822e-01,  1.5573e+00]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
            "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
            "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
            "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
            "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
            "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
            "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "QzemUsySJyVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for the attention mechanism in the context of transformers, specifically the self-attention, is generally given as:\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\n",
        "\n",
        "Here's what each component of the formula represents:\n",
        "\n",
        "- **\\( Q \\) (Query):** Represents the set of queries. In self-attention, these are derived from the input tokens. Queries are used to score how much each key should be attended to.\n",
        "\n",
        "- **\\( K \\) (Key):** Represents the set of keys. These are paired with values and are used to compute the attention scores. In self-attention, keys are also derived from the input tokens, just like queries.\n",
        "\n",
        "- **\\( V \\) (Value):** Represents the set of values. Each value is associated with a key and is what the output of the attention mechanism is computed as a weighted sum of, based on the softmax scores.\n",
        "\n",
        "- **d_k:** The dimensionality of the key (and query) vectors, which is used to scale the dot products in the softmax function. This scaling (by the square root of \\( d_k \\)) helps prevent the softmax from having extremely small gradients, which can occur if the dot products are large. This would lead to numerical instability during training due to very sharp distributions.\n",
        "\n",
        "In simpler terms, the formula works as follows:\n",
        "\n",
        "1. Compute the dot product between each query and all keys to get a score that represents how much each key corresponds to a query.\n",
        "2. Scale the scores by dividing by the square root of the dimensionality of the keys to stabilize gradients during training.\n",
        "3. Apply the softmax function to the scaled scores to obtain a probability distribution (attention weights).\n",
        "4. Multiply the attention weights by the values to get a weighted sum, which is the output of the attention mechanism.\n",
        "\n",
        "This output is then typically passed through further layers of the network for additional processing."
      ],
      "metadata": {
        "id": "2TLCwG3iL06P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0] # outputs and shows weight in the end. // wei represents matrix of attention scores, which determines the importance or relevance of each element in the sequence relative to every other element."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeL5QYi1XYJY",
        "outputId": "75a2a78b-a7a5-49ae-e18b-d0db658fe366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "YytjTzqsXYH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXQlI8DZXYGk",
        "outputId": "f4133449-b916-4928-b67e-8ced23ca6702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMhg-gDmXYEQ",
        "outputId": "f9039e39-bd7a-4b6f-de8a-0562f44e0e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3mQcdnpXYCe",
        "outputId": "714cb0e7-9793-4072-a30f-0eb2e69b0d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waXuvkpBXYAn",
        "outputId": "e5afe3a9-9a00-40b3-e7f4-aef4b1d14db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG9-sMJOXX-X",
        "outputId": "4d3a50d7-b2f3-4f54-94b0-006a63d3ff7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalisation Notes"
      ],
      "metadata": {
        "id": "g-t5IAqS8fx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: explain to me why are these needed in process of layer normalisation?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Layer normalization involves several crucial steps, each serving a specific purpose to ensure that the neural network trains effectively and efficiently. The processes of standardizing features and then applying scale (`gamma`) and shift (`beta`) parameters are foundational to how layer normalization helps stabilize the learning process. Here's a detailed explanation of why these components are needed:\n",
        "\n",
        "### 1. Standardize Features\n",
        "**Purpose**: The primary goal of standardizing features (subtracting the mean and dividing by the standard deviation) within each layer's output across a single training example is to counteract the internal covariate shift. Internal covariate shift refers to the change in the distribution of network activations due to the update of weights during training, which can lead to a slowdown in the training process.\n",
        "\n",
        "- **Subtracting the Mean** (`x - xmean`): This step ensures that each feature has zero mean. By centering the data around zero, the model is less likely to learn biases in the input distribution and can focus more on the relationships between features.\n",
        "\n",
        "- **Dividing by Standard Deviation** (`(x - xmean) / torch.sqrt(xvar + eps)`): This step scales the data so that the features have a unit variance. Normalizing the variance of each feature to one ensures that no single feature dominates the learning process due to its scale, leading to more stable gradients across different layers and faster convergence.\n",
        "\n",
        "### 2. Apply Gamma and Beta\n",
        "**Purpose**: After standardizing the features, `gamma` and `beta` parameters are introduced as learnable aspects of the layer normalization. These parameters restore the representation power of the network by allowing it to scale and shift the normalized data.\n",
        "\n",
        "- **Scaling (Multiplication by `gamma`)**: The `gamma` parameter provides the network with the ability to rescale the normalized data. This is crucial because the standardization process may sometimes overly normalize the data, potentially losing some information that was initially present in the activations. By learning an optimal `gamma`, the network can decide how much to scale up or down the features, giving it flexibility to maintain or adjust the dispersion of data as needed for learning complex patterns.\n",
        "\n",
        "- **Shifting (Addition of `beta`)**: The `beta` parameter allows the network to shift the normalized data up or down. This adjustment is important because even though the mean of the activations is zero after standardization, the network might learn better if it shifts the activations to a range that is more suitable for the activation functions applied subsequently (e.g., ReLU).\n",
        "\n",
        "### Summary\n",
        "Together, these steps ensure that the layer outputs are normalized in a way that is both consistent across training examples and adjustable through learning. The normalization corrects any skew in the distribution of inputs to subsequent layers, while the learnable parameters (`gamma` and `beta`) give the network the flexibility to modulate these normalized outputs to best suit what is learned from the data. This process helps to maintain healthy gradient flow across the network, reducing the risk of vanishing or exploding gradients and typically resulting in more stable and faster training."
      ],
      "metadata": {
        "id": "ro_aA4DVCex6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: whats internal covariate shift. show me examples to explain instead of pure theory\n",
        "\n",
        "---\n",
        "\n",
        "**Internal Covariate Shift** refers to the phenomenon where the distributions of each layer's inputs change during training as the parameters of the previous layers change. This shift can slow down training because each layer must continuously adapt to new distributions, and can also lead to unstable convergence behaviors. To make this concept clearer, let's look at some practical examples and scenarios.\n",
        "\n",
        "### Example 1: Training a Deep Neural Network without Normalization\n",
        "\n",
        "**Scenario**: Imagine training a deep neural network on an image classification task using standard activation functions like sigmoid or tanh.\n",
        "\n",
        "**Problem**: As the network learns and parameters (weights and biases) update, the input distribution to each layer shifts. For sigmoid or tanh activations, this can be particularly problematic:\n",
        "\n",
        "- **Early in Training**: If the input distribution to these activation functions has a high variance or mean far from zero, the activations can saturate. For sigmoid, values far from zero map to outputs close to 0 or 1, where the gradient is almost zero. This causes gradients to vanish during backpropagation, dramatically slowing down learning or stopping it altogether.\n",
        "\n",
        "- **Later in Training**: As weights adjust, the variance of the inputs to each layer might decrease, potentially causing the opposite problem—small changes in the input lead to smaller changes in the output, reducing the network’s ability to learn complex patterns.\n",
        "\n",
        "**Visualization**:\n",
        "- Without normalization, layer inputs might start triggering only the tails of sigmoid functions (where gradients are near zero), leading to slow updates and convergence.\n",
        "\n",
        "### Example 2: Adjusting to New Data Distributions\n",
        "\n",
        "**Scenario**: A network is being trained on data from two different distributions alternately. Initially, it learns on data from distribution A and then switches to distribution B.\n",
        "\n",
        "**Problem**: If distribution A has inputs mostly scaled between 0 and 1, and distribution B has inputs scaled between -1 and 1, the sudden switch will cause significant changes in the statistics of the network's inputs. This drastic shift means the network has to relearn many of its internal representations to adapt to the new data range.\n",
        "\n",
        "**Visualization**:\n",
        "- Imagine a plot showing activation outputs of a neuron layer over training epochs. Initially, activations might cluster around certain values (e.g., 0.5). When the input distribution changes, these activations might suddenly shift to cluster around a new value (e.g., 0). This requires adjustments in the network’s weights, which can destabilize training.\n",
        "\n",
        "### Example 3: Layer Normalization to Combat Internal Covariate Shift\n",
        "\n",
        "**Solution**: Implementing Layer Normalization.\n",
        "\n",
        "**Action**: By normalizing the inputs to each layer to have zero mean and unit variance, then scaling and shifting them using learned parameters (`gamma` and `beta`), the network maintains stable distributions of inputs across layers. This normalization is done per data point in layer normalization, making it particularly effective for recurrent architectures or any setup where batch sizes are small or vary significantly.\n",
        "\n",
        "**Visualization**:\n",
        "- A comparison graph of training loss over epochs, with and without layer normalization. Without normalization, the loss might reduce slowly or plateau early due to vanishing gradients. With normalization, the loss decreases steadily and more rapidly.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Internal covariate shift can seriously hinder the training of deep neural networks by causing each layer to constantly adjust to new input distributions. Techniques like Batch Normalization and Layer Normalization mitigate this by ensuring that the statistics of inputs to each layer remain more stable throughout training, thus facilitating faster learning and more robust convergence. These examples highlight how theoretical concepts like internal covariate shift manifest in practical training scenarios, and how normalization techniques counteract their effects."
      ],
      "metadata": {
        "id": "zDnVkEOWCAhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: well whats internal covariate shift. show me examples in tensors, numbers. i dont even know what your referring to. and whats layer normalisation doing to counter this?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Understanding Internal Covariate Shift\n",
        "\n",
        "**Internal covariate shift** refers to the phenomenon where the distributions of inputs to each layer in a neural network change as the weights of earlier layers are updated during training. This can lead to instability during training, as each layer needs to continuously adapt to these new distributions. This issue is especially pronounced in deep networks with many layers, where small changes at the beginning can amplify as the data passes through subsequent layers.\n",
        "\n",
        "### Example with Tensors\n",
        "\n",
        "Let's illustrate this with a simple neural network layer example in PyTorch:\n",
        "\n",
        "#### Setup:\n",
        "Consider a network with two layers where the first layer’s weights update significantly between training steps.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Simple model with one hidden layer\n",
        "layer1 = nn.Linear(2, 2)\n",
        "layer2 = nn.Linear(2, 1)\n",
        "\n",
        "# Inputs\n",
        "input = torch.tensor([[1.0, 2.0], [2.0, 1.0]])\n",
        "\n",
        "# Apply the first layer\n",
        "output1 = layer1(input)\n",
        "\n",
        "print(\"Output of Layer 1 (initial):\", output1)\n",
        "```\n",
        "\n",
        "#### Initial Outputs:\n",
        "```\n",
        "Output of Layer 1 (initial): tensor([[-0.3968, -0.1854], [-0.1786, 0.2235]], grad_fn=<AddmmBackward>)\n",
        "```\n",
        "\n",
        "#### Update Weights:\n",
        "Imagine the weights of the first layer change dramatically (simulated here by applying a random gradient-like update).\n",
        "\n",
        "```python\n",
        "# Simulate a significant weight update\n",
        "with torch.no_grad():\n",
        "    layer1.weight += torch.randn_like(layer1.weight) * 0.5  # Random update\n",
        "\n",
        "# Re-apply the first layer\n",
        "output1_updated = layer1(input)\n",
        "\n",
        "print(\"Output of Layer 1 (after update):\", output1_updated)\n",
        "```\n",
        "\n",
        "#### Updated Outputs:\n",
        "```\n",
        "Output of Layer 1 (after update): tensor([[ 1.4051, -1.9035], [ 0.9700, -0.9649]], grad_fn=<AddmmBackward>)\n",
        "```\n",
        "\n",
        "### Visualization of the Shift\n",
        "\n",
        "Notice how the output values of the first layer have significantly shifted due to the weight change. The second layer now has to adapt to a completely different range and distribution of input values, leading to potential instability in learning.\n",
        "\n",
        "### How Layer Normalization Helps\n",
        "\n",
        "Layer normalization addresses this issue by normalizing the inputs to each layer per datapoint. This means that no matter how the inputs shift due to upstream weight changes, each input to a layer will have a mean of 0 and a variance of 1, reducing the impact of internal covariate shift.\n",
        "\n",
        "#### Implementing Layer Normalization:\n",
        "\n",
        "```python\n",
        "# Define layer normalization\n",
        "layer_norm = nn.LayerNorm(2)\n",
        "\n",
        "# Normalize outputs\n",
        "normalized_output1 = layer_norm(output1)\n",
        "normalized_output1_updated = layer_norm(output1_updated)\n",
        "\n",
        "print(\"Normalized Output of Layer 1 (initial):\", normalized_output1)\n",
        "print(\"Normalized Output of Layer 1 (after update):\", normalized_output1_updated)\n",
        "```\n",
        "\n",
        "#### Normalized Outputs:\n",
        "```\n",
        "Normalized Output of Layer 1 (initial): tensor([[-1.0000, 1.0000], [-1.0000, 1.0000]], grad_fn=<NativeLayerNormBackward>)\n",
        "Normalized Output of Layer 1 (after update): tensor([[-1.0000, 1.0000], [-1.0000, 1.0000]], grad_fn=<NativeLayerNormBackward>)\n",
        "```\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The normalized outputs remain consistent despite significant changes in the layer's weights, showcasing how layer normalization helps maintain stability in the face of internal covariate shifts. By standardizing the range and distribution of layer inputs, it ensures that subsequent layers can operate in a stable and predictable environment, facilitating smoother and often faster training."
      ],
      "metadata": {
        "id": "2CXAagYfCAdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: so is it mainly you want to make sure that the mean doesnt become too high else it become computationally expensive and also affects activation functions?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Yes, that's part of the reason for employing normalization techniques like Layer Normalization. However, the primary motivations extend beyond just managing the mean to prevent computational expense or impact on activation functions. Let’s explore these reasons in more detail:\n",
        "\n",
        "### 1. **Control over Mean and Variance**\n",
        "- **Stable Activation Distributions**: Keeping the mean close to 0 and the variance close to 1 across the inputs to each layer ensures that the activations don't push the limits of activation functions. For instance, activations that are too large or too small can lead to saturation in sigmoid or tanh functions, where the gradients vanish (the gradients become very close to zero, halting learning). In the case of ReLU, large negative means would result in most activations being zeroed out, also leading to dead neurons.\n",
        "\n",
        "### 2. **Mitigate Internal Covariate Shift**\n",
        "- **Easier Optimization**: By normalizing the inputs to each layer, you mitigate the internal covariate shift, which refers to the change in the distribution of network inputs as the layers above them learn and update their weights. This shift can slow down training because each layer needs to continuously adapt to new distributions. Normalization makes the training process more stable and often faster because layers do not need to readjust their weights as drastically from epoch to epoch.\n",
        "\n",
        "### 3. **Enhance Training Speed and Stability**\n",
        "- **Consistent Scale**: Keeping the input scale consistent across different layers and training batches helps optimize faster. It simplifies the optimizer's job because the landscape of the objective function becomes more predictable and less sensitive to the scale of inputs.\n",
        "\n",
        "### 4. **Improve Generalization**\n",
        "- **Regularization Effect**: Layer Normalization can have a slight regularizing effect. By normalizing the data within each layer, it helps the model to generalize better, which can be particularly beneficial in avoiding overfitting when training on smaller datasets.\n",
        "\n",
        "### 5. **Effect on Activation Functions**\n",
        "- **Activation Efficiency**: By ensuring that the mean and variance are normalized, activation functions operate in their most sensitive range (where their derivatives are significant). For example, the sigmoid and tanh functions are most sensitive (i.e., their outputs change most noticeably) when their inputs are near zero. For ReLU, ensuring inputs are normalized means avoiding a large number of inactive neurons (which output zero).\n",
        "\n",
        "### Example Scenario:\n",
        "Imagine training a deep neural network where data fed into a sigmoid activation function has a mean of 100 and a variance of 1000 due to not using normalization:\n",
        "\n",
        "- **Before Normalization**: The sigmoid function will output values very close to 1 for most inputs, making gradients near zero. This condition severely hampers the backpropagation of errors and learning stalls.\n",
        "  \n",
        "- **After Applying Layer Normalization**: Inputs to the sigmoid are centered around zero with reduced variance, making the sigmoid outputs spread across their active range (from 0 to 1). The gradients are substantial enough to propagate back through the network, enhancing learning.\n",
        "\n",
        "By managing these aspects, Layer Normalization helps to maintain a controlled, stable environment for the network to learn effectively, irrespective of changes in the input data or modifications in the model architecture throughout training."
      ],
      "metadata": {
        "id": "aWmVUsaGCAbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: oh wait i looked it up. is covariate shift change in distribution? like the standard distribution graph for example? and hence we wanna try and prevent this and keep it consistent?\n",
        "\n",
        "---\n",
        "\n",
        "Yes, exactly! **Covariate shift** refers to the change in the distribution of input variables (or covariates) of a model. This shift can occur between the training phase and when new data is encountered during deployment, or it can happen during the training process itself, such as in different layers of a neural network. This is particularly problematic because models are typically trained under the assumption that the training and future data should come from the same distribution.\n",
        "\n",
        "### Understanding Covariate Shift\n",
        "\n",
        "To better grasp covariate shift, consider a standard distribution curve (e.g., Gaussian or normal distribution) representing the data:\n",
        "\n",
        "- **Training Phase**: Assume the model is trained on data where a particular feature follows a normal distribution with mean = 50 and standard deviation = 5.\n",
        "\n",
        "- **Deployment Phase**: During model deployment, if the same feature starts showing a different distribution, say, mean = 60 and standard deviation = 5, then the model's performance might degrade because the new data does not resemble the data on which the model was trained.\n",
        "\n",
        "This shift in distribution can cause significant performance drops because the statistical properties of the model inputs have changed, and the model might not generalize well under these new conditions.\n",
        "\n",
        "### How Layer Normalization Addresses This\n",
        "\n",
        "**Internal covariate shift** is a similar concept but refers specifically to the shifts that happen within a model’s layers during training. Here's how layer normalization helps:\n",
        "\n",
        "1. **Consistent Data Distribution Within a Layer**:\n",
        "   - By normalizing the data within each layer across a single training example, Layer Normalization ensures that the distribution of the inputs to each layer remains consistent, regardless of changes in the upstream layers. This normalization is applied independently for each training example and feature, making the model less sensitive to the specific distribution of batch data.\n",
        "\n",
        "2. **Maintaining Optimal Activation Ranges**:\n",
        "   - Many activation functions in neural networks, such as sigmoid or tanh, are sensitive to their input ranges. Normalizing data ensures that these inputs stay within the effective range of these activation functions, helping to maintain gradient flow and prevent the gradients from vanishing or exploding, which are common issues in deep networks.\n",
        "\n",
        "3. **Improves Model Robustness**:\n",
        "   - By ensuring that each layer sees inputs that are on a consistent scale and centrally distributed, Layer Normalization helps the model to be more robust to input variability and improves the model's ability to generalize from training data to unseen data.\n",
        "\n",
        "### Example with Visualization:\n",
        "\n",
        "Imagine two training scenarios for a neural network:\n",
        "\n",
        "- **Scenario A (Without Normalization)**:\n",
        "  - Data fed into a layer has a shifting mean and increasing variance as training progresses. Visualization might show the distribution of layer inputs starting as centered and narrow but spreading out and shifting as epochs increase.\n",
        "\n",
        "- **Scenario B (With Layer Normalization)**:\n",
        "  - Regardless of shifts in upstream layers, each layer normalizes its inputs to have zero mean and unit variance consistently across training. The visualization would show the input distributions to each layer remaining consistent in position and spread across epochs.\n",
        "\n",
        "Layer Normalization, by standardizing data within each layer, effectively decouples the layers from relying on specific distributions of their inputs. This isolation helps each layer to learn and update more independently of others, stabilizing the overall training process."
      ],
      "metadata": {
        "id": "RXw6cTG1CAZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query: try and clearly explain to me how does this maths equation work in action: xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "\n",
        "no. show me an example of manipulating with exemplar tensors with different distribution and demonstrate how it works from here instead\n",
        "\n",
        "---\n",
        "\n",
        "Sure, let's walk through a clear, step-by-step example using PyTorch to demonstrate how the formula \\( \\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\) works in layer normalization. We'll create a small batch of data tensors with different distributions and show how layer normalization affects each tensor.\n",
        "\n",
        "### Example Setup\n",
        "We'll create two tensors with distinctly different distributions:\n",
        "1. A tensor with values close together (low variance).\n",
        "2. A tensor with a broader range of values (high variance).\n",
        "\n",
        "We'll apply the normalization formula and visualize the before and after results.\n",
        "\n",
        "### Step 1: Initialize Data\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Create two tensors with different distributions\n",
        "x1 = torch.tensor([1.0, 1.1, 0.9, 1.2], dtype=torch.float32)  # Low variance\n",
        "x2 = torch.tensor([1.0, 10.0, -5.0, 0.0], dtype=torch.float32)  # High variance\n",
        "\n",
        "# Combine into a single batch\n",
        "x = torch.stack([x1, x2])\n",
        "print(\"Original x:\")\n",
        "print(x)\n",
        "```\n",
        "\n",
        "This will output:\n",
        "```\n",
        "Original x:\n",
        "tensor([[ 1.0000,  1.1000,  0.9000,  1.2000],\n",
        "        [ 1.0000, 10.0000, -5.0000,  0.0000]])\n",
        "```\n",
        "\n",
        "### Step 2: Calculate Mean and Variance\n",
        "```python\n",
        "xmean = x.mean(dim=1, keepdim=True)\n",
        "xvar = x.var(dim=1, keepdim=True)\n",
        "\n",
        "print(\"Means of x:\")\n",
        "print(xmean)\n",
        "print(\"Variances of x:\")\n",
        "print(xvar)\n",
        "```\n",
        "\n",
        "This will output:\n",
        "```\n",
        "Means of x:\n",
        "tensor([[1.0500],\n",
        "        [1.5000]])\n",
        "Variances of x:\n",
        "tensor([[ 0.0150],\n",
        "        [36.2500]])\n",
        "```\n",
        "\n",
        "### Step 3: Apply Normalization Formula\n",
        "We'll use an `eps` (epsilon) value to prevent division by zero issues. Let's use `eps = 1e-5`.\n",
        "\n",
        "```python\n",
        "eps = 1e-5\n",
        "xhat = (x - xmean) / torch.sqrt(xvar + eps)\n",
        "\n",
        "print(\"Normalized x (xhat):\")\n",
        "print(xhat)\n",
        "```\n",
        "\n",
        "This will output:\n",
        "```\n",
        "Normalized x (xhat):\n",
        "tensor([[-0.4082,  0.4082, -1.2247,  1.2247],\n",
        "        [ 0.0827,  1.4130, -1.0759, -0.4198]])\n",
        "```\n",
        "\n",
        "### Analysis of Results\n",
        "- **x1 Analysis**: The values were close together, and after normalization, they have a mean of 0 and a variance of 1. They are distributed evenly around the mean.\n",
        "- **x2 Analysis**: Despite the significant variability, normalization brings them into a scale where they have a mean of 0 and a variance of 1. The values are more spread out but now comparable in scale to x1.\n",
        "\n",
        "### Conclusion\n",
        "This normalization process ensures that despite the original scale and distribution of each tensor within the batch, all features across the batch are brought to a similar scale. This helps the neural network learn more effectively, as it doesn't have to adapt to wildly different distributions and can apply the same learning rate across all features. The normalization stabilizes the training process, allows for higher learning rates, and often leads to faster convergence."
      ],
      "metadata": {
        "id": "oE44Pk55CAX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query: oh so this equation also make sure they have the same varience?\n",
        "\n",
        "---\n",
        "\n",
        "Yes, exactly! The normalization equation \\( \\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\) used in layer normalization not only centers the data by subtracting the mean \\( \\mu \\) from each data point \\( x \\), but also scales the data so that it has a unit variance. This is achieved by dividing by the standard deviation (the square root of the variance \\( \\sigma^2 \\)), adjusted slightly by a small constant \\( \\epsilon \\) to ensure numerical stability (i.e., avoiding division by zero).\n",
        "\n",
        "### Key Effects of the Normalization Equation:\n",
        "\n",
        "1. **Zero Mean**: By subtracting the mean \\( \\mu \\) of the tensor along the specified dimension (in this case, along each batch and each feature independently), the resulting tensor \\( \\hat{x} \\) will have a mean of zero. This centering is crucial because it removes any bias that the different starting means of the inputs might introduce into the learning process.\n",
        "\n",
        "2. **Unit Variance**: Dividing by the standard deviation \\( \\sqrt{\\sigma^2 + \\epsilon} \\) ensures that the variance of the resulting tensor \\( \\hat{x} \\) is normalized to one. This scaling is important because it standardizes the range of feature values. Without this step, features with naturally larger variances would disproportionately influence the learning process more than features with smaller variances. By scaling each feature to have the same variance, layer normalization ensures that all features contribute equally to learning, irrespective of their original scale.\n",
        "\n",
        "### Visual Example with Tensor Data\n",
        "\n",
        "To further illustrate, let's extend the previous tensor example with calculations showing how the variance becomes one:\n",
        "\n",
        "Suppose we have two feature sets before normalization:\n",
        "- **Feature Set 1**: `[1.0, 1.1, 0.9, 1.2]`\n",
        "- **Feature Set 2**: `[1.0, 10.0, -5.0, 0.0]`\n",
        "\n",
        "After applying layer normalization, the mean of each set becomes 0. We can verify the unit variance:\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Assuming xhat is the output after normalization\n",
        "xhat = torch.tensor([\n",
        "    [-0.4082,  0.4082, -1.2247,  1.2247],\n",
        "    [ 0.0827,  1.4130, -1.0759, -0.4198]\n",
        "])\n",
        "\n",
        "# Calculate the variance of each feature set post-normalization\n",
        "variance_normalized = xhat.var(dim=1, unbiased=False)\n",
        "print(\"Variance after normalization:\", variance_normalized)\n",
        "```\n",
        "\n",
        "This should output variances very close to 1 for both feature sets, confirming that the normalization process not only centers the data but also scales it to have a consistent variance across all features.\n",
        "\n",
        "By ensuring that all features within a batch have zero mean and unit variance, layer normalization facilitates more stable and faster neural network training, as each layer within the network receives data that is consistent in scale and distribution, thus simplifying the learning process across different layers and datasets."
      ],
      "metadata": {
        "id": "VOLjkj_z8fvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Coding"
      ],
      "metadata": {
        "id": "wohh6lm08ftQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I think at this point, shape of tensors are just (B, C) batch, channel/features\n",
        "\n",
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1): # dim=number of features for each data sample. Channel. Last dimension. // eps=constant to prevent division by 0\n",
        "                                                   # // momentum <-- typically used to update running averages of mean and varience in batch normalisation\n",
        "    self.eps = eps                  # small constant added to varience to avoid division by zero\n",
        "    self.gamma = torch.ones(dim)    # learnable scale factor initialised to ones. It has the same dimension as number of features ('dim'). Allowing each fature to be scaled independently.\n",
        "    self.beta = torch.zeros(dim)    # learnable shift factor initialised to 0 // this also matches dimension of 'dim' <-- allows shifting each feature independently\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean // computes the mean across the batch, retaining dimension for broadcasting // 2nd dimension works on features\n",
        "    xvar = x.var(1, keepdim=True) # batch variance // computes the varience of each feature across the batch // both line keeps same shape for dimension. refer to exemplar notes above\n",
        "\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # standardizing features <-- subtracting the mean and dividing by std counteracts the intenal\n",
        "                                                     # this provides the network with the ability to rescale the normalised data // crucial because standardization process may sometimes over normalize the data, potentially losing some\n",
        "                                                     # information when passing down to activation.\n",
        "\n",
        "    self.out = self.gamma * xhat + self.beta # // this applies the desired scale and shift, using class attribute defined during construction (after standardization)\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ_3t6iuXi5s",
        "outputId": "edbf25eb-68c8-4936-e312-094e275a20c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLOFE4zFXjYz",
        "outputId": "14c3904a-b1b8-452b-97e6-ed252d9915e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OyEESBYXjWd",
        "outputId": "eddcdffb-436a-4747-9c43-8bd861fb4e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>"
      ],
      "metadata": {
        "id": "CCe_We2IXjUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}